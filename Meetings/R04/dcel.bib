
@article{barequet_dcel:_1998,
	title = {{DCEL}: {A} {Polyhedral} {Database} and {Programming} {Environment}},
	volume = {08},
	issn = {0218-1959, 1793-6357},
	shorttitle = {{DCEL}},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0218195998000308},
	doi = {10.1142/S0218195998000308},
	abstract = {In this paper we describe the DCEL system: a geometric software package which implements a polyhedral programming environment. This package enables fast prototyping of geometric algorithms for polyhedra or for polyhedral surfaces. We provide an overview of the system's functionality and demonstrate its use in several applications.},
	language = {en},
	number = {05n06},
	urldate = {2019-06-03},
	journal = {International Journal of Computational Geometry \& Applications},
	author = {Barequet, Gill},
	month = oct,
	year = {1998},
	note = {3},
	pages = {619--636},
	file = {Barequet - 1998 - DCEL A Polyhedral Database and Programming Enviro.pdf:/home/and/Zotero/storage/H9KZ72HQ/Barequet - 1998 - DCEL A Polyhedral Database and Programming Enviro.pdf:application/pdf}
}

@book{preparata_computational_1985,
	address = {New York},
	series = {Monographs in {Computer} {Science}},
	title = {Computational {Geometry}: {An} {Introduction}},
	isbn = {978-0-387-96131-6},
	shorttitle = {Computational {Geometry}},
	url = {https://www.springer.com/us/book/9780387961316},
	abstract = {From the reviews: "This book offers a coherent treatment, at the graduate textbook level, of the field that has come to be known in the last decade or so as computational geometry. ... ... The book is well organized and lucidly written; a timely contribution by two founders of the field. It clearly demonstrates that computational geometry in the plane is now a fairly well-understood branch of computer science and mathematics. It also points the way to the solution of the more challenging problems in dimensions higher than two." \#Mathematical Reviews\#1 "... This remarkable book is a comprehensive and systematic study on research results obtained especially in the last ten years. The very clear presentation concentrates on basic ideas, fundamental combinatorial structures, and crucial algorithmic techniques. The plenty of results is clever organized following these guidelines and within the framework of some detailed case studies. A large number of figures and examples also aid the understanding of the material. Therefore, it can be highly recommended as an early graduate text but it should prove also to be essential to researchers and professionals in applied fields of computer-aided design, computer graphics, and robotics." \#Biometrical Journal\#2},
	language = {en},
	urldate = {2019-06-03},
	publisher = {Springer-Verlag},
	author = {Preparata, Franco P. and Shamos, Michael},
	year = {1985},
	note = {2},
	file = {Snapshot:/home/and/Zotero/storage/XPFVM9CF/9780387961316.html:text/html}
}

@article{gonzalez_graphx:_nodate,
	title = {{GraphX}: {Graph} {Processing} in a {Distributed} {Dataﬂow} {Framework}},
	abstract = {In pursuit of graph processing performance, the systems community has largely abandoned general-purpose distributed dataﬂow frameworks in favor of specialized graph processing systems that provide tailored programming abstractions and accelerate the execution of iterative graph algorithms. In this paper we argue that many of the advantages of specialized graph processing systems can be recovered in a modern general-purpose distributed dataﬂow system. We introduce GraphX, an embedded graph processing framework built on top of Apache Spark, a widely used distributed dataﬂow system. GraphX presents a familiar composable graph abstraction that is sufﬁcient to express existing graph APIs, yet can be implemented using only a few basic dataﬂow operators (e.g., join, map, group-by). To achieve performance parity with specialized graph systems, GraphX recasts graph-speciﬁc optimizations as distributed join optimizations and materialized view maintenance. By leveraging advances in distributed dataﬂow frameworks, GraphX brings low-cost fault tolerance to graph processing. We evaluate GraphX on real workloads and demonstrate that GraphX achieves an order of magnitude performance gain over the base dataﬂow framework and matches the performance of specialized graph processing systems while enabling a wider range of computation.},
	language = {en},
	author = {Gonzalez, Joseph E and Xin, Reynold S and Dave, Ankur and Crankshaw, Daniel and Franklin, Michael J and Stoica, Ion},
	note = {13},
	pages = {16},
	file = {Gonzalez et al. - GraphX Graph Processing in a Distributed Dataﬂow .pdf:/home/and/Zotero/storage/ZKSX6V5D/Gonzalez et al. - GraphX Graph Processing in a Distributed Dataﬂow .pdf:application/pdf}
}

@article{asano_constant-working-space_2009,
	title = {Constant-{Working}-{Space} {Algorithms} for {Geometric} {Problems}},
	volume = {2},
	doi = {10.20382/jocg.v2i1a4},
	abstract = {Constant-work-space algorithms may use only constantly many cells of storage in addition to their input, which is provided as a read-only array. We show how to construct several geometric structures efficiently in the constant-work-space model. Traditional algorithms process the input into a suitable data structure (like a doubly-connected edge list) that allows efficient traversal of the structure at hand. In the constant-work-space setting, however, we cannot afford to do this. Instead, we provide operations that compute the desired features on the fly by accessing the input with no extra space. The whole geometric structure can be obtained by using these operations to enumerate all the features. Of course, we must pay for the space savings by slower running times. While the standard data structure allows us to implement traversal operations in constant time, our schemes typically take linear time to read the input data in each step. We begin with two simple problems: triangulating a planar point set and finding the trapezoidal decomposition of a simple polygon. In both cases adjacent features can be enumerated in linear time per step, resulting in total quadratic running time to output the whole structure. Actually, we show that the former result carries over to the Delaunay triangulation, and hence the Voronoi diagram. This also means that we can compute the largest empty circle of a planar point set in quadratic time and constant work-space. As another application, we demonstrate how to enumerate the features of an Euclidean minimum spanning tree (EMST) in quadratic time per step, so that the whole EMST can be found in cubic time using constant work-space. Finally, we describe how to compute a shortest geodesic path between two points in a simple polygon. Although the shortest path problem in general graphs is NL-complete [18], this constrained problem can be solved in quadratic time using only constant work-space.},
	journal = {JoCG},
	author = {Asano, Tetsuo and Rote, Günter},
	year = {2009},
	note = {4},
	pages = {46--68},
	file = {Full Text PDF:/home/and/Zotero/storage/8WPR3PHT/Asano and Rote - 2009 - Constant-Working-Space Algorithms for Geometric Pr.pdf:application/pdf}
}

@book{berg_computational_2008,
	address = {Berlin Heidelberg},
	edition = {3},
	title = {Computational {Geometry}: {Algorithms} and {Applications}},
	isbn = {978-3-540-77973-5},
	shorttitle = {Computational {Geometry}},
	url = {https://www.springer.com/us/book/9783540779735},
	abstract = {Computational geometry emerged from the ?eld of algorithms design and analysis in the late 1970s. It has grown into a recognized discipline with its own journals, conferences, and a large community of active researchers. The success of the ?eld as a research discipline can on the one hand be explained from the beauty of the problems studied and the solutions obtained, and, on the other hand, by the many application domains—computer graphics, geographic information systems (GIS), robotics, and others—in which geometric algorithms play a fundamental role. For many geometric problems the early algorithmic solutions were either slow or dif?cult to understand and implement. In recent years a number of new algorithmic techniques have been developed that improved and simpli?ed many of the previous approaches. In this textbook we have tried to make these modern algorithmic solutions accessible to a large audience. The book has been written as a textbook for a course in computational geometry, but it can also be used for self-study.},
	language = {en},
	urldate = {2019-06-03},
	publisher = {Springer-Verlag},
	author = {Berg, Mark de and Cheong, Otfried and Kreveld, Marc van and Overmars, Mark},
	year = {2008},
	note = {1},
	file = {Snapshot:/home/and/Zotero/storage/S6IAJG4W/9783540779735.html:text/html}
}

@article{zhou_parallel_2018,
	title = {A parallel method to accelerate spatial operations involving polygon intersections},
	volume = {32},
	issn = {1365-8816, 1362-3087},
	url = {https://www.tandfonline.com/doi/full/10.1080/13658816.2018.1508689},
	doi = {10.1080/13658816.2018.1508689},
	abstract = {Polygon intersection is an important spatial data-handling process, on which many spatial operations are based. However, this process is computationally intensive because it involves the detection and calculation of polygon intersections. We addressed this computation issue based on two perspectives. First, we improved a method called boundary algebra ﬁlling to eﬃciently rasterize the input polygons. Polygon intersections were subsequently detected in the cells of the raster. Owing to the use of a raster data structure, this method oﬀers advantages of reduced task dependence and improved performance. Based on this method, we developed parallel strategies for diﬀerent procedures in terms of workload decomposition and task scheduling. Thus, the workload across diﬀerent parallel processes can be balanced. The results suggest that our method can eﬀectively accelerate the process of polygon intersection. When addressing datasets with 1,409,020 groups of overlapping polygons, our method could reduce the total execution time from 987.82 to 53.66 s, thereby obtaining an optimal speedup ratio of 18.41 while consistently balancing the workloads. We also tested the eﬀect of task scheduling on the parallel eﬃciency, showing that reducing the total runtime is eﬀective, especially for a lower number of processes. Finally, the good scalability of the method is demonstrated.},
	language = {en},
	number = {12},
	urldate = {2019-06-03},
	journal = {International Journal of Geographical Information Science},
	author = {Zhou, Chen and Chen, Zhenjie and Li, Manchun},
	month = dec,
	year = {2018},
	note = {11},
	pages = {2402--2426},
	file = {Zhou et al. - 2018 - A parallel method to accelerate spatial operations.pdf:/home/and/Zotero/storage/TYQTHK6K/Zhou et al. - 2018 - A parallel method to accelerate spatial operations.pdf:application/pdf}
}

@inproceedings{magalhaes_fast_2015,
	title = {Fast exact parallel map overlay using a two-level uniform grid},
	isbn = {978-1-4503-3974-2},
	url = {http://dl.acm.org/citation.cfm?doid=2835185.2835188},
	doi = {10.1145/2835185.2835188},
	abstract = {We present EPUG-Overlay (Exact Parallel Uniform Grid Overlay), an algorithm to overlay two maps that is fast and parallel, has no roundoﬀ errors, and is freely available. EPUG-Overlay combines several novel aspects. It represents coordinates with rational numbers, thereby ensuring exact computations with no roundoﬀ errors and the ensuing sliver problems and topological impossibilities. For eﬃciency, EPUG-Overlay performs the map overlay in parallel, thereby utilizing the ubiquitous multicore architecture. Our application goes beyond merely using existing packages, which are ineﬃcient when used in parallel on large problems. Indeed, overlaying two maps with 53,000,000 edges and 730,000 faces took only 322 elapsed seconds (plus 116 seconds for I/O) on a dual 8-core 3.1 GHz Intel Xeon E5-2687 workstation. In contrast, GRASS, executing sequentially and generating roundoﬀ errors, takes 5300 seconds.},
	language = {en},
	urldate = {2019-06-03},
	publisher = {ACM Press},
	author = {Magalhães, Salles V. G. and Andrade, Marcus V. A. and Franklin, W. Randolph and Li, Wenli},
	year = {2015},
	note = {8},
	pages = {45--54},
	file = {Magalhães et al. - 2015 - Fast exact parallel map overlay using a two-level .pdf:/home/and/Zotero/storage/WG4YB9MK/Magalhães et al. - 2015 - Fast exact parallel map overlay using a two-level .pdf:application/pdf}
}

@incollection{goos_algorithms_1999,
	address = {Berlin, Heidelberg},
	title = {Algorithms for {Performing} {Polygonal} {Map} {Overlay} and {Spatial} {Join} on {Massive} {Data} {Sets}},
	volume = {1651},
	isbn = {978-3-540-66247-1 978-3-540-48482-0},
	url = {http://link.springer.com/10.1007/3-540-48482-5_17},
	abstract = {We consider the problem of performing polygonal map overlay and the reﬁnement step of spatial overlay joins. We show how to adapt algorithms from computational geometry to solve these problems for massive data sets. A performance study with artiﬁcial and real-world data sets helps to identify the algorithm that should be used for given input data.},
	language = {en},
	urldate = {2019-06-03},
	booktitle = {Advances in {Spatial} {Databases}},
	publisher = {Springer Berlin Heidelberg},
	author = {Becker, Ludger and Giesen, André and Hinrichs, Klaus H. and Vahrenhold, Jan},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Güting, Ralf Hartmut and Papadias, Dimitris and Lochovsky, Fred},
	year = {1999},
	note = {5},
	pages = {270--285},
	file = {Becker et al. - 1999 - Algorithms for Performing Polygonal Map Overlay an.pdf:/home/and/Zotero/storage/5HQD7RPD/Becker et al. - 1999 - Algorithms for Performing Polygonal Map Overlay an.pdf:application/pdf}
}

@inproceedings{puri_mapreduce_2013,
	address = {Washington, DC, USA},
	series = {{IPDPSW} '13},
	title = {{MapReduce} {Algorithms} for {GIS} {Polygonal} {Overlay} {Processing}},
	isbn = {978-0-7695-4979-8},
	url = {http://dx.doi.org/10.1109/IPDPSW.2013.254},
	doi = {10.1109/IPDPSW.2013.254},
	urldate = {2019-06-04},
	booktitle = {Proceedings of the 2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing} {Workshops} and {PhD} {Forum}},
	publisher = {IEEE Computer Society},
	author = {Puri, Satish and Agarwal, Dinesh and He, Xi and Prasad, Sushil K.},
	year = {2013},
	note = {9},
	pages = {1009--1016}
}

@inproceedings{sabek_spatial_2017,
	title = {On {Spatial} {Joins} in {MapReduce}},
	isbn = {978-1-4503-5490-5},
	url = {http://dl.acm.org/citation.cfm?doid=3139958.3139967},
	doi = {10.1145/3139958.3139967},
	abstract = {This paper provides the first attempt for a full-fledged query optimizer for MapReduce-based spatial join algorithms. The optimizer develops its own taxonomy that covers almost all possible ways of doing a spatial join for any two input datasets. The optimizer comes in two flavors; cost-based and rule-based. Given two input data sets, the cost-based query optimizer evaluates the costs of all possible options in the developed taxonomy, and selects the one with the lowest cost. The rule-based query optimizer abstracts the developed cost models of the cost-based optimizer into a set of simple easy-tocheck heuristic rules. Then, it applies its rules to select the lowest cost option. Both query optimizers are deployed and experimentally evaluated inside a widely used open-source MapReduce-based big spatial data system. Exhaustive experiments show that both query optimizers are always successful in taking the right decision for spatially joining any two datasets of up to 500GB each.},
	language = {en},
	urldate = {2019-06-05},
	publisher = {ACM Press},
	author = {Sabek, Ibrahim and Mokbel, Mohamed F.},
	year = {2017},
	note = {10},
	pages = {1--10},
	file = {Sabek and Mokbel - 2017 - On Spatial Joins in MapReduce.pdf:/home/and/Zotero/storage/TTJ5MKK5/Sabek and Mokbel - 2017 - On Spatial Joins in MapReduce.pdf:application/pdf}
}

@inproceedings{puri_efficient_2013,
	address = {Washington, DC, USA},
	series = {{IPDPSW} '13},
	title = {Efficient {Parallel} and {Distributed} {Algorithms} for {GIS} {Polygonal} {Overlay} {Processing}},
	isbn = {978-0-7695-4979-8},
	url = {http://dx.doi.org/10.1109/IPDPSW.2013.174},
	doi = {10.1109/IPDPSW.2013.174},
	abstract = {Polygon overlay is one of the complex operations in Geographic Information Systems (GIS). In GIS, a typical polygon tends to be large in size often consisting of thousands of vertices. Sequential algorithms for this problem are in abundance in literature and most of the parallel algorithms concentrate on parallelizing edge intersection phase only. Our research aims to develop parallel algorithms to find overlay for two input polygons which can be extended to handle multiple polygons and implement it on General Purpose Graphics Processing Units (GPGPU) which offers massive parallelism at relatively low cost. Moreover, spatial data files tend to be large in size (in GBs) and the underlying overlay computation is highly irregular and compute intensive. MapReduce paradigm is now standard in industry and academia for processing large-scale data. Motivated by MapReduce programming model, we propose to develop and implement scalable distributed algorithms to solve large-scale overlay processing in this dissertation.},
	urldate = {2019-06-05},
	booktitle = {Proceedings of the 2013 {IEEE} 27th {International} {Symposium} on {Parallel} and {Distributed} {Processing} {Workshops} and {PhD} {Forum}},
	publisher = {IEEE Computer Society},
	author = {Puri, Satish and Prasad, Sushil K.},
	year = {2013},
	note = {12},
	pages = {2238--2241}
}

@inproceedings{franklin_data_2018,
	title = {Data {Structures} for {Parallel} {Spatial} {Algorithms} on {Large} {Datasets} ({Vision} paper)},
	isbn = {978-1-4503-6041-8},
	url = {http://dl.acm.org/citation.cfm?doid=3282834.3282839},
	doi = {10.1145/3282834.3282839},
	abstract = {This paper describes data structures and algorithms for efficient implementation of GIS operations for large datasets on multicore Intel CPUs and on NVIDA GPUs. Typical operations are boolean combinations of polygons and map overlay. Efficient parallelization prefers simple regular data structures, such as structures of arrays of plain old datatypes. Warps of 32 threads are required to execute the same instruction (or be idle). Ideally, the data used by adjacent threads is adjacent in memory. Minimizing storage is important, as is accessing it in a regular pattern. That disparages pointers, linked lists, and trees. That implies that explicitly representing global topology is bad. If using only local topological formulae is sufficient, then it will be much faster. E.g., for many operations on a 2-D map (aka planar graph), the set of oriented edges suffices. Each edge knows the locations of its endpoints and the ids of its adjacent polygons. Any mass operation, such as area computation or point location, can be implemented as a map-reduce. All these techniques also apply in 3D to CAD/CAM and additive manufacturing. Indeed they are more important there.},
	language = {en},
	urldate = {2019-06-05},
	publisher = {ACM Press},
	author = {Franklin, W. Randolph and de Magalhães, Salles Viana Gomes and Andrade, Marcus Vinícius Alvim},
	year = {2018},
	note = {7},
	pages = {16--19},
	file = {Franklin et al. - 2018 - Data Structures for Parallel Spatial Algorithms on.pdf:/home/and/Zotero/storage/A7NCQKS3/Franklin et al. - 2018 - Data Structures for Parallel Spatial Algorithms on.pdf:application/pdf}
}

@inproceedings{challa_dd-rtree:_2016,
	title = {{DD}-{Rtree}: {A} dynamic distributed data structure for efficient data distribution among cluster nodes for spatial data mining algorithms},
	shorttitle = {{DD}-{Rtree}},
	doi = {10.1109/BigData.2016.7840586},
	abstract = {Parallelizing data mining algorithms has become a necessity as we try to mine ever increasing volumes of data. Spatial data mining algorithms like Dbscan, Optics, Slink, etc. have been parallelized to exploit a cluster infrastructure. The efficiency achieved by existing algorithms can be attributed to spatial locality preservation using spatial indexing structures like k-d-tree, quad-tree, grid files, etc. for distributing data among cluster nodes. However, these indexing structures are static in nature, i.e., they need to scan the entire dataset to determine the partitioning coordinates. This results in high data distribution cost when the data size is large. In this paper, we propose a dynamic distributed data structure, DD-Rtree, which preserves spatial locality while distributing data across compute nodes in a shared nothing environment. Moreover, DD-Rtree is dynamic, i.e., it can be constructed incrementally making it useful for handling big data. We compare the quality of data distribution achieved by DD-Rtree with one of the recent distributed indexing structure, SD-Rtree. We also compare the efficiency of queries supported by these indexing structures along with the overall efficiency of DBSCAN algorithm. Our experimental results show that DD-Rtree achieves better data distribution and thereby resulting in improved overall efficiency.},
	booktitle = {2016 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Challa, J. S. and Goyal, P. and Nikhil, S. and Mangla, A. and Balasubramaniam, S. S. and Goyal, N.},
	month = dec,
	year = {2016},
	note = {6},
	pages = {27--36},
	file = {IEEE Xplore Abstract Record:/home/and/Zotero/storage/MUNN8YKU/7840586.html:text/html;IEEE Xplore Full Text PDF:/home/and/Zotero/storage/I8T64XVB/Challa et al. - 2016 - DD-Rtree A dynamic distributed data structure for.pdf:application/pdf}
}

@article{muller_finding_1978,
	title = {Finding the intersection of two convex polyhedra},
	volume = {7},
	issn = {0304-3975},
	url = {http://www.sciencedirect.com/science/article/pii/0304397578900518},
	doi = {10.1016/0304-3975(78)90051-8},
	abstract = {Given two convex polyhedra in three-dimensional space, we develop an algorithm to (i) test whether their intersection is empty, and (ii) if so to find a separating plane, while (iii) if not to find a point in the intersection and explicitly construct their intersection polyhedron. The algorithm runs in timeO (n log n), where n is the sum of the numbers of vertices of the two polyhedra. The part of the algorithm concerned with (iii) (constructing the intersection) is based upon the fact that if a point in the intersection is known, then the entire intersection is obtained from the convex hull of suitable geometric duals of the two polyhedra taken with respect to this point.},
	number = {2},
	urldate = {2019-06-12},
	journal = {Theoretical Computer Science},
	author = {Muller, D. E. and Preparata, F. P.},
	month = jan,
	year = {1978},
	note = {0},
	pages = {217--236},
	file = {ScienceDirect Full Text PDF:/home/and/Zotero/storage/XZA3BRJX/Muller and Preparata - 1978 - Finding the intersection of two convex polyhedra.pdf:application/pdf;ScienceDirect Snapshot:/home/and/Zotero/storage/2GXICFHE/0304397578900518.html:text/html}
}

@techreport{freiseisen_colored_1998,
	title = {Colored {DCEL} for {Boolean} {Operations} in 2D},
	abstract = {Finding the intersection, union, or difference of two simple polygons are well known problems in computational geometry. In this paper, new algorithms and their implementation solving these problems are presented. Assuming that line segment intersection is done already, the common main idea is to build up a colored doubly connected edge list (DCEL), independently of the actual operation. Colors indicate the relationship between polygons and elements, i.e. every element covered by a polygon will get its color. In order to perform the respective boolean operation the DCEL is traversed for elements with a certain color. It will be shown that the DCEL can be constructed in O(n log(n)) time and using this constructed DCEL the boolean operations need O(n) time. The given implementation, which is part of the Cgal  1  library, is parameterized with traits classes that define geometric predicates, number types, and representation classes. This approach makes it easy to adapt the algorithms to o...},
	author = {Freiseisen, Wolfgang},
	year = {1998},
	note = {3},
	file = {Citeseer - Full Text PDF:/home/and/Zotero/storage/R9FEPKV6/Freiseisen - 1998 - Colored DCEL for Boolean Operations in 2D.pdf:application/pdf;Citeseer - Snapshot:/home/and/Zotero/storage/NXJSX8TM/summary.html:text/html}
}

@inproceedings{gang_luo_non-blocking_2002,
	title = {A non-blocking parallel spatial join algorithm},
	doi = {10.1109/ICDE.2002.994786},
	abstract = {Interest in incremental and adaptive query processing has led to the investigation of equijoin evaluation algorithms that are non-blocking. This investigation has yielded a number of algorithms, including the symmetric hash join, the XJoin, the Ripple Join, and their variants. However, to our knowledge no one has proposed a nonblocking spatial join algorithm. In this paper, we propose a parallel non-blocking spatial join algorithm that uses duplicate avoidance rather than duplicate elimination. Results from a prototype implementation in a commercial parallel object-relational DBMS show that it generates answer tuples steadily even in the presence of memory overflow, and that its rate of producing answer tuples scales with the number of processors. Also, when allowed to run to completion, its performance is comparable with the state-of-the-art blocking parallel spatial join algorithm.},
	booktitle = {Proceedings 18th {International} {Conference} on {Data} {Engineering}},
	author = {{Gang Luo} and Naughton, J. F. and Ellmann, C. J.},
	month = feb,
	year = {2002},
	keywords = {adaptive query processing, answer tuples, Concurrent computing, Data engineering, Data visualization, duplicate avoidance, equijoin evaluation algorithms, incremental query processing, memory overflow, nonblocking parallel spatial join algorithm, object-oriented databases, parallel algorithms, parallel databases, parallel object-relational DBMS, Prototypes, query processing, Query processing, relational databases},
	pages = {697--705},
	file = {IEEE Xplore Abstract Record:/home/and/Zotero/storage/BKXEIY4W/994786.html:text/html;IEEE Xplore Full Text PDF:/home/and/Zotero/storage/IDZVCKNH/Gang Luo et al. - 2002 - A non-blocking parallel spatial join algorithm.pdf:application/pdf}
}