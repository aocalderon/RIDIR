hdfs dfs -rm -f -r Census/S/
hdfs dfs -mkdir Census/S/
hdfs dfs -mkdir Census/S/AL/
hdfs dfs -put ~/Datasets/Census/AL/AL2000.wkt Census/S/AL/A.wkt
hdfs dfs -put ~/Datasets/Census/AL/AL2010.wkt Census/S/AL/B.wkt
./QuadPart -d Census/S/AL -p 511 -t 1e-3
./Perf -d Census/S/AL -p 511 -t 1e-3 -n 1
DATASET    = Census/S/AL
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 511
Run 1 ./sdcel2_debug Census/S/AL/P511 /home/acald013/RIDIR/local_path/Census/S/AL/P511/ 1e-3 "511_Census/S/AL_1e-3_1"
2022-03-21 08:29:50,496|15460|application_1639015019875_1402|15724|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AL/P511/edgesA --input2 Census/S/AL/P511/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AL/P511//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AL/P511//boundary.wkt --tolerance 1e-3 --qtag 511_Census/S/AL_1e-3_1 --debug --local
2022-03-21 08:29:50,602|15566|application_1639015019875_1402|INFO|scale=1000.0
2022-03-21 08:29:50,716|15680|Saved /tmp/edgesCells_511.wkt in 0.00s [1333 records].
2022-03-21 08:29:50,717|15681|application_1639015019875_1402|INFO|npartitions=1333
2022-03-21 08:29:50,717|15681|application_1639015019875_1402|221|TIME|start|511_Census/S/AL_1e-3_1
2022-03-21 08:30:04,042|29006|application_1639015019875_1402|INFO|nEdgesA=994131
2022-03-21 08:30:07,376|32340|application_1639015019875_1402|INFO|nEdgesB=1053567
2022-03-21 08:30:07,376|32340|application_1639015019875_1402|16659|TIME|read|511_Census/S/AL_1e-3_1
2022-03-21 08:30:10,466|35430|application_1639015019875_1402|3090|TIME|layer1S|511_Census/S/AL_1e-3_1
2022-03-21 08:30:12,599|37563|Saved /tmp/edgesFAC.wkt in 0.21s [5947 records].
2022-03-21 08:30:14,911|39875|application_1639015019875_1402|4445|TIME|layer2S|511_Census/S/AL_1e-3_1
2022-03-21 08:30:17,081|42045|Saved /tmp/edgesFBC.wkt in 0.20s [6189 records].
2022-03-21 08:30:53,932|78896|Saved /tmp/edgesS.wkt in 0.26s [15019 records].
2022-03-21 08:30:55,553|80517|application_1639015019875_1402|40642|TIME|overlayS|511_Census/S/AL_1e-3_1
2022-03-21 08:30:56,570|81534|Saved /tmp/edgesFE.wkt in 0.22s [2569 records].
2022-03-21 08:30:56,571|81535|application_1639015019875_1402|1018|TIME|end|511_Census/S/AL_1e-3_1
hdfs dfs -mkdir Census/S/AK/
hdfs dfs -put ~/Datasets/Census/AK/AK2000.wkt Census/S/AK/A.wkt
hdfs dfs -put ~/Datasets/Census/AK/AK2010.wkt Census/S/AK/B.wkt
./QuadPart -d Census/S/AK -p 144 -t 1e-3
./Perf -d Census/S/AK -p 144 -t 1e-3 -n 1
DATASET    = Census/S/AK
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 144
Run 1 ./sdcel2_debug Census/S/AK/P144 /home/acald013/RIDIR/local_path/Census/S/AK/P144/ 1e-3 "144_Census/S/AK_1e-3_1"
2022-03-21 08:31:14,169|13614|application_1639015019875_1403|13871|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AK/P144/edgesA --input2 Census/S/AK/P144/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AK/P144//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AK/P144//boundary.wkt --tolerance 1e-3 --qtag 144_Census/S/AK_1e-3_1 --debug --local
2022-03-21 08:31:14,262|13707|application_1639015019875_1403|INFO|scale=1000.0
2022-03-21 08:31:14,340|13785|Saved /tmp/edgesCells_144.wkt in 0.00s [532 records].
2022-03-21 08:31:14,340|13785|application_1639015019875_1403|INFO|npartitions=532
2022-03-21 08:31:14,341|13786|application_1639015019875_1403|172|TIME|start|144_Census/S/AK_1e-3_1
2022-03-21 08:31:25,789|25234|application_1639015019875_1403|INFO|nEdgesA=296525
2022-03-21 08:31:27,947|27392|application_1639015019875_1403|INFO|nEdgesB=297275
2022-03-21 08:31:27,947|27392|application_1639015019875_1403|13606|TIME|read|144_Census/S/AK_1e-3_1
2022-03-21 08:31:30,107|29552|application_1639015019875_1403|2160|TIME|layer1S|144_Census/S/AK_1e-3_1
2022-03-21 08:31:31,114|30559|Saved /tmp/edgesFAC.wkt in 0.07s [1569 records].
2022-03-21 08:31:32,456|31901|application_1639015019875_1403|2349|TIME|layer2S|144_Census/S/AK_1e-3_1
2022-03-21 08:31:33,411|32856|Saved /tmp/edgesFBC.wkt in 0.11s [1555 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/AZ/
hdfs dfs -put ~/Datasets/Census/AZ/AZ2000.wkt Census/S/AZ/A.wkt
hdfs dfs -put ~/Datasets/Census/AZ/AZ2010.wkt Census/S/AZ/B.wkt
./QuadPart -d Census/S/AZ -p 364 -t 1e-3
./Perf -d Census/S/AZ -p 364 -t 1e-3 -n 1
DATASET    = Census/S/AZ
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 364
Run 1 ./sdcel2_debug Census/S/AZ/P364 /home/acald013/RIDIR/local_path/Census/S/AZ/P364/ 1e-3 "364_Census/S/AZ_1e-3_1"
2022-03-21 08:32:34,673|13623|application_1639015019875_1404|13880|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AZ/P364/edgesA --input2 Census/S/AZ/P364/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AZ/P364//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AZ/P364//boundary.wkt --tolerance 1e-3 --qtag 364_Census/S/AZ_1e-3_1 --debug --local
2022-03-21 08:32:34,784|13734|application_1639015019875_1404|INFO|scale=1000.0
2022-03-21 08:32:34,898|13848|Saved /tmp/edgesCells_364.wkt in 0.00s [1051 records].
2022-03-21 08:32:34,898|13848|application_1639015019875_1404|INFO|npartitions=1051
2022-03-21 08:32:34,898|13848|application_1639015019875_1404|225|TIME|start|364_Census/S/AZ_1e-3_1
2022-03-21 08:32:47,508|26458|application_1639015019875_1404|INFO|nEdgesA=668973
2022-03-21 08:32:50,359|29309|application_1639015019875_1404|INFO|nEdgesB=748853
2022-03-21 08:32:50,359|29309|application_1639015019875_1404|15461|TIME|read|364_Census/S/AZ_1e-3_1
2022-03-21 08:32:53,097|32047|application_1639015019875_1404|2738|TIME|layer1S|364_Census/S/AZ_1e-3_1
2022-03-21 08:32:54,512|33462|Saved /tmp/edgesFAC.wkt in 0.14s [4498 records].
2022-03-21 08:32:56,528|35478|application_1639015019875_1404|3431|TIME|layer2S|364_Census/S/AZ_1e-3_1
2022-03-21 08:32:58,188|37138|Saved /tmp/edgesFBC.wkt in 0.21s [5195 records].
2022-03-21 08:33:33,514|72464|Saved /tmp/edgesS.wkt in 0.20s [10960 records].
2022-03-21 08:33:35,068|74018|application_1639015019875_1404|38540|TIME|overlayS|364_Census/S/AZ_1e-3_1
2022-03-21 08:33:35,867|74817|Saved /tmp/edgesFE.wkt in 0.18s [1971 records].
2022-03-21 08:33:35,867|74817|application_1639015019875_1404|799|TIME|end|364_Census/S/AZ_1e-3_1
hdfs dfs -mkdir Census/S/AR/
hdfs dfs -put ~/Datasets/Census/AR/AR2000.wkt Census/S/AR/A.wkt
hdfs dfs -put ~/Datasets/Census/AR/AR2010.wkt Census/S/AR/B.wkt
./QuadPart -d Census/S/AR -p 429 -t 1e-3
./Perf -d Census/S/AR -p 429 -t 1e-3 -n 1
DATASET    = Census/S/AR
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 429
Run 1 ./sdcel2_debug Census/S/AR/P429 /home/acald013/RIDIR/local_path/Census/S/AR/P429/ 1e-3 "429_Census/S/AR_1e-3_1"
2022-03-21 08:33:54,208|13915|application_1639015019875_1405|14164|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AR/P429/edgesA --input2 Census/S/AR/P429/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AR/P429//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AR/P429//boundary.wkt --tolerance 1e-3 --qtag 429_Census/S/AR_1e-3_1 --debug --local
2022-03-21 08:33:54,305|14012|application_1639015019875_1405|INFO|scale=1000.0
2022-03-21 08:33:54,405|14112|Saved /tmp/edgesCells_429.wkt in 0.00s [1135 records].
2022-03-21 08:33:54,406|14113|application_1639015019875_1405|INFO|npartitions=1135
2022-03-21 08:33:54,406|14113|application_1639015019875_1405|198|TIME|start|429_Census/S/AR_1e-3_1
2022-03-21 08:34:06,975|26682|application_1639015019875_1405|INFO|nEdgesA=859695
2022-03-21 08:34:09,825|29532|application_1639015019875_1405|INFO|nEdgesB=884111
2022-03-21 08:34:09,825|29532|application_1639015019875_1405|15419|TIME|read|429_Census/S/AR_1e-3_1
2022-03-21 08:34:12,634|32341|application_1639015019875_1405|2809|TIME|layer1S|429_Census/S/AR_1e-3_1
2022-03-21 08:34:14,471|34178|Saved /tmp/edgesFAC.wkt in 0.34s [4244 records].
2022-03-21 08:34:16,464|36171|application_1639015019875_1405|3830|TIME|layer2S|429_Census/S/AR_1e-3_1
2022-03-21 08:34:18,258|37965|Saved /tmp/edgesFBC.wkt in 0.18s [4375 records].
2022-03-21 08:34:51,594|71301|Saved /tmp/edgesS.wkt in 0.21s [9568 records].
2022-03-21 08:34:53,157|72864|application_1639015019875_1405|36693|TIME|overlayS|429_Census/S/AR_1e-3_1
2022-03-21 08:34:54,067|73774|Saved /tmp/edgesFE.wkt in 0.27s [912 records].
2022-03-21 08:34:54,067|73774|application_1639015019875_1405|910|TIME|end|429_Census/S/AR_1e-3_1
hdfs dfs -mkdir Census/S/CA/
hdfs dfs -put ~/Datasets/Census/CA/CA2000.wkt Census/S/CA/A.wkt
hdfs dfs -put ~/Datasets/Census/CA/CA2010.wkt Census/S/CA/B.wkt
./QuadPart -d Census/S/CA -p 1432 -t 1e-3
./Perf -d Census/S/CA -p 1432 -t 1e-3 -n 1
DATASET    = Census/S/CA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 1432
Run 1 ./sdcel2_debug Census/S/CA/P1432 /home/acald013/RIDIR/local_path/Census/S/CA/P1432/ 1e-3 "1432_Census/S/CA_1e-3_1"
2022-03-21 08:35:13,121|13337|application_1639015019875_1406|13582|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/CA/P1432/edgesA --input2 Census/S/CA/P1432/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/CA/P1432//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/CA/P1432//boundary.wkt --tolerance 1e-3 --qtag 1432_Census/S/CA_1e-3_1 --debug --local
2022-03-21 08:35:13,265|13481|application_1639015019875_1406|INFO|scale=1000.0
2022-03-21 08:35:13,500|13716|Saved /tmp/edgesCells_1432.wkt in 0.01s [3649 records].
2022-03-21 08:35:13,501|13717|application_1639015019875_1406|INFO|npartitions=3649
2022-03-21 08:35:13,501|13717|application_1639015019875_1406|380|TIME|start|1432_Census/S/CA_1e-3_1
2022-03-21 08:35:32,060|32276|application_1639015019875_1406|INFO|nEdgesA=2735505
2022-03-21 08:35:40,391|40607|application_1639015019875_1406|INFO|nEdgesB=2942658
2022-03-21 08:35:40,392|40608|application_1639015019875_1406|26891|TIME|read|1432_Census/S/CA_1e-3_1
2022-03-21 08:35:46,885|47101|application_1639015019875_1406|6493|TIME|layer1S|1432_Census/S/CA_1e-3_1
2022-03-21 08:35:52,706|52922|Saved /tmp/edgesFAC.wkt in 0.64s [22623 records].
2022-03-21 08:35:59,319|59535|application_1639015019875_1406|12434|TIME|layer2S|1432_Census/S/CA_1e-3_1
2022-03-21 08:36:06,001|66217|Saved /tmp/edgesFBC.wkt in 0.79s [24226 records].
2022-03-21 08:37:09,210|129426|Saved /tmp/edgesS.wkt in 0.73s [53956 records].
2022-03-21 08:37:14,233|134449|application_1639015019875_1406|74914|TIME|overlayS|1432_Census/S/CA_1e-3_1
2022-03-21 08:37:16,601|136817|Saved /tmp/edgesFE.wkt in 0.67s [12436 records].
2022-03-21 08:37:16,602|136818|application_1639015019875_1406|2369|TIME|end|1432_Census/S/CA_1e-3_1
hdfs dfs -mkdir Census/S/CO/
hdfs dfs -put ~/Datasets/Census/CO/CO2000.wkt Census/S/CO/A.wkt
hdfs dfs -put ~/Datasets/Census/CO/CO2010.wkt Census/S/CO/B.wkt
./QuadPart -d Census/S/CO -p 367 -t 1e-3
./Perf -d Census/S/CO -p 367 -t 1e-3 -n 1
DATASET    = Census/S/CO
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 367
Run 1 ./sdcel2_debug Census/S/CO/P367 /home/acald013/RIDIR/local_path/Census/S/CO/P367/ 1e-3 "367_Census/S/CO_1e-3_1"
2022-03-21 08:37:35,846|13547|application_1639015019875_1407|13790|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/CO/P367/edgesA --input2 Census/S/CO/P367/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/CO/P367//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/CO/P367//boundary.wkt --tolerance 1e-3 --qtag 367_Census/S/CO_1e-3_1 --debug --local
2022-03-21 08:37:35,938|13639|application_1639015019875_1407|INFO|scale=1000.0
2022-03-21 08:37:36,037|13738|Saved /tmp/edgesCells_367.wkt in 0.00s [988 records].
2022-03-21 08:37:36,037|13738|application_1639015019875_1407|INFO|npartitions=988
2022-03-21 08:37:36,037|13738|application_1639015019875_1407|191|TIME|start|367_Census/S/CO_1e-3_1
2022-03-21 08:37:48,502|26203|application_1639015019875_1407|INFO|nEdgesA=718108
2022-03-21 08:37:51,626|29327|application_1639015019875_1407|INFO|nEdgesB=755838
2022-03-21 08:37:51,626|29327|application_1639015019875_1407|15589|TIME|read|367_Census/S/CO_1e-3_1
2022-03-21 08:37:54,495|32196|application_1639015019875_1407|2869|TIME|layer1S|367_Census/S/CO_1e-3_1
2022-03-21 08:37:56,556|34257|Saved /tmp/edgesFAC.wkt in 0.36s [4523 records].
2022-03-21 08:37:58,464|36165|application_1639015019875_1407|3969|TIME|layer2S|367_Census/S/CO_1e-3_1
2022-03-21 08:38:00,274|37975|Saved /tmp/edgesFBC.wkt in 0.28s [4895 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /hadoop/yarn/local/usercache/acald013/appcache/application_1639015019875_1407/blockmgr-d1fcb24e-92b8-4741-a6dd-6aa6c72b7cc6/0a/shuffle_0_44_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:459) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/acald013/appcache/application_1639015019875_1407/blockmgr-d1fcb24e-92b8-4741-a6dd-6aa6c72b7cc6/0a/shuffle_0_44_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:205) 	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:377) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:329) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:364) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:154) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	... 44 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/CT/
hdfs dfs -put ~/Datasets/Census/CT/CT2000.wkt Census/S/CT/A.wkt
hdfs dfs -put ~/Datasets/Census/CT/CT2010.wkt Census/S/CT/B.wkt
./QuadPart -d Census/S/CT -p 127 -t 1e-3
./Perf -d Census/S/CT -p 127 -t 1e-3 -n 1
DATASET    = Census/S/CT
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 127
Run 1 ./sdcel2_debug Census/S/CT/P127 /home/acald013/RIDIR/local_path/Census/S/CT/P127/ 1e-3 "127_Census/S/CT_1e-3_1"
2022-03-21 08:38:55,997|13408|application_1639015019875_1408|13650|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/CT/P127/edgesA --input2 Census/S/CT/P127/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/CT/P127//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/CT/P127//boundary.wkt --tolerance 1e-3 --qtag 127_Census/S/CT_1e-3_1 --debug --local
2022-03-21 08:38:56,071|13482|application_1639015019875_1408|INFO|scale=1000.0
2022-03-21 08:38:56,132|13543|Saved /tmp/edgesCells_127.wkt in 0.00s [328 records].
2022-03-21 08:38:56,132|13543|application_1639015019875_1408|INFO|npartitions=328
2022-03-21 08:38:56,132|13543|application_1639015019875_1408|135|TIME|start|127_Census/S/CT_1e-3_1
2022-03-21 08:39:06,828|24239|application_1639015019875_1408|INFO|nEdgesA=261763
2022-03-21 08:39:08,384|25795|application_1639015019875_1408|INFO|nEdgesB=263192
2022-03-21 08:39:08,384|25795|application_1639015019875_1408|12252|TIME|read|127_Census/S/CT_1e-3_1
2022-03-21 08:39:09,906|27317|application_1639015019875_1408|1522|TIME|layer1S|127_Census/S/CT_1e-3_1
2022-03-21 08:39:10,807|28218|Saved /tmp/edgesFAC.wkt in 0.06s [2568 records].
2022-03-21 08:39:11,690|29101|application_1639015019875_1408|1784|TIME|layer2S|127_Census/S/CT_1e-3_1
2022-03-21 08:39:12,520|29931|Saved /tmp/edgesFBC.wkt in 0.05s [2591 records].
2022-03-21 08:39:32,164|49575|Saved /tmp/edgesS.wkt in 0.08s [5778 records].
2022-03-21 08:39:33,609|51020|application_1639015019875_1408|21919|TIME|overlayS|127_Census/S/CT_1e-3_1
2022-03-21 08:39:33,966|51377|Saved /tmp/edgesFE.wkt in 0.07s [1188 records].
2022-03-21 08:39:33,966|51377|application_1639015019875_1408|357|TIME|end|127_Census/S/CT_1e-3_1
hdfs dfs -mkdir Census/S/DE/
hdfs dfs -put ~/Datasets/Census/DE/DE2000.wkt Census/S/DE/A.wkt
hdfs dfs -put ~/Datasets/Census/DE/DE2010.wkt Census/S/DE/B.wkt
./QuadPart -d Census/S/DE -p 49 -t 1e-3
./Perf -d Census/S/DE -p 49 -t 1e-3 -n 1
DATASET    = Census/S/DE
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 49
Run 1 ./sdcel2_debug Census/S/DE/P49 /home/acald013/RIDIR/local_path/Census/S/DE/P49/ 1e-3 "49_Census/S/DE_1e-3_1"
2022-03-21 08:39:52,240|13589|application_1639015019875_1409|13846|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/DE/P49/edgesA --input2 Census/S/DE/P49/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/DE/P49//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/DE/P49//boundary.wkt --tolerance 1e-3 --qtag 49_Census/S/DE_1e-3_1 --debug --local
2022-03-21 08:39:52,297|13646|application_1639015019875_1409|INFO|scale=1000.0
2022-03-21 08:39:52,335|13684|Saved /tmp/edgesCells_49.wkt in 0.00s [133 records].
2022-03-21 08:39:52,336|13685|application_1639015019875_1409|INFO|npartitions=133
2022-03-21 08:39:52,336|13685|application_1639015019875_1409|96|TIME|start|49_Census/S/DE_1e-3_1
2022-03-21 08:40:02,125|23474|application_1639015019875_1409|INFO|nEdgesA=89835
2022-03-21 08:40:03,122|24471|application_1639015019875_1409|INFO|nEdgesB=101279
2022-03-21 08:40:03,122|24471|application_1639015019875_1409|10786|TIME|read|49_Census/S/DE_1e-3_1
2022-03-21 08:40:04,400|25749|application_1639015019875_1409|1278|TIME|layer1S|49_Census/S/DE_1e-3_1
2022-03-21 08:40:04,960|26309|Saved /tmp/edgesFAC.wkt in 0.02s [717 records].
2022-03-21 08:40:05,626|26975|application_1639015019875_1409|1226|TIME|layer2S|49_Census/S/DE_1e-3_1
2022-03-21 08:40:06,227|27576|Saved /tmp/edgesFBC.wkt in 0.02s [785 records].
2022-03-21 08:40:25,717|47066|Saved /tmp/edgesS.wkt in 0.03s [1797 records].
2022-03-21 08:40:27,154|48503|application_1639015019875_1409|21528|TIME|overlayS|49_Census/S/DE_1e-3_1
2022-03-21 08:40:27,357|48706|Saved /tmp/edgesFE.wkt in 0.03s [316 records].
2022-03-21 08:40:27,357|48706|application_1639015019875_1409|203|TIME|end|49_Census/S/DE_1e-3_1
hdfs dfs -mkdir Census/S/DC/
hdfs dfs -put ~/Datasets/Census/DC/DC2000.wkt Census/S/DC/A.wkt
hdfs dfs -put ~/Datasets/Census/DC/DC2010.wkt Census/S/DC/B.wkt
./QuadPart -d Census/S/DC -p 20 -t 1e-3
./Perf -d Census/S/DC -p 20 -t 1e-3 -n 1
DATASET    = Census/S/DC
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 20
Run 1 ./sdcel2_debug Census/S/DC/P20 /home/acald013/RIDIR/local_path/Census/S/DC/P20/ 1e-3 "20_Census/S/DC_1e-3_1"
2022-03-21 08:40:44,790|13757|application_1639015019875_1410|14012|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/DC/P20/edgesA --input2 Census/S/DC/P20/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/DC/P20//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/DC/P20//boundary.wkt --tolerance 1e-3 --qtag 20_Census/S/DC_1e-3_1 --debug --local
2022-03-21 08:40:44,847|13814|application_1639015019875_1410|INFO|scale=1000.0
2022-03-21 08:40:44,870|13837|Saved /tmp/edgesCells_20.wkt in 0.00s [55 records].
2022-03-21 08:40:44,871|13838|application_1639015019875_1410|INFO|npartitions=55
2022-03-21 08:40:44,871|13838|application_1639015019875_1410|81|TIME|start|20_Census/S/DC_1e-3_1
2022-03-21 08:40:54,307|23274|application_1639015019875_1410|INFO|nEdgesA=41817
2022-03-21 08:40:55,113|24080|application_1639015019875_1410|INFO|nEdgesB=41633
2022-03-21 08:40:55,113|24080|application_1639015019875_1410|10242|TIME|read|20_Census/S/DC_1e-3_1
2022-03-21 08:40:56,254|25221|application_1639015019875_1410|1141|TIME|layer1S|20_Census/S/DC_1e-3_1
2022-03-21 08:40:56,700|25667|Saved /tmp/edgesFAC.wkt in 0.01s [472 records].
2022-03-21 08:40:57,394|26361|application_1639015019875_1410|1140|TIME|layer2S|20_Census/S/DC_1e-3_1
2022-03-21 08:40:57,765|26732|Saved /tmp/edgesFBC.wkt in 0.01s [459 records].
2022-03-21 08:40:59,688|28655|Saved /tmp/edgesS.wkt in 0.01s [938 records].
2022-03-21 08:41:00,113|29080|application_1639015019875_1410|2719|TIME|overlayS|20_Census/S/DC_1e-3_1
2022-03-21 08:41:00,198|29165|Saved /tmp/edgesFE.wkt in 0.01s [200 records].
2022-03-21 08:41:00,198|29165|application_1639015019875_1410|85|TIME|end|20_Census/S/DC_1e-3_1
hdfs dfs -mkdir Census/S/FL/
hdfs dfs -put ~/Datasets/Census/FL/FL2000.wkt Census/S/FL/A.wkt
hdfs dfs -put ~/Datasets/Census/FL/FL2010.wkt Census/S/FL/B.wkt
./QuadPart -d Census/S/FL -p 598 -t 1e-3
./Perf -d Census/S/FL -p 598 -t 1e-3 -n 1
DATASET    = Census/S/FL
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 598
Run 1 ./sdcel2_debug Census/S/FL/P598 /home/acald013/RIDIR/local_path/Census/S/FL/P598/ 1e-3 "598_Census/S/FL_1e-3_1"
2022-03-21 08:41:18,184|13436|application_1639015019875_1411|13686|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/FL/P598/edgesA --input2 Census/S/FL/P598/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/FL/P598//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/FL/P598//boundary.wkt --tolerance 1e-3 --qtag 598_Census/S/FL_1e-3_1 --debug --local
2022-03-21 08:41:18,306|13558|application_1639015019875_1411|INFO|scale=1000.0
2022-03-21 08:41:18,434|13686|Saved /tmp/edgesCells_598.wkt in 0.00s [1600 records].
2022-03-21 08:41:18,434|13686|application_1639015019875_1411|INFO|npartitions=1600
2022-03-21 08:41:18,435|13687|application_1639015019875_1411|251|TIME|start|598_Census/S/FL_1e-3_1
2022-03-21 08:41:32,943|28195|application_1639015019875_1411|INFO|nEdgesA=1097692
2022-03-21 08:41:37,428|32680|application_1639015019875_1411|INFO|nEdgesB=1229077
2022-03-21 08:41:37,429|32681|application_1639015019875_1411|18993|TIME|read|598_Census/S/FL_1e-3_1
2022-03-21 08:41:41,824|37076|application_1639015019875_1411|4396|TIME|layer1S|598_Census/S/FL_1e-3_1
2022-03-21 08:41:44,599|39851|Saved /tmp/edgesFAC.wkt in 0.26s [10234 records].
2022-03-21 08:41:46,935|42187|application_1639015019875_1411|5111|TIME|layer2S|598_Census/S/FL_1e-3_1
2022-03-21 08:41:49,613|44865|Saved /tmp/edgesFBC.wkt in 0.27s [12138 records].
2022-03-21 08:42:24,281|79533|Saved /tmp/edgesS.wkt in 0.39s [26857 records].
2022-03-21 08:42:27,507|82759|application_1639015019875_1411|40572|TIME|overlayS|598_Census/S/FL_1e-3_1
2022-03-21 08:42:28,712|83964|Saved /tmp/edgesFE.wkt in 0.35s [6245 records].
2022-03-21 08:42:28,712|83964|application_1639015019875_1411|1205|TIME|end|598_Census/S/FL_1e-3_1
hdfs dfs -mkdir Census/S/GA/
hdfs dfs -put ~/Datasets/Census/GA/GA2000.wkt Census/S/GA/A.wkt
hdfs dfs -put ~/Datasets/Census/GA/GA2010.wkt Census/S/GA/B.wkt
./QuadPart -d Census/S/GA -p 653 -t 1e-3
./Perf -d Census/S/GA -p 653 -t 1e-3 -n 1
DATASET    = Census/S/GA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 653
Run 1 ./sdcel2_debug Census/S/GA/P653 /home/acald013/RIDIR/local_path/Census/S/GA/P653/ 1e-3 "653_Census/S/GA_1e-3_1"
2022-03-21 08:42:46,895|13472|application_1639015019875_1412|13722|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/GA/P653/edgesA --input2 Census/S/GA/P653/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/GA/P653//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/GA/P653//boundary.wkt --tolerance 1e-3 --qtag 653_Census/S/GA_1e-3_1 --debug --local
2022-03-21 08:42:47,009|13586|application_1639015019875_1412|INFO|scale=1000.0
2022-03-21 08:42:47,125|13702|Saved /tmp/edgesCells_653.wkt in 0.00s [1663 records].
2022-03-21 08:42:47,125|13702|application_1639015019875_1412|INFO|npartitions=1663
2022-03-21 08:42:47,126|13703|application_1639015019875_1412|231|TIME|start|653_Census/S/GA_1e-3_1
2022-03-21 08:43:01,165|27742|application_1639015019875_1412|INFO|nEdgesA=1259077
2022-03-21 08:43:05,404|31981|application_1639015019875_1412|INFO|nEdgesB=1344407
2022-03-21 08:43:05,404|31981|application_1639015019875_1412|18278|TIME|read|653_Census/S/GA_1e-3_1
2022-03-21 08:43:09,028|35605|application_1639015019875_1412|3624|TIME|layer1S|653_Census/S/GA_1e-3_1
2022-03-21 08:43:11,149|37726|Saved /tmp/edgesFAC.wkt in 0.22s [7843 records].
2022-03-21 08:43:13,838|40415|application_1639015019875_1412|4810|TIME|layer2S|653_Census/S/GA_1e-3_1
2022-03-21 08:43:16,682|43259|Saved /tmp/edgesFBC.wkt in 0.45s [8596 records].
2022-03-21 08:43:51,732|78309|Saved /tmp/edgesS.wkt in 0.30s [19981 records].
2022-03-21 08:43:54,146|80723|application_1639015019875_1412|40308|TIME|overlayS|653_Census/S/GA_1e-3_1
2022-03-21 08:43:55,338|81915|Saved /tmp/edgesFE.wkt in 0.28s [3424 records].
2022-03-21 08:43:55,338|81915|application_1639015019875_1412|1192|TIME|end|653_Census/S/GA_1e-3_1
hdfs dfs -mkdir Census/S/HI/
hdfs dfs -put ~/Datasets/Census/HI/HI2000.wkt Census/S/HI/A.wkt
hdfs dfs -put ~/Datasets/Census/HI/HI2010.wkt Census/S/HI/B.wkt
./QuadPart -d Census/S/HI -p 70 -t 1e-3
./Perf -d Census/S/HI -p 70 -t 1e-3 -n 1
DATASET    = Census/S/HI
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 70
Run 1 ./sdcel2_debug Census/S/HI/P70 /home/acald013/RIDIR/local_path/Census/S/HI/P70/ 1e-3 "70_Census/S/HI_1e-3_1"
2022-03-21 08:44:13,551|14072|application_1639015019875_1413|14350|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/HI/P70/edgesA --input2 Census/S/HI/P70/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/HI/P70//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/HI/P70//boundary.wkt --tolerance 1e-3 --qtag 70_Census/S/HI_1e-3_1 --debug --local
2022-03-21 08:44:13,612|14133|application_1639015019875_1413|INFO|scale=1000.0
2022-03-21 08:44:13,661|14182|Saved /tmp/edgesCells_70.wkt in 0.00s [232 records].
2022-03-21 08:44:13,662|14183|application_1639015019875_1413|INFO|npartitions=232
2022-03-21 08:44:13,662|14183|application_1639015019875_1413|111|TIME|start|70_Census/S/HI_1e-3_1
2022-03-21 08:44:23,993|24514|application_1639015019875_1413|INFO|nEdgesA=126405
2022-03-21 08:44:25,357|25878|application_1639015019875_1413|INFO|nEdgesB=144457
2022-03-21 08:44:25,357|25878|application_1639015019875_1413|11695|TIME|read|70_Census/S/HI_1e-3_1
2022-03-21 08:44:27,083|27604|application_1639015019875_1413|1726|TIME|layer1S|70_Census/S/HI_1e-3_1
2022-03-21 08:44:27,817|28338|Saved /tmp/edgesFAC.wkt in 0.16s [1142 records].
2022-03-21 08:44:28,659|29180|application_1639015019875_1413|1576|TIME|layer2S|70_Census/S/HI_1e-3_1
2022-03-21 08:44:29,195|29716|Saved /tmp/edgesFBC.wkt in 0.04s [1304 records].
2022-03-21 08:44:47,129|47650|Saved /tmp/edgesS.wkt in 0.06s [3248 records].
2022-03-21 08:44:48,659|49180|application_1639015019875_1413|20000|TIME|overlayS|70_Census/S/HI_1e-3_1
2022-03-21 08:44:48,956|49477|Saved /tmp/edgesFE.wkt in 0.08s [648 records].
2022-03-21 08:44:48,956|49477|application_1639015019875_1413|297|TIME|end|70_Census/S/HI_1e-3_1
hdfs dfs -mkdir Census/S/ID/
hdfs dfs -put ~/Datasets/Census/ID/ID2000.wkt Census/S/ID/A.wkt
hdfs dfs -put ~/Datasets/Census/ID/ID2010.wkt Census/S/ID/B.wkt
./QuadPart -d Census/S/ID -p 197 -t 1e-3
./Perf -d Census/S/ID -p 197 -t 1e-3 -n 1
DATASET    = Census/S/ID
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 197
Run 1 ./sdcel2_debug Census/S/ID/P197 /home/acald013/RIDIR/local_path/Census/S/ID/P197/ 1e-3 "197_Census/S/ID_1e-3_1"
2022-03-21 08:45:06,591|13535|application_1639015019875_1414|13782|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/ID/P197/edgesA --input2 Census/S/ID/P197/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/ID/P197//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/ID/P197//boundary.wkt --tolerance 1e-3 --qtag 197_Census/S/ID_1e-3_1 --debug --local
2022-03-21 08:45:06,669|13613|application_1639015019875_1414|INFO|scale=1000.0
2022-03-21 08:45:06,745|13689|Saved /tmp/edgesCells_197.wkt in 0.00s [553 records].
2022-03-21 08:45:06,745|13689|application_1639015019875_1414|INFO|npartitions=553
2022-03-21 08:45:06,745|13689|application_1639015019875_1414|154|TIME|start|197_Census/S/ID_1e-3_1
2022-03-21 08:45:18,026|24970|application_1639015019875_1414|INFO|nEdgesA=411041
2022-03-21 08:45:20,560|27504|application_1639015019875_1414|INFO|nEdgesB=406301
2022-03-21 08:45:20,561|27505|application_1639015019875_1414|13816|TIME|read|197_Census/S/ID_1e-3_1
2022-03-21 08:45:22,490|29434|application_1639015019875_1414|1929|TIME|layer1S|197_Census/S/ID_1e-3_1
2022-03-21 08:45:23,454|30398|Saved /tmp/edgesFAC.wkt in 0.08s [1775 records].
2022-03-21 08:45:25,127|32071|application_1639015019875_1414|2637|TIME|layer2S|197_Census/S/ID_1e-3_1
2022-03-21 08:45:26,189|33133|Saved /tmp/edgesFBC.wkt in 0.07s [1803 records].
2022-03-21 08:46:04,603|71547|Saved /tmp/edgesS.wkt in 0.10s [3837 records].
2022-03-21 08:46:05,659|72603|application_1639015019875_1414|40532|TIME|overlayS|197_Census/S/ID_1e-3_1
2022-03-21 08:46:06,256|73200|Saved /tmp/edgesFE.wkt in 0.10s [419 records].
2022-03-21 08:46:06,257|73201|application_1639015019875_1414|598|TIME|end|197_Census/S/ID_1e-3_1
hdfs dfs -mkdir Census/S/IL/
hdfs dfs -put ~/Datasets/Census/IL/IL2000.wkt Census/S/IL/A.wkt
hdfs dfs -put ~/Datasets/Census/IL/IL2010.wkt Census/S/IL/B.wkt
./QuadPart -d Census/S/IL -p 501 -t 1e-3
./Perf -d Census/S/IL -p 501 -t 1e-3 -n 1
DATASET    = Census/S/IL
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 501
Run 1 ./sdcel2_debug Census/S/IL/P501 /home/acald013/RIDIR/local_path/Census/S/IL/P501/ 1e-3 "501_Census/S/IL_1e-3_1"
2022-03-21 08:46:25,173|13609|application_1639015019875_1415|13854|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/IL/P501/edgesA --input2 Census/S/IL/P501/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/IL/P501//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/IL/P501//boundary.wkt --tolerance 1e-3 --qtag 501_Census/S/IL_1e-3_1 --debug --local
2022-03-21 08:46:25,280|13716|application_1639015019875_1415|INFO|scale=1000.0
2022-03-21 08:46:25,395|13831|Saved /tmp/edgesCells_501.wkt in 0.00s [1282 records].
2022-03-21 08:46:25,395|13831|application_1639015019875_1415|INFO|npartitions=1282
2022-03-21 08:46:25,396|13832|application_1639015019875_1415|223|TIME|start|501_Census/S/IL_1e-3_1
2022-03-21 08:46:38,837|27273|application_1639015019875_1415|INFO|nEdgesA=1005784
2022-03-21 08:46:42,118|30554|application_1639015019875_1415|INFO|nEdgesB=1031873
2022-03-21 08:46:42,118|30554|application_1639015019875_1415|16722|TIME|read|501_Census/S/IL_1e-3_1
2022-03-21 08:46:44,898|33334|application_1639015019875_1415|2780|TIME|layer1S|501_Census/S/IL_1e-3_1
2022-03-21 08:46:47,105|35541|Saved /tmp/edgesFAC.wkt in 0.24s [9259 records].
2022-03-21 08:46:48,788|37224|application_1639015019875_1415|3890|TIME|layer2S|501_Census/S/IL_1e-3_1
2022-03-21 08:46:50,822|39258|Saved /tmp/edgesFBC.wkt in 0.25s [9576 records].
2022-03-21 08:47:16,206|64642|Saved /tmp/edgesS.wkt in 0.27s [20723 records].
2022-03-21 08:47:17,934|66370|application_1639015019875_1415|29146|TIME|overlayS|501_Census/S/IL_1e-3_1
2022-03-21 08:47:19,287|67723|Saved /tmp/edgesFE.wkt in 0.26s [4081 records].
2022-03-21 08:47:19,288|67724|application_1639015019875_1415|1354|TIME|end|501_Census/S/IL_1e-3_1
hdfs dfs -mkdir Census/S/IN/
hdfs dfs -put ~/Datasets/Census/IN/IN2000.wkt Census/S/IN/A.wkt
hdfs dfs -put ~/Datasets/Census/IN/IN2010.wkt Census/S/IN/B.wkt
./QuadPart -d Census/S/IN -p 312 -t 1e-3
./Perf -d Census/S/IN -p 312 -t 1e-3 -n 1
DATASET    = Census/S/IN
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 312
Run 1 ./sdcel2_debug Census/S/IN/P312 /home/acald013/RIDIR/local_path/Census/S/IN/P312/ 1e-3 "312_Census/S/IN_1e-3_1"
2022-03-21 08:47:37,383|13605|application_1639015019875_1416|13885|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/IN/P312/edgesA --input2 Census/S/IN/P312/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/IN/P312//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/IN/P312//boundary.wkt --tolerance 1e-3 --qtag 312_Census/S/IN_1e-3_1 --debug --local
2022-03-21 08:47:37,471|13693|application_1639015019875_1416|INFO|scale=1000.0
2022-03-21 08:47:37,563|13785|Saved /tmp/edgesCells_312.wkt in 0.00s [850 records].
2022-03-21 08:47:37,563|13785|application_1639015019875_1416|INFO|npartitions=850
2022-03-21 08:47:37,563|13785|application_1639015019875_1416|180|TIME|start|312_Census/S/IN_1e-3_1
2022-03-21 08:47:49,826|26048|application_1639015019875_1416|INFO|nEdgesA=623109
2022-03-21 08:47:52,954|29176|application_1639015019875_1416|INFO|nEdgesB=642651
2022-03-21 08:47:52,954|29176|application_1639015019875_1416|15391|TIME|read|312_Census/S/IN_1e-3_1
2022-03-21 08:47:55,477|31699|application_1639015019875_1416|2523|TIME|layer1S|312_Census/S/IN_1e-3_1
2022-03-21 08:47:57,474|33696|Saved /tmp/edgesFAC.wkt in 0.15s [5050 records].
2022-03-21 08:47:59,272|35494|application_1639015019875_1416|3795|TIME|layer2S|312_Census/S/IN_1e-3_1
2022-03-21 08:48:01,139|37361|Saved /tmp/edgesFBC.wkt in 0.18s [5235 records].
2022-03-21 08:48:35,573|71795|Saved /tmp/edgesS.wkt in 0.15s [11356 records].
2022-03-21 08:48:37,191|73413|application_1639015019875_1416|37919|TIME|overlayS|312_Census/S/IN_1e-3_1
2022-03-21 08:48:38,092|74314|Saved /tmp/edgesFE.wkt in 0.15s [1944 records].
2022-03-21 08:48:38,092|74314|application_1639015019875_1416|901|TIME|end|312_Census/S/IN_1e-3_1
hdfs dfs -mkdir Census/S/IA/
hdfs dfs -put ~/Datasets/Census/IA/IA2000.wkt Census/S/IA/A.wkt
hdfs dfs -put ~/Datasets/Census/IA/IA2010.wkt Census/S/IA/B.wkt
./QuadPart -d Census/S/IA -p 216 -t 1e-3
./Perf -d Census/S/IA -p 216 -t 1e-3 -n 1
DATASET    = Census/S/IA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 216
Run 1 ./sdcel2_debug Census/S/IA/P216 /home/acald013/RIDIR/local_path/Census/S/IA/P216/ 1e-3 "216_Census/S/IA_1e-3_1"
2022-03-21 08:48:55,850|13687|application_1639015019875_1417|13939|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/IA/P216/edgesA --input2 Census/S/IA/P216/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/IA/P216//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/IA/P216//boundary.wkt --tolerance 1e-3 --qtag 216_Census/S/IA_1e-3_1 --debug --local
2022-03-21 08:48:55,935|13772|application_1639015019875_1417|INFO|scale=1000.0
2022-03-21 08:48:56,022|13859|Saved /tmp/edgesCells_216.wkt in 0.00s [574 records].
2022-03-21 08:48:56,023|13860|application_1639015019875_1417|INFO|npartitions=574
2022-03-21 08:48:56,023|13860|application_1639015019875_1417|173|TIME|start|216_Census/S/IA_1e-3_1
2022-03-21 08:49:07,570|25407|application_1639015019875_1417|INFO|nEdgesA=441171
2022-03-21 08:49:09,840|27677|application_1639015019875_1417|INFO|nEdgesB=444146
2022-03-21 08:49:09,840|27677|application_1639015019875_1417|13817|TIME|read|216_Census/S/IA_1e-3_1
2022-03-21 08:49:11,826|29663|application_1639015019875_1417|1986|TIME|layer1S|216_Census/S/IA_1e-3_1
2022-03-21 08:49:13,074|30911|Saved /tmp/edgesFAC.wkt in 0.11s [2891 records].
2022-03-21 08:49:14,380|32217|application_1639015019875_1417|2553|TIME|layer2S|216_Census/S/IA_1e-3_1
2022-03-21 08:49:15,381|33218|Saved /tmp/edgesFBC.wkt in 0.08s [2934 records].
2022-03-21 08:49:49,383|67220|Saved /tmp/edgesS.wkt in 0.11s [6191 records].
2022-03-21 08:49:51,003|68840|application_1639015019875_1417|36624|TIME|overlayS|216_Census/S/IA_1e-3_1
2022-03-21 08:49:51,538|69375|Saved /tmp/edgesFE.wkt in 0.10s [1016 records].
2022-03-21 08:49:51,539|69376|application_1639015019875_1417|536|TIME|end|216_Census/S/IA_1e-3_1
hdfs dfs -mkdir Census/S/KS/
hdfs dfs -put ~/Datasets/Census/KS/KS2000.wkt Census/S/KS/A.wkt
hdfs dfs -put ~/Datasets/Census/KS/KS2010.wkt Census/S/KS/B.wkt
./QuadPart -d Census/S/KS -p 159 -t 1e-3
./Perf -d Census/S/KS -p 159 -t 1e-3 -n 1
DATASET    = Census/S/KS
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 159
Run 1 ./sdcel2_debug Census/S/KS/P159 /home/acald013/RIDIR/local_path/Census/S/KS/P159/ 1e-3 "159_Census/S/KS_1e-3_1"
2022-03-21 08:50:09,032|13588|application_1639015019875_1418|13850|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/KS/P159/edgesA --input2 Census/S/KS/P159/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/KS/P159//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/KS/P159//boundary.wkt --tolerance 1e-3 --qtag 159_Census/S/KS_1e-3_1 --debug --local
2022-03-21 08:50:09,110|13666|application_1639015019875_1418|INFO|scale=1000.0
2022-03-21 08:50:09,176|13732|Saved /tmp/edgesCells_159.wkt in 0.00s [430 records].
2022-03-21 08:50:09,176|13732|application_1639015019875_1418|INFO|npartitions=430
2022-03-21 08:50:09,176|13732|application_1639015019875_1418|144|TIME|start|159_Census/S/KS_1e-3_1
2022-03-21 08:50:20,733|25289|application_1639015019875_1418|INFO|nEdgesA=317886
2022-03-21 08:50:22,421|26977|application_1639015019875_1418|INFO|nEdgesB=327330
2022-03-21 08:50:22,421|26977|application_1639015019875_1418|13245|TIME|read|159_Census/S/KS_1e-3_1
2022-03-21 08:50:24,050|28606|application_1639015019875_1418|1629|TIME|layer1S|159_Census/S/KS_1e-3_1
2022-03-21 08:50:25,319|29875|Saved /tmp/edgesFAC.wkt in 0.08s [2372 records].
2022-03-21 08:50:26,129|30685|application_1639015019875_1418|2079|TIME|layer2S|159_Census/S/KS_1e-3_1
2022-03-21 08:50:27,341|31897|Saved /tmp/edgesFBC.wkt in 0.07s [2430 records].
2022-03-21 08:51:09,926|74482|Saved /tmp/edgesS.wkt in 0.08s [5154 records].
2022-03-21 08:51:11,472|76028|application_1639015019875_1418|45343|TIME|overlayS|159_Census/S/KS_1e-3_1
2022-03-21 08:51:11,835|76391|Saved /tmp/edgesFE.wkt in 0.08s [1028 records].
2022-03-21 08:51:11,835|76391|application_1639015019875_1418|363|TIME|end|159_Census/S/KS_1e-3_1
hdfs dfs -mkdir Census/S/KY/
hdfs dfs -put ~/Datasets/Census/KY/KY2000.wkt Census/S/KY/A.wkt
hdfs dfs -put ~/Datasets/Census/KY/KY2010.wkt Census/S/KY/B.wkt
./QuadPart -d Census/S/KY -p 573 -t 1e-3
./Perf -d Census/S/KY -p 573 -t 1e-3 -n 1
DATASET    = Census/S/KY
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 573
Run 1 ./sdcel2_debug Census/S/KY/P573 /home/acald013/RIDIR/local_path/Census/S/KY/P573/ 1e-3 "573_Census/S/KY_1e-3_1"
2022-03-21 08:51:31,318|14123|application_1639015019875_1419|14371|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/KY/P573/edgesA --input2 Census/S/KY/P573/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/KY/P573//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/KY/P573//boundary.wkt --tolerance 1e-3 --qtag 573_Census/S/KY_1e-3_1 --debug --local
2022-03-21 08:51:31,454|14259|application_1639015019875_1419|INFO|scale=1000.0
2022-03-21 08:51:31,571|14376|Saved /tmp/edgesCells_573.wkt in 0.00s [1552 records].
2022-03-21 08:51:31,571|14376|application_1639015019875_1419|INFO|npartitions=1552
2022-03-21 08:51:31,572|14377|application_1639015019875_1419|254|TIME|start|573_Census/S/KY_1e-3_1
2022-03-21 08:51:45,525|28330|application_1639015019875_1419|INFO|nEdgesA=1128408
2022-03-21 08:51:49,890|32695|application_1639015019875_1419|INFO|nEdgesB=1180665
2022-03-21 08:51:49,890|32695|application_1639015019875_1419|18318|TIME|read|573_Census/S/KY_1e-3_1
2022-03-21 08:51:53,387|36192|application_1639015019875_1419|3497|TIME|layer1S|573_Census/S/KY_1e-3_1
2022-03-21 08:51:55,444|38249|Saved /tmp/edgesFAC.wkt in 0.22s [6319 records].
2022-03-21 08:51:59,485|42290|application_1639015019875_1419|6098|TIME|layer2S|573_Census/S/KY_1e-3_1
2022-03-21 08:52:02,037|44842|Saved /tmp/edgesFBC.wkt in 0.24s [6595 records].
2022-03-21 08:52:35,509|78314|Saved /tmp/edgesS.wkt in 0.42s [15453 records].
2022-03-21 08:52:38,551|81356|application_1639015019875_1419|39066|TIME|overlayS|573_Census/S/KY_1e-3_1
2022-03-21 08:52:39,657|82462|Saved /tmp/edgesFE.wkt in 0.30s [2050 records].
2022-03-21 08:52:39,657|82462|application_1639015019875_1419|1106|TIME|end|573_Census/S/KY_1e-3_1
hdfs dfs -mkdir Census/S/LA/
hdfs dfs -put ~/Datasets/Census/LA/LA2000.wkt Census/S/LA/A.wkt
hdfs dfs -put ~/Datasets/Census/LA/LA2010.wkt Census/S/LA/B.wkt
./QuadPart -d Census/S/LA -p 365 -t 1e-3
./Perf -d Census/S/LA -p 365 -t 1e-3 -n 1
DATASET    = Census/S/LA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 365
Run 1 ./sdcel2_debug Census/S/LA/P365 /home/acald013/RIDIR/local_path/Census/S/LA/P365/ 1e-3 "365_Census/S/LA_1e-3_1"
2022-03-21 08:52:57,607|13436|application_1639015019875_1420|13675|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/LA/P365/edgesA --input2 Census/S/LA/P365/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/LA/P365//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/LA/P365//boundary.wkt --tolerance 1e-3 --qtag 365_Census/S/LA_1e-3_1 --debug --local
2022-03-21 08:52:57,703|13532|application_1639015019875_1420|INFO|scale=1000.0
2022-03-21 08:52:57,805|13634|Saved /tmp/edgesCells_365.wkt in 0.00s [985 records].
2022-03-21 08:52:57,805|13634|application_1639015019875_1420|INFO|npartitions=985
2022-03-21 08:52:57,805|13634|application_1639015019875_1420|198|TIME|start|365_Census/S/LA_1e-3_1
2022-03-21 08:53:10,169|25998|application_1639015019875_1420|INFO|nEdgesA=754141
2022-03-21 08:53:13,403|29232|application_1639015019875_1420|INFO|nEdgesB=752373
2022-03-21 08:53:13,403|29232|application_1639015019875_1420|15598|TIME|read|365_Census/S/LA_1e-3_1
2022-03-21 08:53:16,254|32083|application_1639015019875_1420|2851|TIME|layer1S|365_Census/S/LA_1e-3_1
2022-03-21 08:53:18,107|33936|Saved /tmp/edgesFAC.wkt in 0.23s [4841 records].
2022-03-21 08:53:20,173|36002|application_1639015019875_1420|3919|TIME|layer2S|365_Census/S/LA_1e-3_1
2022-03-21 08:53:22,130|37959|Saved /tmp/edgesFBC.wkt in 0.21s [4916 records].
2022-03-21 08:54:04,510|80339|Saved /tmp/edgesS.wkt in 0.18s [11439 records].
2022-03-21 08:54:06,392|82221|application_1639015019875_1420|46219|TIME|overlayS|365_Census/S/LA_1e-3_1
2022-03-21 08:54:07,279|83108|Saved /tmp/edgesFE.wkt in 0.23s [1958 records].
2022-03-21 08:54:07,280|83109|application_1639015019875_1420|888|TIME|end|365_Census/S/LA_1e-3_1
hdfs dfs -mkdir Census/S/ME/
hdfs dfs -put ~/Datasets/Census/ME/ME2000.wkt Census/S/ME/A.wkt
hdfs dfs -put ~/Datasets/Census/ME/ME2010.wkt Census/S/ME/B.wkt
./QuadPart -d Census/S/ME -p 146 -t 1e-3
./Perf -d Census/S/ME -p 146 -t 1e-3 -n 1
DATASET    = Census/S/ME
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 146
Run 1 ./sdcel2_debug Census/S/ME/P146 /home/acald013/RIDIR/local_path/Census/S/ME/P146/ 1e-3 "146_Census/S/ME_1e-3_1"
2022-03-21 08:54:26,025|13906|application_1639015019875_1421|14155|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/ME/P146/edgesA --input2 Census/S/ME/P146/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/ME/P146//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/ME/P146//boundary.wkt --tolerance 1e-3 --qtag 146_Census/S/ME_1e-3_1 --debug --local
2022-03-21 08:54:26,096|13977|application_1639015019875_1421|INFO|scale=1000.0
2022-03-21 08:54:26,159|14040|Saved /tmp/edgesCells_146.wkt in 0.00s [421 records].
2022-03-21 08:54:26,160|14041|application_1639015019875_1421|INFO|npartitions=421
2022-03-21 08:54:26,160|14041|application_1639015019875_1421|135|TIME|start|146_Census/S/ME_1e-3_1
2022-03-21 08:54:37,335|25216|application_1639015019875_1421|INFO|nEdgesA=293724
2022-03-21 08:54:39,492|27373|application_1639015019875_1421|INFO|nEdgesB=301016
2022-03-21 08:54:39,493|27374|application_1639015019875_1421|13333|TIME|read|146_Census/S/ME_1e-3_1
2022-03-21 08:54:41,207|29088|application_1639015019875_1421|1714|TIME|layer1S|146_Census/S/ME_1e-3_1
2022-03-21 08:54:42,136|30017|Saved /tmp/edgesFAC.wkt in 0.06s [1788 records].
2022-03-21 08:54:43,144|31025|application_1639015019875_1421|1937|TIME|layer2S|146_Census/S/ME_1e-3_1
2022-03-21 08:54:44,037|31918|Saved /tmp/edgesFBC.wkt in 0.05s [1857 records].
2022-03-21 08:55:17,934|65815|Saved /tmp/edgesS.wkt in 0.09s [4445 records].
2022-03-21 08:55:18,933|66814|application_1639015019875_1421|35789|TIME|overlayS|146_Census/S/ME_1e-3_1
2022-03-21 08:55:19,375|67256|Saved /tmp/edgesFE.wkt in 0.07s [763 records].
2022-03-21 08:55:19,376|67257|application_1639015019875_1421|443|TIME|end|146_Census/S/ME_1e-3_1
hdfs dfs -mkdir Census/S/MD/
hdfs dfs -put ~/Datasets/Census/MD/MD2000.wkt Census/S/MD/A.wkt
hdfs dfs -put ~/Datasets/Census/MD/MD2010.wkt Census/S/MD/B.wkt
./QuadPart -d Census/S/MD -p 274 -t 1e-3
./Perf -d Census/S/MD -p 274 -t 1e-3 -n 1
DATASET    = Census/S/MD
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 274
Run 1 ./sdcel2_debug Census/S/MD/P274 /home/acald013/RIDIR/local_path/Census/S/MD/P274/ 1e-3 "274_Census/S/MD_1e-3_1"
2022-03-21 08:55:37,156|13624|application_1639015019875_1422|13878|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MD/P274/edgesA --input2 Census/S/MD/P274/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MD/P274//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MD/P274//boundary.wkt --tolerance 1e-3 --qtag 274_Census/S/MD_1e-3_1 --debug --local
2022-03-21 08:55:37,247|13715|application_1639015019875_1422|INFO|scale=1000.0
2022-03-21 08:55:37,333|13801|Saved /tmp/edgesCells_274.wkt in 0.00s [697 records].
2022-03-21 08:55:37,333|13801|application_1639015019875_1422|INFO|npartitions=697
2022-03-21 08:55:37,333|13801|application_1639015019875_1422|177|TIME|start|274_Census/S/MD_1e-3_1
2022-03-21 08:55:49,233|25701|application_1639015019875_1422|INFO|nEdgesA=540365
2022-03-21 08:55:52,051|28519|application_1639015019875_1422|INFO|nEdgesB=565684
2022-03-21 08:55:52,051|28519|application_1639015019875_1422|14718|TIME|read|274_Census/S/MD_1e-3_1
2022-03-21 08:55:54,196|30664|application_1639015019875_1422|2145|TIME|layer1S|274_Census/S/MD_1e-3_1
2022-03-21 08:55:55,777|32245|Saved /tmp/edgesFAC.wkt in 0.20s [4379 records].
2022-03-21 08:55:57,422|33890|application_1639015019875_1422|3226|TIME|layer2S|274_Census/S/MD_1e-3_1
2022-03-21 08:55:58,867|35335|Saved /tmp/edgesFBC.wkt in 0.13s [4799 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/MA/
hdfs dfs -put ~/Datasets/Census/MA/MA2000.wkt Census/S/MA/A.wkt
hdfs dfs -put ~/Datasets/Census/MA/MA2010.wkt Census/S/MA/B.wkt
./QuadPart -d Census/S/MA -p 188 -t 1e-3
./Perf -d Census/S/MA -p 188 -t 1e-3 -n 1
DATASET    = Census/S/MA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 188
Run 1 ./sdcel2_debug Census/S/MA/P188 /home/acald013/RIDIR/local_path/Census/S/MA/P188/ 1e-3 "188_Census/S/MA_1e-3_1"
2022-03-21 08:57:00,071|13682|application_1639015019875_1423|13961|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MA/P188/edgesA --input2 Census/S/MA/P188/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MA/P188//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MA/P188//boundary.wkt --tolerance 1e-3 --qtag 188_Census/S/MA_1e-3_1 --debug --local
2022-03-21 08:57:00,149|13760|application_1639015019875_1423|INFO|scale=1000.0
2022-03-21 08:57:00,219|13830|Saved /tmp/edgesCells_188.wkt in 0.00s [478 records].
2022-03-21 08:57:00,219|13830|application_1639015019875_1423|INFO|npartitions=478
2022-03-21 08:57:00,219|13830|application_1639015019875_1423|148|TIME|start|188_Census/S/MA_1e-3_1
2022-03-21 08:57:11,594|25205|application_1639015019875_1423|INFO|nEdgesA=356042
2022-03-21 08:57:13,530|27141|application_1639015019875_1423|INFO|nEdgesB=387951
2022-03-21 08:57:13,530|27141|application_1639015019875_1423|13311|TIME|read|188_Census/S/MA_1e-3_1
2022-03-21 08:57:15,520|29131|application_1639015019875_1423|1990|TIME|layer1S|188_Census/S/MA_1e-3_1
2022-03-21 08:57:16,742|30353|Saved /tmp/edgesFAC.wkt in 0.09s [4186 records].
2022-03-21 08:57:17,750|31361|application_1639015019875_1423|2230|TIME|layer2S|188_Census/S/MA_1e-3_1
2022-03-21 08:57:19,069|32680|Saved /tmp/edgesFBC.wkt in 0.09s [4261 records].
2022-03-21 08:57:56,058|69669|Saved /tmp/edgesS.wkt in 0.10s [13956 records].
2022-03-21 08:57:57,257|70868|application_1639015019875_1423|39507|TIME|overlayS|188_Census/S/MA_1e-3_1
2022-03-21 08:57:57,765|71376|Saved /tmp/edgesFE.wkt in 0.11s [5310 records].
2022-03-21 08:57:57,766|71377|application_1639015019875_1423|509|TIME|end|188_Census/S/MA_1e-3_1
hdfs dfs -mkdir Census/S/MI/
hdfs dfs -put ~/Datasets/Census/MI/MI2000.wkt Census/S/MI/A.wkt
hdfs dfs -put ~/Datasets/Census/MI/MI2010.wkt Census/S/MI/B.wkt
./QuadPart -d Census/S/MI -p 262 -t 1e-3
./Perf -d Census/S/MI -p 262 -t 1e-3 -n 1
DATASET    = Census/S/MI
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 262
Run 1 ./sdcel2_debug Census/S/MI/P262 /home/acald013/RIDIR/local_path/Census/S/MI/P262/ 1e-3 "262_Census/S/MI_1e-3_1"
2022-03-21 08:58:16,438|13548|application_1639015019875_1424|13793|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MI/P262/edgesA --input2 Census/S/MI/P262/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MI/P262//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MI/P262//boundary.wkt --tolerance 1e-3 --qtag 262_Census/S/MI_1e-3_1 --debug --local
2022-03-21 08:58:16,526|13636|application_1639015019875_1424|INFO|scale=1000.0
2022-03-21 08:58:16,609|13719|Saved /tmp/edgesCells_262.wkt in 0.00s [676 records].
2022-03-21 08:58:16,610|13720|application_1639015019875_1424|INFO|npartitions=676
2022-03-21 08:58:16,610|13720|application_1639015019875_1424|172|TIME|start|262_Census/S/MI_1e-3_1
2022-03-21 08:58:28,461|25571|application_1639015019875_1424|INFO|nEdgesA=607355
2022-03-21 08:58:30,973|28083|application_1639015019875_1424|INFO|nEdgesB=538722
2022-03-21 08:58:30,973|28083|application_1639015019875_1424|14363|TIME|read|262_Census/S/MI_1e-3_1
2022-03-21 08:58:33,975|31085|application_1639015019875_1424|3002|TIME|layer1S|262_Census/S/MI_1e-3_1
2022-03-21 08:58:36,140|33250|Saved /tmp/edgesFAC.wkt in 0.16s [6878 records].
2022-03-21 08:58:37,430|34540|application_1639015019875_1424|3454|TIME|layer2S|262_Census/S/MI_1e-3_1
2022-03-21 08:58:39,020|36130|Saved /tmp/edgesFBC.wkt in 0.13s [6851 records].
2022-03-21 08:59:24,617|81727|Saved /tmp/edgesS.wkt in 0.20s [15097 records].
2022-03-21 08:59:26,890|84000|application_1639015019875_1424|49461|TIME|overlayS|262_Census/S/MI_1e-3_1
2022-03-21 08:59:27,547|84657|Saved /tmp/edgesFE.wkt in 0.19s [4225 records].
2022-03-21 08:59:27,547|84657|application_1639015019875_1424|657|TIME|end|262_Census/S/MI_1e-3_1
hdfs dfs -mkdir Census/S/MN/
hdfs dfs -put ~/Datasets/Census/MN/MN2000.wkt Census/S/MN/A.wkt
hdfs dfs -put ~/Datasets/Census/MN/MN2010.wkt Census/S/MN/B.wkt
./QuadPart -d Census/S/MN -p 353 -t 1e-3
./Perf -d Census/S/MN -p 353 -t 1e-3 -n 1
DATASET    = Census/S/MN
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 353
Run 1 ./sdcel2_debug Census/S/MN/P353 /home/acald013/RIDIR/local_path/Census/S/MN/P353/ 1e-3 "353_Census/S/MN_1e-3_1"
2022-03-21 08:59:46,454|13726|application_1639015019875_1425|13975|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MN/P353/edgesA --input2 Census/S/MN/P353/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MN/P353//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MN/P353//boundary.wkt --tolerance 1e-3 --qtag 353_Census/S/MN_1e-3_1 --debug --local
2022-03-21 08:59:46,546|13818|application_1639015019875_1425|INFO|scale=1000.0
2022-03-21 08:59:46,642|13914|Saved /tmp/edgesCells_353.wkt in 0.00s [943 records].
2022-03-21 08:59:46,642|13914|application_1639015019875_1425|INFO|npartitions=943
2022-03-21 08:59:46,642|13914|application_1639015019875_1425|188|TIME|start|353_Census/S/MN_1e-3_1
2022-03-21 08:59:59,152|26424|application_1639015019875_1425|INFO|nEdgesA=707424
2022-03-21 09:00:02,975|30247|application_1639015019875_1425|INFO|nEdgesB=726090
2022-03-21 09:00:02,975|30247|application_1639015019875_1425|16333|TIME|read|353_Census/S/MN_1e-3_1
2022-03-21 09:00:05,722|32994|application_1639015019875_1425|2747|TIME|layer1S|353_Census/S/MN_1e-3_1
2022-03-21 09:00:07,532|34804|Saved /tmp/edgesFAC.wkt in 0.18s [4892 records].
2022-03-21 09:00:09,575|36847|application_1639015019875_1425|3853|TIME|layer2S|353_Census/S/MN_1e-3_1
2022-03-21 09:00:11,699|38971|Saved /tmp/edgesFBC.wkt in 0.26s [4947 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /hadoop/yarn/local/usercache/acald013/appcache/application_1639015019875_1425/blockmgr-00684b92-3e12-49de-ab19-95c3f84bc2fe/0c/shuffle_0_46_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:459) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/acald013/appcache/application_1639015019875_1425/blockmgr-00684b92-3e12-49de-ab19-95c3f84bc2fe/0c/shuffle_0_46_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:205) 	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:377) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:329) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:364) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:154) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	... 44 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/MS/
hdfs dfs -put ~/Datasets/Census/MS/MS2000.wkt Census/S/MS/A.wkt
hdfs dfs -put ~/Datasets/Census/MS/MS2010.wkt Census/S/MS/B.wkt
./QuadPart -d Census/S/MS -p 394 -t 1e-3
./Perf -d Census/S/MS -p 394 -t 1e-3 -n 1
DATASET    = Census/S/MS
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 394
Run 1 ./sdcel2_debug Census/S/MS/P394 /home/acald013/RIDIR/local_path/Census/S/MS/P394/ 1e-3 "394_Census/S/MS_1e-3_1"
2022-03-21 09:01:08,466|13634|application_1639015019875_1426|13885|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MS/P394/edgesA --input2 Census/S/MS/P394/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MS/P394//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MS/P394//boundary.wkt --tolerance 1e-3 --qtag 394_Census/S/MS_1e-3_1 --debug --local
2022-03-21 09:01:08,562|13730|application_1639015019875_1426|INFO|scale=1000.0
2022-03-21 09:01:08,670|13838|Saved /tmp/edgesCells_394.wkt in 0.00s [1126 records].
2022-03-21 09:01:08,670|13838|application_1639015019875_1426|INFO|npartitions=1126
2022-03-21 09:01:08,671|13839|application_1639015019875_1426|205|TIME|start|394_Census/S/MS_1e-3_1
2022-03-21 09:01:21,963|27131|application_1639015019875_1426|INFO|nEdgesA=777899
2022-03-21 09:01:25,651|30819|application_1639015019875_1426|INFO|nEdgesB=812259
2022-03-21 09:01:25,651|30819|application_1639015019875_1426|16980|TIME|read|394_Census/S/MS_1e-3_1
2022-03-21 09:01:28,503|33671|application_1639015019875_1426|2852|TIME|layer1S|394_Census/S/MS_1e-3_1
2022-03-21 09:01:30,095|35263|Saved /tmp/edgesFAC.wkt in 0.21s [4052 records].
2022-03-21 09:01:32,234|37402|application_1639015019875_1426|3731|TIME|layer2S|394_Census/S/MS_1e-3_1
2022-03-21 09:01:34,103|39271|Saved /tmp/edgesFBC.wkt in 0.17s [4219 records].
2022-03-21 09:02:10,035|75203|Saved /tmp/edgesS.wkt in 0.21s [9671 records].
2022-03-21 09:02:11,473|76641|application_1639015019875_1426|39239|TIME|overlayS|394_Census/S/MS_1e-3_1
2022-03-21 09:02:12,206|77374|Saved /tmp/edgesFE.wkt in 0.14s [1222 records].
2022-03-21 09:02:12,207|77375|application_1639015019875_1426|734|TIME|end|394_Census/S/MS_1e-3_1
hdfs dfs -mkdir Census/S/MO/
hdfs dfs -put ~/Datasets/Census/MO/MO2000.wkt Census/S/MO/A.wkt
hdfs dfs -put ~/Datasets/Census/MO/MO2010.wkt Census/S/MO/B.wkt
./QuadPart -d Census/S/MO -p 485 -t 1e-3
./Perf -d Census/S/MO -p 485 -t 1e-3 -n 1
DATASET    = Census/S/MO
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 485
Run 1 ./sdcel2_debug Census/S/MO/P485 /home/acald013/RIDIR/local_path/Census/S/MO/P485/ 1e-3 "485_Census/S/MO_1e-3_1"
2022-03-21 09:02:30,423|13876|application_1639015019875_1427|14121|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MO/P485/edgesA --input2 Census/S/MO/P485/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MO/P485//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MO/P485//boundary.wkt --tolerance 1e-3 --qtag 485_Census/S/MO_1e-3_1 --debug --local
2022-03-21 09:02:30,537|13990|application_1639015019875_1427|INFO|scale=1000.0
2022-03-21 09:02:30,652|14105|Saved /tmp/edgesCells_485.wkt in 0.01s [1270 records].
2022-03-21 09:02:30,652|14105|application_1639015019875_1427|INFO|npartitions=1270
2022-03-21 09:02:30,653|14106|application_1639015019875_1427|230|TIME|start|485_Census/S/MO_1e-3_1
2022-03-21 09:02:44,014|27467|application_1639015019875_1427|INFO|nEdgesA=967864
2022-03-21 09:03:03,530|46983|application_1639015019875_1427|INFO|nEdgesB=998479
2022-03-21 09:03:03,530|46983|application_1639015019875_1427|32877|TIME|read|485_Census/S/MO_1e-3_1
2022-03-21 09:03:06,326|49779|application_1639015019875_1427|2796|TIME|layer1S|485_Census/S/MO_1e-3_1
2022-03-21 09:03:08,375|51828|Saved /tmp/edgesFAC.wkt in 0.20s [5824 records].
2022-03-21 09:03:10,757|54210|application_1639015019875_1427|4431|TIME|layer2S|485_Census/S/MO_1e-3_1
2022-03-21 09:03:12,669|56122|Saved /tmp/edgesFBC.wkt in 0.22s [5992 records].
2022-03-21 09:03:46,545|89998|Saved /tmp/edgesS.wkt in 0.24s [13363 records].
2022-03-21 09:03:48,612|92065|application_1639015019875_1427|37855|TIME|overlayS|485_Census/S/MO_1e-3_1
2022-03-21 09:03:49,908|93361|Saved /tmp/edgesFE.wkt in 0.30s [2040 records].
2022-03-21 09:03:49,909|93362|application_1639015019875_1427|1297|TIME|end|485_Census/S/MO_1e-3_1
hdfs dfs -mkdir Census/S/MT/
hdfs dfs -put ~/Datasets/Census/MT/MT2000.wkt Census/S/MT/A.wkt
hdfs dfs -put ~/Datasets/Census/MT/MT2010.wkt Census/S/MT/B.wkt
./QuadPart -d Census/S/MT -p 303 -t 1e-3
./Perf -d Census/S/MT -p 303 -t 1e-3 -n 1
DATASET    = Census/S/MT
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 303
Run 1 ./sdcel2_debug Census/S/MT/P303 /home/acald013/RIDIR/local_path/Census/S/MT/P303/ 1e-3 "303_Census/S/MT_1e-3_1"
2022-03-21 09:04:08,107|14118|application_1639015019875_1428|14358|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MT/P303/edgesA --input2 Census/S/MT/P303/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MT/P303//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MT/P303//boundary.wkt --tolerance 1e-3 --qtag 303_Census/S/MT_1e-3_1 --debug --local
2022-03-21 09:04:08,206|14217|application_1639015019875_1428|INFO|scale=1000.0
2022-03-21 09:04:08,302|14313|Saved /tmp/edgesCells_303.wkt in 0.00s [907 records].
2022-03-21 09:04:08,302|14313|application_1639015019875_1428|INFO|npartitions=907
2022-03-21 09:04:08,302|14313|application_1639015019875_1428|195|TIME|start|303_Census/S/MT_1e-3_1
2022-03-21 09:04:27,479|33490|application_1639015019875_1428|INFO|nEdgesA=660492
2022-03-21 09:04:30,224|36235|application_1639015019875_1428|INFO|nEdgesB=624860
2022-03-21 09:04:30,224|36235|application_1639015019875_1428|21922|TIME|read|303_Census/S/MT_1e-3_1
2022-03-21 09:04:33,246|39257|application_1639015019875_1428|3022|TIME|layer1S|303_Census/S/MT_1e-3_1
2022-03-21 09:04:34,975|40986|Saved /tmp/edgesFAC.wkt in 0.16s [2724 records].
2022-03-21 09:04:36,873|42884|application_1639015019875_1428|3627|TIME|layer2S|303_Census/S/MT_1e-3_1
2022-03-21 09:04:38,338|44349|Saved /tmp/edgesFBC.wkt in 0.19s [2694 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.FetchFailedException: /hadoop/yarn/local/usercache/acald013/appcache/application_1639015019875_1428/blockmgr-45877cfd-a054-46ac-8f3c-e85835b8ef76/33/shuffle_0_25_0.index 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.throwFetchFailedException(ShuffleBlockFetcherIterator.scala:528) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:459) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.next(ShuffleBlockFetcherIterator.scala:62) 	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) 	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:30) 	at org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37) 	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) 	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:216) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) Caused by: java.nio.file.NoSuchFileException: /hadoop/yarn/local/usercache/acald013/appcache/application_1639015019875_1428/blockmgr-45877cfd-a054-46ac-8f3c-e85835b8ef76/33/shuffle_0_25_0.index 	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214) 	at java.nio.file.Files.newByteChannel(Files.java:361) 	at java.nio.file.Files.newByteChannel(Files.java:407) 	at org.apache.spark.shuffle.IndexShuffleBlockResolver.getBlockData(IndexShuffleBlockResolver.scala:205) 	at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:377) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.fetchLocalBlocks(ShuffleBlockFetcherIterator.scala:329) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.initialize(ShuffleBlockFetcherIterator.scala:364) 	at org.apache.spark.storage.ShuffleBlockFetcherIterator.<init>(ShuffleBlockFetcherIterator.scala:154) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:45) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	... 44 more 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/NE/
hdfs dfs -put ~/Datasets/Census/NE/NE2000.wkt Census/S/NE/A.wkt
hdfs dfs -put ~/Datasets/Census/NE/NE2010.wkt Census/S/NE/B.wkt
./QuadPart -d Census/S/NE -p 123 -t 1e-3
./Perf -d Census/S/NE -p 123 -t 1e-3 -n 1
DATASET    = Census/S/NE
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 123
Run 1 ./sdcel2_debug Census/S/NE/P123 /home/acald013/RIDIR/local_path/Census/S/NE/P123/ 1e-3 "123_Census/S/NE_1e-3_1"
2022-03-21 09:05:31,423|13644|application_1639015019875_1429|13895|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NE/P123/edgesA --input2 Census/S/NE/P123/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NE/P123//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NE/P123//boundary.wkt --tolerance 1e-3 --qtag 123_Census/S/NE_1e-3_1 --debug --local
2022-03-21 09:05:31,502|13723|application_1639015019875_1429|INFO|scale=1000.0
2022-03-21 09:05:31,557|13778|Saved /tmp/edgesCells_123.wkt in 0.00s [316 records].
2022-03-21 09:05:31,557|13778|application_1639015019875_1429|INFO|npartitions=316
2022-03-21 09:05:31,557|13778|application_1639015019875_1429|134|TIME|start|123_Census/S/NE_1e-3_1
2022-03-21 09:05:42,543|24764|application_1639015019875_1429|INFO|nEdgesA=257608
2022-03-21 09:05:44,508|26729|application_1639015019875_1429|INFO|nEdgesB=253489
2022-03-21 09:05:44,509|26730|application_1639015019875_1429|12952|TIME|read|123_Census/S/NE_1e-3_1
2022-03-21 09:05:46,164|28385|application_1639015019875_1429|1655|TIME|layer1S|123_Census/S/NE_1e-3_1
2022-03-21 09:05:47,063|29284|Saved /tmp/edgesFAC.wkt in 0.06s [1628 records].
2022-03-21 09:05:48,046|30267|application_1639015019875_1429|1882|TIME|layer2S|123_Census/S/NE_1e-3_1
2022-03-21 09:05:49,001|31222|Saved /tmp/edgesFBC.wkt in 0.06s [1663 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/NV/
hdfs dfs -put ~/Datasets/Census/NV/NV2000.wkt Census/S/NV/A.wkt
hdfs dfs -put ~/Datasets/Census/NV/NV2010.wkt Census/S/NV/B.wkt
./QuadPart -d Census/S/NV -p 148 -t 1e-3
./Perf -d Census/S/NV -p 148 -t 1e-3 -n 1
DATASET    = Census/S/NV
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 148
Run 1 ./sdcel2_debug Census/S/NV/P148 /home/acald013/RIDIR/local_path/Census/S/NV/P148/ 1e-3 "148_Census/S/NV_1e-3_1"
2022-03-21 09:06:41,505|13431|application_1639015019875_1430|13682|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NV/P148/edgesA --input2 Census/S/NV/P148/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NV/P148//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NV/P148//boundary.wkt --tolerance 1e-3 --qtag 148_Census/S/NV_1e-3_1 --debug --local
2022-03-21 09:06:41,582|13508|application_1639015019875_1430|INFO|scale=1000.0
2022-03-21 09:06:41,653|13579|Saved /tmp/edgesCells_148.wkt in 0.00s [472 records].
2022-03-21 09:06:41,653|13579|application_1639015019875_1430|INFO|npartitions=472
2022-03-21 09:06:41,653|13579|application_1639015019875_1430|148|TIME|start|148_Census/S/NV_1e-3_1
2022-03-21 09:06:52,593|24519|application_1639015019875_1430|INFO|nEdgesA=279508
2022-03-21 09:06:54,442|26368|application_1639015019875_1430|INFO|nEdgesB=304191
2022-03-21 09:06:54,443|26369|application_1639015019875_1430|12790|TIME|read|148_Census/S/NV_1e-3_1
2022-03-21 09:06:56,067|27993|application_1639015019875_1430|1624|TIME|layer1S|148_Census/S/NV_1e-3_1
2022-03-21 09:06:56,872|28798|Saved /tmp/edgesFAC.wkt in 0.06s [1897 records].
2022-03-21 09:06:58,520|30446|application_1639015019875_1430|2453|TIME|layer2S|148_Census/S/NV_1e-3_1
2022-03-21 09:06:59,441|31367|Saved /tmp/edgesFBC.wkt in 0.05s [2206 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 1 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/NH/
hdfs dfs -put ~/Datasets/Census/NH/NH2000.wkt Census/S/NH/A.wkt
hdfs dfs -put ~/Datasets/Census/NH/NH2010.wkt Census/S/NH/B.wkt
./QuadPart -d Census/S/NH -p 41 -t 1e-3
./Perf -d Census/S/NH -p 41 -t 1e-3 -n 1
DATASET    = Census/S/NH
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 41
Run 1 ./sdcel2_debug Census/S/NH/P41 /home/acald013/RIDIR/local_path/Census/S/NH/P41/ 1e-3 "41_Census/S/NH_1e-3_1"
2022-03-21 09:07:47,897|13436|application_1639015019875_1431|13675|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NH/P41/edgesA --input2 Census/S/NH/P41/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NH/P41//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NH/P41//boundary.wkt --tolerance 1e-3 --qtag 41_Census/S/NH_1e-3_1 --debug --local
2022-03-21 09:07:47,954|13493|application_1639015019875_1431|INFO|scale=1000.0
2022-03-21 09:07:47,989|13528|Saved /tmp/edgesCells_41.wkt in 0.00s [115 records].
2022-03-21 09:07:47,990|13529|application_1639015019875_1431|INFO|npartitions=115
2022-03-21 09:07:47,990|13529|application_1639015019875_1431|93|TIME|start|41_Census/S/NH_1e-3_1
2022-03-21 09:07:57,643|23182|application_1639015019875_1431|INFO|nEdgesA=79097
2022-03-21 09:07:58,927|24466|application_1639015019875_1431|INFO|nEdgesB=84479
2022-03-21 09:07:58,928|24467|application_1639015019875_1431|10938|TIME|read|41_Census/S/NH_1e-3_1
2022-03-21 09:08:00,214|25753|application_1639015019875_1431|1286|TIME|layer1S|41_Census/S/NH_1e-3_1
2022-03-21 09:08:00,779|26318|Saved /tmp/edgesFAC.wkt in 0.02s [811 records].
2022-03-21 09:08:01,495|27034|application_1639015019875_1431|1281|TIME|layer2S|41_Census/S/NH_1e-3_1
2022-03-21 09:08:01,993|27532|Saved /tmp/edgesFBC.wkt in 0.03s [841 records].
2022-03-21 09:08:09,329|34868|Saved /tmp/edgesS.wkt in 0.02s [2016 records].
2022-03-21 09:08:09,807|35346|application_1639015019875_1431|8312|TIME|overlayS|41_Census/S/NH_1e-3_1
2022-03-21 09:08:09,960|35499|Saved /tmp/edgesFE.wkt in 0.02s [569 records].
2022-03-21 09:08:09,960|35499|application_1639015019875_1431|153|TIME|end|41_Census/S/NH_1e-3_1
hdfs dfs -mkdir Census/S/NJ/
hdfs dfs -put ~/Datasets/Census/NJ/NJ2000.wkt Census/S/NJ/A.wkt
hdfs dfs -put ~/Datasets/Census/NJ/NJ2010.wkt Census/S/NJ/B.wkt
./QuadPart -d Census/S/NJ -p 228 -t 1e-3
./Perf -d Census/S/NJ -p 228 -t 1e-3 -n 1
DATASET    = Census/S/NJ
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 228
Run 1 ./sdcel2_debug Census/S/NJ/P228 /home/acald013/RIDIR/local_path/Census/S/NJ/P228/ 1e-3 "228_Census/S/NJ_1e-3_1"
2022-03-21 09:08:27,898|13839|application_1639015019875_1432|14085|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NJ/P228/edgesA --input2 Census/S/NJ/P228/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NJ/P228//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NJ/P228//boundary.wkt --tolerance 1e-3 --qtag 228_Census/S/NJ_1e-3_1 --debug --local
2022-03-21 09:08:27,974|13915|application_1639015019875_1432|INFO|scale=1000.0
2022-03-21 09:08:28,052|13993|Saved /tmp/edgesCells_228.wkt in 0.00s [553 records].
2022-03-21 09:08:28,053|13994|application_1639015019875_1432|INFO|npartitions=553
2022-03-21 09:08:28,053|13994|application_1639015019875_1432|155|TIME|start|228_Census/S/NJ_1e-3_1
2022-03-21 09:08:39,565|25506|application_1639015019875_1432|INFO|nEdgesA=477769
2022-03-21 09:08:41,836|27777|application_1639015019875_1432|INFO|nEdgesB=470425
2022-03-21 09:08:41,836|27777|application_1639015019875_1432|13783|TIME|read|228_Census/S/NJ_1e-3_1
2022-03-21 09:08:43,940|29881|application_1639015019875_1432|2104|TIME|layer1S|228_Census/S/NJ_1e-3_1
2022-03-21 09:08:45,391|31332|Saved /tmp/edgesFAC.wkt in 0.11s [5358 records].
2022-03-21 09:08:46,548|32489|application_1639015019875_1432|2608|TIME|layer2S|228_Census/S/NJ_1e-3_1
2022-03-21 09:08:47,919|33860|Saved /tmp/edgesFBC.wkt in 0.07s [5413 records].
2022-03-21 09:09:23,258|69199|Saved /tmp/edgesS.wkt in 0.12s [12466 records].
2022-03-21 09:09:24,397|70338|application_1639015019875_1432|37849|TIME|overlayS|228_Census/S/NJ_1e-3_1
2022-03-21 09:09:24,842|70783|Saved /tmp/edgesFE.wkt in 0.11s [3257 records].
2022-03-21 09:09:24,842|70783|application_1639015019875_1432|445|TIME|end|228_Census/S/NJ_1e-3_1
hdfs dfs -mkdir Census/S/NM/
hdfs dfs -put ~/Datasets/Census/NM/NM2000.wkt Census/S/NM/A.wkt
hdfs dfs -put ~/Datasets/Census/NM/NM2010.wkt Census/S/NM/B.wkt
./QuadPart -d Census/S/NM -p 206 -t 1e-3
./Perf -d Census/S/NM -p 206 -t 1e-3 -n 1
DATASET    = Census/S/NM
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 206
Run 1 ./sdcel2_debug Census/S/NM/P206 /home/acald013/RIDIR/local_path/Census/S/NM/P206/ 1e-3 "206_Census/S/NM_1e-3_1"
2022-03-21 09:09:58,328|29202|application_1639015019875_1433|29444|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NM/P206/edgesA --input2 Census/S/NM/P206/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NM/P206//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NM/P206//boundary.wkt --tolerance 1e-3 --qtag 206_Census/S/NM_1e-3_1 --debug --local
2022-03-21 09:09:58,413|29287|application_1639015019875_1433|INFO|scale=1000.0
2022-03-21 09:09:58,521|29395|Saved /tmp/edgesCells_206.wkt in 0.00s [598 records].
2022-03-21 09:09:58,522|29396|application_1639015019875_1433|INFO|npartitions=598
2022-03-21 09:09:58,522|29396|application_1639015019875_1433|194|TIME|start|206_Census/S/NM_1e-3_1
2022-03-21 09:10:10,180|41054|application_1639015019875_1433|INFO|nEdgesA=415370
2022-03-21 09:10:12,982|43856|application_1639015019875_1433|INFO|nEdgesB=424093
2022-03-21 09:10:12,982|43856|application_1639015019875_1433|14460|TIME|read|206_Census/S/NM_1e-3_1
2022-03-21 09:10:15,111|45985|application_1639015019875_1433|2129|TIME|layer1S|206_Census/S/NM_1e-3_1
2022-03-21 09:10:16,265|47139|Saved /tmp/edgesFAC.wkt in 0.13s [2311 records].
2022-03-21 09:10:17,591|48465|application_1639015019875_1433|2480|TIME|layer2S|206_Census/S/NM_1e-3_1
2022-03-21 09:10:18,669|49543|Saved /tmp/edgesFBC.wkt in 0.09s [2332 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/NY/
hdfs dfs -put ~/Datasets/Census/NY/NY2000.wkt Census/S/NY/A.wkt
hdfs dfs -put ~/Datasets/Census/NY/NY2010.wkt Census/S/NY/B.wkt
./QuadPart -d Census/S/NY -p 365 -t 1e-3
./Perf -d Census/S/NY -p 365 -t 1e-3 -n 1
DATASET    = Census/S/NY
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 365
Run 1 ./sdcel2_debug Census/S/NY/P365 /home/acald013/RIDIR/local_path/Census/S/NY/P365/ 1e-3 "365_Census/S/NY_1e-3_1"
2022-03-21 09:11:11,547|13586|application_1639015019875_1434|13829|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NY/P365/edgesA --input2 Census/S/NY/P365/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NY/P365//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NY/P365//boundary.wkt --tolerance 1e-3 --qtag 365_Census/S/NY_1e-3_1 --debug --local
2022-03-21 09:11:11,639|13678|application_1639015019875_1434|INFO|scale=1000.0
2022-03-21 09:11:11,744|13783|Saved /tmp/edgesCells_365.wkt in 0.00s [997 records].
2022-03-21 09:11:11,746|13785|application_1639015019875_1434|INFO|npartitions=997
2022-03-21 09:11:11,746|13785|application_1639015019875_1434|199|TIME|start|365_Census/S/NY_1e-3_1
2022-03-21 09:11:24,542|26581|application_1639015019875_1434|INFO|nEdgesA=756336
2022-03-21 09:11:27,748|29787|application_1639015019875_1434|INFO|nEdgesB=749117
2022-03-21 09:11:27,749|29788|application_1639015019875_1434|16003|TIME|read|365_Census/S/NY_1e-3_1
2022-03-21 09:11:30,195|32234|application_1639015019875_1434|2446|TIME|layer1S|365_Census/S/NY_1e-3_1
2022-03-21 09:11:32,850|34889|Saved /tmp/edgesFAC.wkt in 0.22s [10886 records].
2022-03-21 09:11:34,546|36585|application_1639015019875_1434|4351|TIME|layer2S|365_Census/S/NY_1e-3_1
2022-03-21 09:11:37,165|39204|Saved /tmp/edgesFBC.wkt in 0.24s [10938 records].
2022-03-21 09:12:41,495|103534|Saved /tmp/edgesS.wkt in 0.22s [23788 records].
2022-03-21 09:12:43,882|105921|application_1639015019875_1434|69336|TIME|overlayS|365_Census/S/NY_1e-3_1
2022-03-21 09:12:44,539|106578|Saved /tmp/edgesFE.wkt in 0.14s [7845 records].
2022-03-21 09:12:44,539|106578|application_1639015019875_1434|657|TIME|end|365_Census/S/NY_1e-3_1
hdfs dfs -mkdir Census/S/NC/
hdfs dfs -put ~/Datasets/Census/NC/NC2000.wkt Census/S/NC/A.wkt
hdfs dfs -put ~/Datasets/Census/NC/NC2010.wkt Census/S/NC/B.wkt
./QuadPart -d Census/S/NC -p 764 -t 1e-3
./Perf -d Census/S/NC -p 764 -t 1e-3 -n 1
DATASET    = Census/S/NC
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 764
Run 1 ./sdcel2_debug Census/S/NC/P764 /home/acald013/RIDIR/local_path/Census/S/NC/P764/ 1e-3 "764_Census/S/NC_1e-3_1"
2022-03-21 09:13:03,910|13622|application_1639015019875_1435|13881|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/NC/P764/edgesA --input2 Census/S/NC/P764/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/NC/P764//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/NC/P764//boundary.wkt --tolerance 1e-3 --qtag 764_Census/S/NC_1e-3_1 --debug --local
2022-03-21 09:13:04,028|13740|application_1639015019875_1435|INFO|scale=1000.0
2022-03-21 09:13:04,161|13873|Saved /tmp/edgesCells_764.wkt in 0.01s [1987 records].
2022-03-21 09:13:04,162|13874|application_1639015019875_1435|INFO|npartitions=1987
2022-03-21 09:13:04,162|13874|application_1639015019875_1435|252|TIME|start|764_Census/S/NC_1e-3_1
2022-03-21 09:13:18,950|28662|application_1639015019875_1435|INFO|nEdgesA=1367933
2022-03-21 09:13:24,366|34078|application_1639015019875_1435|INFO|nEdgesB=1573367
2022-03-21 09:13:24,366|34078|application_1639015019875_1435|20204|TIME|read|764_Census/S/NC_1e-3_1
2022-03-21 09:13:28,852|38564|application_1639015019875_1435|4486|TIME|layer1S|764_Census/S/NC_1e-3_1
2022-03-21 09:13:31,299|41011|Saved /tmp/edgesFAC.wkt in 0.26s [8776 records].
2022-03-21 09:13:34,686|44398|application_1639015019875_1435|5834|TIME|layer2S|764_Census/S/NC_1e-3_1
2022-03-21 09:13:37,599|47311|Saved /tmp/edgesFBC.wkt in 0.39s [10147 records].
2022-03-21 09:14:17,191|86903|Saved /tmp/edgesS.wkt in 0.33s [23676 records].
2022-03-21 09:14:20,357|90069|application_1639015019875_1435|45671|TIME|overlayS|764_Census/S/NC_1e-3_1
2022-03-21 09:14:22,005|91717|Saved /tmp/edgesFE.wkt in 0.39s [3571 records].
2022-03-21 09:14:22,006|91718|application_1639015019875_1435|1649|TIME|end|764_Census/S/NC_1e-3_1
hdfs dfs -mkdir Census/S/ND/
hdfs dfs -put ~/Datasets/Census/ND/ND2000.wkt Census/S/ND/A.wkt
hdfs dfs -put ~/Datasets/Census/ND/ND2010.wkt Census/S/ND/B.wkt
./QuadPart -d Census/S/ND -p 90 -t 1e-3
./Perf -d Census/S/ND -p 90 -t 1e-3 -n 1
DATASET    = Census/S/ND
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 90
Run 1 ./sdcel2_debug Census/S/ND/P90 /home/acald013/RIDIR/local_path/Census/S/ND/P90/ 1e-3 "90_Census/S/ND_1e-3_1"
2022-03-21 09:14:39,735|13742|application_1639015019875_1436|13988|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/ND/P90/edgesA --input2 Census/S/ND/P90/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/ND/P90//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/ND/P90//boundary.wkt --tolerance 1e-3 --qtag 90_Census/S/ND_1e-3_1 --debug --local
2022-03-21 09:14:39,816|13823|application_1639015019875_1436|INFO|scale=1000.0
2022-03-21 09:14:39,871|13878|Saved /tmp/edgesCells_90.wkt in 0.00s [262 records].
2022-03-21 09:14:39,871|13878|application_1639015019875_1436|INFO|npartitions=262
2022-03-21 09:14:39,872|13879|application_1639015019875_1436|137|TIME|start|90_Census/S/ND_1e-3_1
2022-03-21 09:14:50,485|24492|application_1639015019875_1436|INFO|nEdgesA=201897
2022-03-21 09:14:51,821|25828|application_1639015019875_1436|INFO|nEdgesB=186089
2022-03-21 09:14:51,822|25829|application_1639015019875_1436|11950|TIME|read|90_Census/S/ND_1e-3_1
2022-03-21 09:14:53,682|27689|application_1639015019875_1436|1860|TIME|layer1S|90_Census/S/ND_1e-3_1
2022-03-21 09:14:54,489|28496|Saved /tmp/edgesFAC.wkt in 0.04s [1030 records].
2022-03-21 09:14:55,966|29973|application_1639015019875_1436|2284|TIME|layer2S|90_Census/S/ND_1e-3_1
2022-03-21 09:14:56,716|30723|Saved /tmp/edgesFBC.wkt in 0.05s [953 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/OH/
hdfs dfs -put ~/Datasets/Census/OH/OH2000.wkt Census/S/OH/A.wkt
hdfs dfs -put ~/Datasets/Census/OH/OH2010.wkt Census/S/OH/B.wkt
./QuadPart -d Census/S/OH -p 439 -t 1e-3
./Perf -d Census/S/OH -p 439 -t 1e-3 -n 1
DATASET    = Census/S/OH
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 439
Run 1 ./sdcel2_debug Census/S/OH/P439 /home/acald013/RIDIR/local_path/Census/S/OH/P439/ 1e-3 "439_Census/S/OH_1e-3_1"
2022-03-21 09:15:42,629|13606|application_1639015019875_1437|13853|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/OH/P439/edgesA --input2 Census/S/OH/P439/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/OH/P439//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/OH/P439//boundary.wkt --tolerance 1e-3 --qtag 439_Census/S/OH_1e-3_1 --debug --local
2022-03-21 09:15:42,737|13714|application_1639015019875_1437|INFO|scale=1000.0
2022-03-21 09:15:42,876|13853|Saved /tmp/edgesCells_439.wkt in 0.00s [1138 records].
2022-03-21 09:15:42,877|13854|application_1639015019875_1437|INFO|npartitions=1138
2022-03-21 09:15:42,877|13854|application_1639015019875_1437|248|TIME|start|439_Census/S/OH_1e-3_1
2022-03-21 09:15:56,388|27365|application_1639015019875_1437|INFO|nEdgesA=906482
2022-03-21 09:15:59,769|30746|application_1639015019875_1437|INFO|nEdgesB=902531
2022-03-21 09:15:59,769|30746|application_1639015019875_1437|16892|TIME|read|439_Census/S/OH_1e-3_1
2022-03-21 09:16:02,309|33286|application_1639015019875_1437|2540|TIME|layer1S|439_Census/S/OH_1e-3_1
2022-03-21 09:16:04,370|35347|Saved /tmp/edgesFAC.wkt in 0.24s [8577 records].
2022-03-21 09:16:06,134|37111|application_1639015019875_1437|3825|TIME|layer2S|439_Census/S/OH_1e-3_1
2022-03-21 09:16:08,157|39134|Saved /tmp/edgesFBC.wkt in 0.30s [8602 records].
2022-03-21 09:16:34,190|65167|Saved /tmp/edgesS.wkt in 0.26s [19279 records].
2022-03-21 09:16:36,722|67699|application_1639015019875_1437|30588|TIME|overlayS|439_Census/S/OH_1e-3_1
2022-03-21 09:16:37,529|68506|Saved /tmp/edgesFE.wkt in 0.24s [4552 records].
2022-03-21 09:16:37,529|68506|application_1639015019875_1437|807|TIME|end|439_Census/S/OH_1e-3_1
hdfs dfs -mkdir Census/S/OK/
hdfs dfs -put ~/Datasets/Census/OK/OK2000.wkt Census/S/OK/A.wkt
hdfs dfs -put ~/Datasets/Census/OK/OK2010.wkt Census/S/OK/B.wkt
./QuadPart -d Census/S/OK -p 345 -t 1e-3
./Perf -d Census/S/OK -p 345 -t 1e-3 -n 1
DATASET    = Census/S/OK
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 345
Run 1 ./sdcel2_debug Census/S/OK/P345 /home/acald013/RIDIR/local_path/Census/S/OK/P345/ 1e-3 "345_Census/S/OK_1e-3_1"
2022-03-21 09:16:55,432|13614|application_1639015019875_1438|13858|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/OK/P345/edgesA --input2 Census/S/OK/P345/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/OK/P345//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/OK/P345//boundary.wkt --tolerance 1e-3 --qtag 345_Census/S/OK_1e-3_1 --debug --local
2022-03-21 09:16:55,526|13708|application_1639015019875_1438|INFO|scale=1000.0
2022-03-21 09:16:55,623|13805|Saved /tmp/edgesCells_345.wkt in 0.00s [907 records].
2022-03-21 09:16:55,623|13805|application_1639015019875_1438|INFO|npartitions=907
2022-03-21 09:16:55,623|13805|application_1639015019875_1438|191|TIME|start|345_Census/S/OK_1e-3_1
2022-03-21 09:17:08,919|27101|application_1639015019875_1438|INFO|nEdgesA=692222
2022-03-21 09:17:12,020|30202|application_1639015019875_1438|INFO|nEdgesB=710898
2022-03-21 09:17:12,020|30202|application_1639015019875_1438|16397|TIME|read|345_Census/S/OK_1e-3_1
2022-03-21 09:17:14,886|33068|application_1639015019875_1438|2866|TIME|layer1S|345_Census/S/OK_1e-3_1
2022-03-21 09:17:16,675|34857|Saved /tmp/edgesFAC.wkt in 0.10s [4305 records].
2022-03-21 09:17:18,611|36793|application_1639015019875_1438|3725|TIME|layer2S|345_Census/S/OK_1e-3_1
2022-03-21 09:17:20,525|38707|Saved /tmp/edgesFBC.wkt in 0.14s [4435 records].
2022-03-21 09:18:05,359|83541|Saved /tmp/edgesS.wkt in 0.16s [9677 records].
2022-03-21 09:18:07,238|85420|application_1639015019875_1438|48627|TIME|overlayS|345_Census/S/OK_1e-3_1
2022-03-21 09:18:07,924|86106|Saved /tmp/edgesFE.wkt in 0.19s [1393 records].
2022-03-21 09:18:07,924|86106|application_1639015019875_1438|686|TIME|end|345_Census/S/OK_1e-3_1
hdfs dfs -mkdir Census/S/OR/
hdfs dfs -put ~/Datasets/Census/OR/OR2000.wkt Census/S/OR/A.wkt
hdfs dfs -put ~/Datasets/Census/OR/OR2010.wkt Census/S/OR/B.wkt
./QuadPart -d Census/S/OR -p 464 -t 1e-3
./Perf -d Census/S/OR -p 464 -t 1e-3 -n 1
DATASET    = Census/S/OR
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 464
Run 1 ./sdcel2_debug Census/S/OR/P464 /home/acald013/RIDIR/local_path/Census/S/OR/P464/ 1e-3 "464_Census/S/OR_1e-3_1"
2022-03-21 09:18:26,825|13480|application_1639015019875_1439|13736|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/OR/P464/edgesA --input2 Census/S/OR/P464/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/OR/P464//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/OR/P464//boundary.wkt --tolerance 1e-3 --qtag 464_Census/S/OR_1e-3_1 --debug --local
2022-03-21 09:18:26,937|13592|application_1639015019875_1439|INFO|scale=1000.0
2022-03-21 09:18:27,048|13703|Saved /tmp/edgesCells_464.wkt in 0.00s [1285 records].
2022-03-21 09:18:27,049|13704|application_1639015019875_1439|INFO|npartitions=1285
2022-03-21 09:18:27,049|13704|application_1639015019875_1439|224|TIME|start|464_Census/S/OR_1e-3_1
2022-03-21 09:18:39,834|26489|application_1639015019875_1439|INFO|nEdgesA=929006
2022-03-21 09:18:42,903|29558|application_1639015019875_1439|INFO|nEdgesB=954536
2022-03-21 09:18:42,904|29559|application_1639015019875_1439|15855|TIME|read|464_Census/S/OR_1e-3_1
2022-03-21 09:18:46,228|32883|application_1639015019875_1439|3324|TIME|layer1S|464_Census/S/OR_1e-3_1
2022-03-21 09:18:48,192|34847|Saved /tmp/edgesFAC.wkt in 0.22s [4508 records].
2022-03-21 09:18:50,504|37159|application_1639015019875_1439|4276|TIME|layer2S|464_Census/S/OR_1e-3_1
2022-03-21 09:18:52,266|38921|Saved /tmp/edgesFBC.wkt in 0.16s [4703 records].
2022-03-21 09:19:25,417|72072|Saved /tmp/edgesS.wkt in 0.23s [10015 records].
2022-03-21 09:19:28,137|74792|application_1639015019875_1439|37633|TIME|overlayS|464_Census/S/OR_1e-3_1
2022-03-21 09:19:29,501|76156|Saved /tmp/edgesFE.wkt in 0.19s [1147 records].
2022-03-21 09:19:29,501|76156|application_1639015019875_1439|1364|TIME|end|464_Census/S/OR_1e-3_1
hdfs dfs -mkdir Census/S/PA/
hdfs dfs -put ~/Datasets/Census/PA/PA2000.wkt Census/S/PA/A.wkt
hdfs dfs -put ~/Datasets/Census/PA/PA2010.wkt Census/S/PA/B.wkt
./QuadPart -d Census/S/PA -p 585 -t 1e-3
./Perf -d Census/S/PA -p 585 -t 1e-3 -n 1
DATASET    = Census/S/PA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 585
Run 1 ./sdcel2_debug Census/S/PA/P585 /home/acald013/RIDIR/local_path/Census/S/PA/P585/ 1e-3 "585_Census/S/PA_1e-3_1"
2022-03-21 09:19:48,384|13412|application_1639015019875_1440|13657|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/PA/P585/edgesA --input2 Census/S/PA/P585/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/PA/P585//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/PA/P585//boundary.wkt --tolerance 1e-3 --qtag 585_Census/S/PA_1e-3_1 --debug --local
2022-03-21 09:19:48,502|13530|application_1639015019875_1440|INFO|scale=1000.0
2022-03-21 09:19:48,621|13649|Saved /tmp/edgesCells_585.wkt in 0.00s [1513 records].
2022-03-21 09:19:48,621|13649|application_1639015019875_1440|INFO|npartitions=1513
2022-03-21 09:19:48,622|13650|application_1639015019875_1440|238|TIME|start|585_Census/S/PA_1e-3_1
2022-03-21 09:20:02,341|27369|application_1639015019875_1440|INFO|nEdgesA=1221797
2022-03-21 09:20:06,203|31231|application_1639015019875_1440|INFO|nEdgesB=1205634
2022-03-21 09:20:06,203|31231|application_1639015019875_1440|17581|TIME|read|585_Census/S/PA_1e-3_1
2022-03-21 09:20:09,415|34443|application_1639015019875_1440|3212|TIME|layer1S|585_Census/S/PA_1e-3_1
2022-03-21 09:20:12,074|37102|Saved /tmp/edgesFAC.wkt in 0.30s [10655 records].
2022-03-21 09:20:14,305|39333|application_1639015019875_1440|4890|TIME|layer2S|585_Census/S/PA_1e-3_1
2022-03-21 09:20:16,551|41579|Saved /tmp/edgesFBC.wkt in 0.26s [10790 records].
2022-03-21 09:20:51,442|76470|Saved /tmp/edgesS.wkt in 0.46s [37107 records].
2022-03-21 09:20:54,708|79736|application_1639015019875_1440|40403|TIME|overlayS|585_Census/S/PA_1e-3_1
2022-03-21 09:20:55,762|80790|Saved /tmp/edgesFE.wkt in 0.28s [14670 records].
2022-03-21 09:20:55,762|80790|application_1639015019875_1440|1054|TIME|end|585_Census/S/PA_1e-3_1
hdfs dfs -mkdir Census/S/RI/
hdfs dfs -put ~/Datasets/Census/RI/RI2000.wkt Census/S/RI/A.wkt
hdfs dfs -put ~/Datasets/Census/RI/RI2010.wkt Census/S/RI/B.wkt
./QuadPart -d Census/S/RI -p 39 -t 1e-3
./Perf -d Census/S/RI -p 39 -t 1e-3 -n 1
DATASET    = Census/S/RI
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 39
Run 1 ./sdcel2_debug Census/S/RI/P39 /home/acald013/RIDIR/local_path/Census/S/RI/P39/ 1e-3 "39_Census/S/RI_1e-3_1"
2022-03-21 09:21:13,250|13615|application_1639015019875_1441|13864|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/RI/P39/edgesA --input2 Census/S/RI/P39/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/RI/P39//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/RI/P39//boundary.wkt --tolerance 1e-3 --qtag 39_Census/S/RI_1e-3_1 --debug --local
2022-03-21 09:21:13,313|13678|application_1639015019875_1441|INFO|scale=1000.0
2022-03-21 09:21:13,347|13712|Saved /tmp/edgesCells_39.wkt in 0.00s [103 records].
2022-03-21 09:21:13,347|13712|application_1639015019875_1441|INFO|npartitions=103
2022-03-21 09:21:13,348|13713|application_1639015019875_1441|98|TIME|start|39_Census/S/RI_1e-3_1
2022-03-21 09:21:23,432|23797|application_1639015019875_1441|INFO|nEdgesA=84871
2022-03-21 09:21:24,871|25236|application_1639015019875_1441|INFO|nEdgesB=81491
2022-03-21 09:21:24,871|25236|application_1639015019875_1441|11523|TIME|read|39_Census/S/RI_1e-3_1
2022-03-21 09:21:26,272|26637|application_1639015019875_1441|1401|TIME|layer1S|39_Census/S/RI_1e-3_1
2022-03-21 09:21:26,988|27353|Saved /tmp/edgesFAC.wkt in 0.03s [725 records].
2022-03-21 09:21:27,650|28015|application_1639015019875_1441|1378|TIME|layer2S|39_Census/S/RI_1e-3_1
2022-03-21 09:21:28,235|28600|Saved /tmp/edgesFBC.wkt in 0.03s [772 records].
2022-03-21 09:21:47,688|48053|Saved /tmp/edgesS.wkt in 0.04s [1862 records].
2022-03-21 09:21:49,224|49589|application_1639015019875_1441|21574|TIME|overlayS|39_Census/S/RI_1e-3_1
2022-03-21 09:21:49,419|49784|Saved /tmp/edgesFE.wkt in 0.03s [442 records].
2022-03-21 09:21:49,420|49785|application_1639015019875_1441|196|TIME|end|39_Census/S/RI_1e-3_1
hdfs dfs -mkdir Census/S/SC/
hdfs dfs -put ~/Datasets/Census/SC/SC2000.wkt Census/S/SC/A.wkt
hdfs dfs -put ~/Datasets/Census/SC/SC2010.wkt Census/S/SC/B.wkt
./QuadPart -d Census/S/SC -p 421 -t 1e-3
./Perf -d Census/S/SC -p 421 -t 1e-3 -n 1
DATASET    = Census/S/SC
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 421
Run 1 ./sdcel2_debug Census/S/SC/P421 /home/acald013/RIDIR/local_path/Census/S/SC/P421/ 1e-3 "421_Census/S/SC_1e-3_1"
2022-03-21 09:22:07,782|14176|application_1639015019875_1442|14422|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/SC/P421/edgesA --input2 Census/S/SC/P421/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/SC/P421//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/SC/P421//boundary.wkt --tolerance 1e-3 --qtag 421_Census/S/SC_1e-3_1 --debug --local
2022-03-21 09:22:07,880|14274|application_1639015019875_1442|INFO|scale=1000.0
2022-03-21 09:22:07,985|14379|Saved /tmp/edgesCells_421.wkt in 0.00s [1063 records].
2022-03-21 09:22:07,985|14379|application_1639015019875_1442|INFO|npartitions=1063
2022-03-21 09:22:07,985|14379|application_1639015019875_1442|203|TIME|start|421_Census/S/SC_1e-3_1
2022-03-21 09:22:20,842|27236|application_1639015019875_1442|INFO|nEdgesA=784762
2022-03-21 09:22:24,124|30518|application_1639015019875_1442|INFO|nEdgesB=868105
2022-03-21 09:22:24,125|30519|application_1639015019875_1442|16140|TIME|read|421_Census/S/SC_1e-3_1
2022-03-21 09:22:26,592|32986|application_1639015019875_1442|2467|TIME|layer1S|421_Census/S/SC_1e-3_1
2022-03-21 09:22:28,181|34575|Saved /tmp/edgesFAC.wkt in 0.17s [4729 records].
2022-03-21 09:22:30,080|36474|application_1639015019875_1442|3488|TIME|layer2S|421_Census/S/SC_1e-3_1
2022-03-21 09:22:31,821|38215|Saved /tmp/edgesFBC.wkt in 0.14s [5277 records].
2022-03-21 09:23:06,395|72789|Saved /tmp/edgesS.wkt in 0.25s [12195 records].
2022-03-21 09:23:08,091|74485|application_1639015019875_1442|38011|TIME|overlayS|421_Census/S/SC_1e-3_1
2022-03-21 09:23:08,857|75251|Saved /tmp/edgesFE.wkt in 0.20s [1726 records].
2022-03-21 09:23:08,857|75251|application_1639015019875_1442|766|TIME|end|421_Census/S/SC_1e-3_1
hdfs dfs -mkdir Census/S/SD/
hdfs dfs -put ~/Datasets/Census/SD/SD2000.wkt Census/S/SD/A.wkt
hdfs dfs -put ~/Datasets/Census/SD/SD2010.wkt Census/S/SD/B.wkt
./QuadPart -d Census/S/SD -p 164 -t 1e-3
./Perf -d Census/S/SD -p 164 -t 1e-3 -n 1
DATASET    = Census/S/SD
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 164
Run 1 ./sdcel2_debug Census/S/SD/P164 /home/acald013/RIDIR/local_path/Census/S/SD/P164/ 1e-3 "164_Census/S/SD_1e-3_1"
2022-03-21 09:23:27,129|13450|application_1639015019875_1443|13698|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/SD/P164/edgesA --input2 Census/S/SD/P164/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/SD/P164//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/SD/P164//boundary.wkt --tolerance 1e-3 --qtag 164_Census/S/SD_1e-3_1 --debug --local
2022-03-21 09:23:27,216|13537|application_1639015019875_1443|INFO|scale=1000.0
2022-03-21 09:23:27,290|13611|Saved /tmp/edgesCells_164.wkt in 0.00s [499 records].
2022-03-21 09:23:27,290|13611|application_1639015019875_1443|INFO|npartitions=499
2022-03-21 09:23:27,290|13611|application_1639015019875_1443|161|TIME|start|164_Census/S/SD_1e-3_1
2022-03-21 09:23:38,580|24901|application_1639015019875_1443|INFO|nEdgesA=374206
2022-03-21 09:23:40,380|26701|application_1639015019875_1443|INFO|nEdgesB=338269
2022-03-21 09:23:40,380|26701|application_1639015019875_1443|13090|TIME|read|164_Census/S/SD_1e-3_1
2022-03-21 09:23:42,648|28969|application_1639015019875_1443|2268|TIME|layer1S|164_Census/S/SD_1e-3_1
2022-03-21 09:23:43,930|30251|Saved /tmp/edgesFAC.wkt in 0.09s [1544 records].
2022-03-21 09:23:45,141|31462|application_1639015019875_1443|2493|TIME|layer2S|164_Census/S/SD_1e-3_1
2022-03-21 09:23:46,058|32379|Saved /tmp/edgesFBC.wkt in 0.09s [1436 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/TN/
hdfs dfs -put ~/Datasets/Census/TN/TN2000.wkt Census/S/TN/A.wkt
hdfs dfs -put ~/Datasets/Census/TN/TN2010.wkt Census/S/TN/B.wkt
./QuadPart -d Census/S/TN -p 693 -t 1e-3
./Perf -d Census/S/TN -p 693 -t 1e-3 -n 1
DATASET    = Census/S/TN
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 693
Run 1 ./sdcel2_debug Census/S/TN/P693 /home/acald013/RIDIR/local_path/Census/S/TN/P693/ 1e-3 "693_Census/S/TN_1e-3_1"
2022-03-21 09:24:50,746|13499|application_1639015019875_1444|13739|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/TN/P693/edgesA --input2 Census/S/TN/P693/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/TN/P693//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/TN/P693//boundary.wkt --tolerance 1e-3 --qtag 693_Census/S/TN_1e-3_1 --debug --local
2022-03-21 09:24:50,859|13612|application_1639015019875_1444|INFO|scale=1000.0
2022-03-21 09:24:50,984|13737|Saved /tmp/edgesCells_693.wkt in 0.00s [1777 records].
2022-03-21 09:24:50,984|13737|application_1639015019875_1444|INFO|npartitions=1777
2022-03-21 09:24:50,985|13738|application_1639015019875_1444|239|TIME|start|693_Census/S/TN_1e-3_1
2022-03-21 09:25:05,691|28444|application_1639015019875_1444|INFO|nEdgesA=1327280
2022-03-21 09:25:10,042|32795|application_1639015019875_1444|INFO|nEdgesB=1427991
2022-03-21 09:25:10,042|32795|application_1639015019875_1444|19057|TIME|read|693_Census/S/TN_1e-3_1
2022-03-21 09:25:13,937|36690|application_1639015019875_1444|3895|TIME|layer1S|693_Census/S/TN_1e-3_1
2022-03-21 09:25:16,643|39396|Saved /tmp/edgesFAC.wkt in 0.30s [7595 records].
2022-03-21 09:25:19,387|42140|application_1639015019875_1444|5450|TIME|layer2S|693_Census/S/TN_1e-3_1
2022-03-21 09:25:22,464|45217|Saved /tmp/edgesFBC.wkt in 0.40s [8167 records].
2022-03-21 09:25:48,886|71639|Saved /tmp/edgesS.wkt in 0.33s [19209 records].
2022-03-21 09:25:51,444|74197|application_1639015019875_1444|32057|TIME|overlayS|693_Census/S/TN_1e-3_1
2022-03-21 09:25:52,744|75497|Saved /tmp/edgesFE.wkt in 0.38s [2444 records].
2022-03-21 09:25:52,744|75497|application_1639015019875_1444|1300|TIME|end|693_Census/S/TN_1e-3_1
hdfs dfs -mkdir Census/S/TX/
hdfs dfs -put ~/Datasets/Census/TX/TX2000.wkt Census/S/TX/A.wkt
hdfs dfs -put ~/Datasets/Census/TX/TX2010.wkt Census/S/TX/B.wkt
./QuadPart -d Census/S/TX -p 1417 -t 1e-3
./Perf -d Census/S/TX -p 1417 -t 1e-3 -n 1
DATASET    = Census/S/TX
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 1417
Run 1 ./sdcel2_debug Census/S/TX/P1417 /home/acald013/RIDIR/local_path/Census/S/TX/P1417/ 1e-3 "1417_Census/S/TX_1e-3_1"
2022-03-21 09:26:12,175|13468|application_1639015019875_1445|13718|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/TX/P1417/edgesA --input2 Census/S/TX/P1417/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/TX/P1417//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/TX/P1417//boundary.wkt --tolerance 1e-3 --qtag 1417_Census/S/TX_1e-3_1 --debug --local
2022-03-21 09:26:12,333|13626|application_1639015019875_1445|INFO|scale=1000.0
2022-03-21 09:26:12,525|13818|Saved /tmp/edgesCells_1417.wkt in 0.01s [3871 records].
2022-03-21 09:26:12,525|13818|application_1639015019875_1445|INFO|npartitions=3871
2022-03-21 09:26:12,525|13818|application_1639015019875_1445|350|TIME|start|1417_Census/S/TX_1e-3_1
2022-03-21 09:26:31,375|32668|application_1639015019875_1445|INFO|nEdgesA=2869512
2022-03-21 09:26:40,266|41559|application_1639015019875_1445|INFO|nEdgesB=2914228
2022-03-21 09:26:40,267|41560|application_1639015019875_1445|27742|TIME|read|1417_Census/S/TX_1e-3_1
2022-03-21 09:26:47,829|49122|application_1639015019875_1445|7562|TIME|layer1S|1417_Census/S/TX_1e-3_1
2022-03-21 09:26:54,342|55635|Saved /tmp/edgesFAC.wkt in 0.81s [18465 records].
2022-03-21 09:27:01,289|62582|application_1639015019875_1445|13460|TIME|layer2S|1417_Census/S/TX_1e-3_1
2022-03-21 09:27:07,321|68614|Saved /tmp/edgesFBC.wkt in 0.78s [20048 records].
2022-03-21 09:27:56,635|117928|Saved /tmp/edgesS.wkt in 0.80s [45483 records].
2022-03-21 09:28:01,563|122856|application_1639015019875_1445|60274|TIME|overlayS|1417_Census/S/TX_1e-3_1
2022-03-21 09:28:04,327|125620|Saved /tmp/edgesFE.wkt in 1.04s [8684 records].
2022-03-21 09:28:04,327|125620|application_1639015019875_1445|2764|TIME|end|1417_Census/S/TX_1e-3_1
hdfs dfs -mkdir Census/S/UT/
hdfs dfs -put ~/Datasets/Census/UT/UT2000.wkt Census/S/UT/A.wkt
hdfs dfs -put ~/Datasets/Census/UT/UT2010.wkt Census/S/UT/B.wkt
./QuadPart -d Census/S/UT -p 237 -t 1e-3
./Perf -d Census/S/UT -p 237 -t 1e-3 -n 1
DATASET    = Census/S/UT
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 237
Run 1 ./sdcel2_debug Census/S/UT/P237 /home/acald013/RIDIR/local_path/Census/S/UT/P237/ 1e-3 "237_Census/S/UT_1e-3_1"
2022-03-21 09:28:22,415|13661|application_1639015019875_1446|13907|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/UT/P237/edgesA --input2 Census/S/UT/P237/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/UT/P237//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/UT/P237//boundary.wkt --tolerance 1e-3 --qtag 237_Census/S/UT_1e-3_1 --debug --local
2022-03-21 09:28:22,520|13766|application_1639015019875_1446|INFO|scale=1000.0
2022-03-21 09:28:22,606|13852|Saved /tmp/edgesCells_237.wkt in 0.00s [679 records].
2022-03-21 09:28:22,607|13853|application_1639015019875_1446|INFO|npartitions=679
2022-03-21 09:28:22,607|13853|application_1639015019875_1446|192|TIME|start|237_Census/S/UT_1e-3_1
2022-03-21 09:28:34,618|25864|application_1639015019875_1446|INFO|nEdgesA=483923
2022-03-21 09:28:36,770|28016|application_1639015019875_1446|INFO|nEdgesB=488004
2022-03-21 09:28:36,770|28016|application_1639015019875_1446|14163|TIME|read|237_Census/S/UT_1e-3_1
2022-03-21 09:28:39,036|30282|application_1639015019875_1446|2266|TIME|layer1S|237_Census/S/UT_1e-3_1
2022-03-21 09:28:40,391|31637|Saved /tmp/edgesFAC.wkt in 0.24s [2510 records].
2022-03-21 09:28:42,138|33384|application_1639015019875_1446|3102|TIME|layer2S|237_Census/S/UT_1e-3_1
2022-03-21 09:28:43,296|34542|Saved /tmp/edgesFBC.wkt in 0.14s [2658 records].
2022-03-21 09:29:33,009|84255|Saved /tmp/edgesS.wkt in 0.16s [5818 records].
2022-03-21 09:29:34,334|85580|application_1639015019875_1446|52196|TIME|overlayS|237_Census/S/UT_1e-3_1
2022-03-21 09:29:34,840|86086|Saved /tmp/edgesFE.wkt in 0.13s [905 records].
2022-03-21 09:29:34,840|86086|application_1639015019875_1446|506|TIME|end|237_Census/S/UT_1e-3_1
hdfs dfs -mkdir Census/S/VT/
hdfs dfs -put ~/Datasets/Census/VT/VT2000.wkt Census/S/VT/A.wkt
hdfs dfs -put ~/Datasets/Census/VT/VT2010.wkt Census/S/VT/B.wkt
./QuadPart -d Census/S/VT -p 32 -t 1e-3
./Perf -d Census/S/VT -p 32 -t 1e-3 -n 1
DATASET    = Census/S/VT
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 32
Run 1 ./sdcel2_debug Census/S/VT/P32 /home/acald013/RIDIR/local_path/Census/S/VT/P32/ 1e-3 "32_Census/S/VT_1e-3_1"
2022-03-21 09:29:53,069|13547|application_1639015019875_1447|13805|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/VT/P32/edgesA --input2 Census/S/VT/P32/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/VT/P32//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/VT/P32//boundary.wkt --tolerance 1e-3 --qtag 32_Census/S/VT_1e-3_1 --debug --local
2022-03-21 09:29:53,129|13607|application_1639015019875_1447|INFO|scale=1000.0
2022-03-21 09:29:53,158|13636|Saved /tmp/edgesCells_32.wkt in 0.00s [85 records].
2022-03-21 09:29:53,158|13636|application_1639015019875_1447|INFO|npartitions=85
2022-03-21 09:29:53,158|13636|application_1639015019875_1447|89|TIME|start|32_Census/S/VT_1e-3_1
2022-03-21 09:30:02,596|23074|application_1639015019875_1447|INFO|nEdgesA=65996
2022-03-21 09:30:03,683|24161|application_1639015019875_1447|INFO|nEdgesB=66771
2022-03-21 09:30:03,684|24162|application_1639015019875_1447|10526|TIME|read|32_Census/S/VT_1e-3_1
2022-03-21 09:30:04,815|25293|application_1639015019875_1447|1131|TIME|layer1S|32_Census/S/VT_1e-3_1
2022-03-21 09:30:05,346|25824|Saved /tmp/edgesFAC.wkt in 0.01s [536 records].
2022-03-21 09:30:05,942|26420|application_1639015019875_1447|1127|TIME|layer2S|32_Census/S/VT_1e-3_1
2022-03-21 09:30:06,439|26917|Saved /tmp/edgesFBC.wkt in 0.02s [548 records].
2022-03-21 09:30:33,708|54186|Saved /tmp/edgesS.wkt in 0.02s [1479 records].
2022-03-21 09:30:34,344|54822|application_1639015019875_1447|28402|TIME|overlayS|32_Census/S/VT_1e-3_1
2022-03-21 09:30:34,503|54981|Saved /tmp/edgesFE.wkt in 0.03s [496 records].
2022-03-21 09:30:34,503|54981|application_1639015019875_1447|159|TIME|end|32_Census/S/VT_1e-3_1
hdfs dfs -mkdir Census/S/VA/
hdfs dfs -put ~/Datasets/Census/VA/VA2000.wkt Census/S/VA/A.wkt
hdfs dfs -put ~/Datasets/Census/VA/VA2010.wkt Census/S/VA/B.wkt
./QuadPart -d Census/S/VA -p 655 -t 1e-3
./Perf -d Census/S/VA -p 655 -t 1e-3 -n 1
DATASET    = Census/S/VA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 655
Run 1 ./sdcel2_debug Census/S/VA/P655 /home/acald013/RIDIR/local_path/Census/S/VA/P655/ 1e-3 "655_Census/S/VA_1e-3_1"
2022-03-21 09:30:52,659|13653|application_1639015019875_1448|13904|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/VA/P655/edgesA --input2 Census/S/VA/P655/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/VA/P655//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/VA/P655//boundary.wkt --tolerance 1e-3 --qtag 655_Census/S/VA_1e-3_1 --debug --local
2022-03-21 09:30:52,783|13777|application_1639015019875_1448|INFO|scale=1000.0
2022-03-21 09:30:52,904|13898|Saved /tmp/edgesCells_655.wkt in 0.00s [1690 records].
2022-03-21 09:30:52,904|13898|application_1639015019875_1448|INFO|npartitions=1690
2022-03-21 09:30:52,904|13898|application_1639015019875_1448|245|TIME|start|655_Census/S/VA_1e-3_1
2022-03-21 09:31:06,729|27723|application_1639015019875_1448|INFO|nEdgesA=1258554
2022-03-21 09:31:10,727|31721|application_1639015019875_1448|INFO|nEdgesB=1347781
2022-03-21 09:31:10,727|31721|application_1639015019875_1448|17823|TIME|read|655_Census/S/VA_1e-3_1
2022-03-21 09:31:14,024|35018|application_1639015019875_1448|3297|TIME|layer1S|655_Census/S/VA_1e-3_1
2022-03-21 09:31:16,533|37527|Saved /tmp/edgesFAC.wkt in 0.19s [7715 records].
2022-03-21 09:31:19,027|40021|application_1639015019875_1448|5003|TIME|layer2S|655_Census/S/VA_1e-3_1
2022-03-21 09:31:21,794|42788|Saved /tmp/edgesFBC.wkt in 0.29s [8418 records].
2022-03-21 09:31:55,664|76658|Saved /tmp/edgesS.wkt in 0.29s [20196 records].
2022-03-21 09:32:00,411|81405|application_1639015019875_1448|41384|TIME|overlayS|655_Census/S/VA_1e-3_1
2022-03-21 09:32:01,658|82652|Saved /tmp/edgesFE.wkt in 0.33s [3531 records].
2022-03-21 09:32:01,659|82653|application_1639015019875_1448|1248|TIME|end|655_Census/S/VA_1e-3_1
hdfs dfs -mkdir Census/S/WA/
hdfs dfs -put ~/Datasets/Census/WA/WA2000.wkt Census/S/WA/A.wkt
hdfs dfs -put ~/Datasets/Census/WA/WA2010.wkt Census/S/WA/B.wkt
./QuadPart -d Census/S/WA -p 429 -t 1e-3
./Perf -d Census/S/WA -p 429 -t 1e-3 -n 1
DATASET    = Census/S/WA
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 429
Run 1 ./sdcel2_debug Census/S/WA/P429 /home/acald013/RIDIR/local_path/Census/S/WA/P429/ 1e-3 "429_Census/S/WA_1e-3_1"
2022-03-21 09:32:20,948|13946|application_1639015019875_1449|14195|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/WA/P429/edgesA --input2 Census/S/WA/P429/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/WA/P429//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/WA/P429//boundary.wkt --tolerance 1e-3 --qtag 429_Census/S/WA_1e-3_1 --debug --local
2022-03-21 09:32:21,060|14058|application_1639015019875_1449|INFO|scale=1000.0
2022-03-21 09:32:21,183|14181|Saved /tmp/edgesCells_429.wkt in 0.00s [1135 records].
2022-03-21 09:32:21,183|14181|application_1639015019875_1449|INFO|npartitions=1135
2022-03-21 09:32:21,183|14181|application_1639015019875_1449|235|TIME|start|429_Census/S/WA_1e-3_1
2022-03-21 09:32:33,797|26795|application_1639015019875_1449|INFO|nEdgesA=820157
2022-03-21 09:32:36,803|29801|application_1639015019875_1449|INFO|nEdgesB=883123
2022-03-21 09:32:36,804|29802|application_1639015019875_1449|15621|TIME|read|429_Census/S/WA_1e-3_1
2022-03-21 09:32:39,482|32480|application_1639015019875_1449|2678|TIME|layer1S|429_Census/S/WA_1e-3_1
2022-03-21 09:32:41,064|34062|Saved /tmp/edgesFAC.wkt in 0.13s [5490 records].
2022-03-21 09:32:42,887|35885|application_1639015019875_1449|3405|TIME|layer2S|429_Census/S/WA_1e-3_1
2022-03-21 09:32:44,783|37781|Saved /tmp/edgesFBC.wkt in 0.16s [5971 records].
2022-03-21 09:33:18,799|71797|Saved /tmp/edgesS.wkt in 0.23s [13015 records].
2022-03-21 09:33:21,112|74110|application_1639015019875_1449|38225|TIME|overlayS|429_Census/S/WA_1e-3_1
2022-03-21 09:33:22,010|75008|Saved /tmp/edgesFE.wkt in 0.21s [2186 records].
2022-03-21 09:33:22,010|75008|application_1639015019875_1449|898|TIME|end|429_Census/S/WA_1e-3_1
hdfs dfs -mkdir Census/S/WV/
hdfs dfs -put ~/Datasets/Census/WV/WV2000.wkt Census/S/WV/A.wkt
hdfs dfs -put ~/Datasets/Census/WV/WV2010.wkt Census/S/WV/B.wkt
./QuadPart -d Census/S/WV -p 365 -t 1e-3
./Perf -d Census/S/WV -p 365 -t 1e-3 -n 1
DATASET    = Census/S/WV
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 365
Run 1 ./sdcel2_debug Census/S/WV/P365 /home/acald013/RIDIR/local_path/Census/S/WV/P365/ 1e-3 "365_Census/S/WV_1e-3_1"
2022-03-21 09:33:40,928|13426|application_1639015019875_1450|13665|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/WV/P365/edgesA --input2 Census/S/WV/P365/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/WV/P365//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/WV/P365//boundary.wkt --tolerance 1e-3 --qtag 365_Census/S/WV_1e-3_1 --debug --local
2022-03-21 09:33:41,029|13527|application_1639015019875_1450|INFO|scale=1000.0
2022-03-21 09:33:41,133|13631|Saved /tmp/edgesCells_365.wkt in 0.00s [949 records].
2022-03-21 09:33:41,133|13631|application_1639015019875_1450|INFO|npartitions=949
2022-03-21 09:33:41,133|13631|application_1639015019875_1450|205|TIME|start|365_Census/S/WV_1e-3_1
2022-03-21 09:33:53,742|26240|application_1639015019875_1450|INFO|nEdgesA=729575
2022-03-21 09:33:57,005|29503|application_1639015019875_1450|INFO|nEdgesB=752375
2022-03-21 09:33:57,006|29504|application_1639015019875_1450|15873|TIME|read|365_Census/S/WV_1e-3_1
2022-03-21 09:33:59,745|32243|application_1639015019875_1450|2739|TIME|layer1S|365_Census/S/WV_1e-3_1
2022-03-21 09:34:01,272|33770|Saved /tmp/edgesFAC.wkt in 0.10s [3347 records].
2022-03-21 09:34:03,299|35797|application_1639015019875_1450|3554|TIME|layer2S|365_Census/S/WV_1e-3_1
2022-03-21 09:34:05,046|37544|Saved /tmp/edgesFBC.wkt in 0.17s [3400 records].
2022-03-21 09:34:58,315|90813|Saved /tmp/edgesS.wkt in 0.18s [8359 records].
2022-03-21 09:34:59,814|92312|application_1639015019875_1450|56515|TIME|overlayS|365_Census/S/WV_1e-3_1
2022-03-21 09:35:00,767|93265|Saved /tmp/edgesFE.wkt in 0.17s [1109 records].
2022-03-21 09:35:00,768|93266|application_1639015019875_1450|954|TIME|end|365_Census/S/WV_1e-3_1
hdfs dfs -mkdir Census/S/WI/
hdfs dfs -put ~/Datasets/Census/WI/WI2000.wkt Census/S/WI/A.wkt
hdfs dfs -put ~/Datasets/Census/WI/WI2010.wkt Census/S/WI/B.wkt
./QuadPart -d Census/S/WI -p 362 -t 1e-3
./Perf -d Census/S/WI -p 362 -t 1e-3 -n 1
DATASET    = Census/S/WI
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 362
Run 1 ./sdcel2_debug Census/S/WI/P362 /home/acald013/RIDIR/local_path/Census/S/WI/P362/ 1e-3 "362_Census/S/WI_1e-3_1"
2022-03-21 09:35:19,498|13552|application_1639015019875_1451|13798|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/WI/P362/edgesA --input2 Census/S/WI/P362/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/WI/P362//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/WI/P362//boundary.wkt --tolerance 1e-3 --qtag 362_Census/S/WI_1e-3_1 --debug --local
2022-03-21 09:35:19,649|13703|application_1639015019875_1451|INFO|scale=1000.0
2022-03-21 09:35:19,782|13836|Saved /tmp/edgesCells_362.wkt in 0.01s [970 records].
2022-03-21 09:35:19,783|13837|application_1639015019875_1451|INFO|npartitions=970
2022-03-21 09:35:19,783|13837|application_1639015019875_1451|285|TIME|start|362_Census/S/WI_1e-3_1
2022-03-21 09:35:32,703|26757|application_1639015019875_1451|INFO|nEdgesA=773292
2022-03-21 09:35:36,130|30184|application_1639015019875_1451|INFO|nEdgesB=745762
2022-03-21 09:35:36,131|30185|application_1639015019875_1451|16348|TIME|read|362_Census/S/WI_1e-3_1
2022-03-21 09:35:38,885|32939|application_1639015019875_1451|2754|TIME|layer1S|362_Census/S/WI_1e-3_1
2022-03-21 09:35:40,972|35026|Saved /tmp/edgesFAC.wkt in 0.21s [5108 records].
2022-03-21 09:35:42,773|36827|application_1639015019875_1451|3888|TIME|layer2S|362_Census/S/WI_1e-3_1
2022-03-21 09:35:44,460|38514|Saved /tmp/edgesFBC.wkt in 0.18s [5204 records].
2022-03-21 09:36:26,226|80280|Saved /tmp/edgesS.wkt in 0.25s [11508 records].
2022-03-21 09:36:28,264|82318|application_1639015019875_1451|45491|TIME|overlayS|362_Census/S/WI_1e-3_1
2022-03-21 09:36:29,037|83091|Saved /tmp/edgesFE.wkt in 0.20s [1948 records].
2022-03-21 09:36:29,037|83091|application_1639015019875_1451|773|TIME|end|362_Census/S/WI_1e-3_1
hdfs dfs -mkdir Census/S/WY/
hdfs dfs -put ~/Datasets/Census/WY/WY2000.wkt Census/S/WY/A.wkt
hdfs dfs -put ~/Datasets/Census/WY/WY2010.wkt Census/S/WY/B.wkt
./QuadPart -d Census/S/WY -p 109 -t 1e-3
./Perf -d Census/S/WY -p 109 -t 1e-3 -n 1
DATASET    = Census/S/WY
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 109
Run 1 ./sdcel2_debug Census/S/WY/P109 /home/acald013/RIDIR/local_path/Census/S/WY/P109/ 1e-3 "109_Census/S/WY_1e-3_1"
2022-03-21 09:36:47,409|13596|application_1639015019875_1452|13842|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/WY/P109/edgesA --input2 Census/S/WY/P109/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/WY/P109//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/WY/P109//boundary.wkt --tolerance 1e-3 --qtag 109_Census/S/WY_1e-3_1 --debug --local
2022-03-21 09:36:47,484|13671|application_1639015019875_1452|INFO|scale=1000.0
2022-03-21 09:36:47,544|13731|Saved /tmp/edgesCells_109.wkt in 0.00s [343 records].
2022-03-21 09:36:47,544|13731|application_1639015019875_1452|INFO|npartitions=343
2022-03-21 09:36:47,544|13731|application_1639015019875_1452|135|TIME|start|109_Census/S/WY_1e-3_1
2022-03-21 09:36:59,171|25358|application_1639015019875_1452|INFO|nEdgesA=225166
2022-03-21 09:37:00,960|27147|application_1639015019875_1452|INFO|nEdgesB=225574
2022-03-21 09:37:00,960|27147|application_1639015019875_1452|13416|TIME|read|109_Census/S/WY_1e-3_1
2022-03-21 09:37:02,427|28614|application_1639015019875_1452|1467|TIME|layer1S|109_Census/S/WY_1e-3_1
2022-03-21 09:37:03,197|29384|Saved /tmp/edgesFAC.wkt in 0.06s [973 records].
2022-03-21 09:37:04,118|30305|application_1639015019875_1452|1691|TIME|layer2S|109_Census/S/WY_1e-3_1
2022-03-21 09:37:04,806|30993|Saved /tmp/edgesFBC.wkt in 0.05s [982 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/AS/
hdfs dfs -put ~/Datasets/Census/AS/AS2000.wkt Census/S/AS/A.wkt
hdfs dfs -put ~/Datasets/Census/AS/AS2010.wkt Census/S/AS/B.wkt
./QuadPart -d Census/S/AS -p 1 -t 1e-3
./Perf -d Census/S/AS -p 1 -t 1e-3 -n 1
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 09:37:47,094|13838|application_1639015019875_1453|14122|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 09:37:47,141|13885|application_1639015019875_1453|INFO|scale=1000.0
2022-03-21 09:37:47,147|13891|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 09:37:47,147|13891|application_1639015019875_1453|INFO|npartitions=1
2022-03-21 09:37:47,147|13891|application_1639015019875_1453|53|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 09:37:55,923|22667|application_1639015019875_1453|INFO|nEdgesA=4884
2022-03-21 09:38:00,957|27701|application_1639015019875_1453|INFO|nEdgesB=3306
2022-03-21 09:38:00,957|27701|application_1639015019875_1453|13810|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 09:38:01,779|28523|application_1639015019875_1453|822|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 09:38:02,775|29519|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 09:38:03,498|30242|application_1639015019875_1453|1719|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 09:38:04,174|30918|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
hdfs dfs -mkdir Census/S/GU/
hdfs dfs -put ~/Datasets/Census/GU/GU2000.wkt Census/S/GU/A.wkt
hdfs dfs -put ~/Datasets/Census/GU/GU2010.wkt Census/S/GU/B.wkt
./QuadPart -d Census/S/GU -p 26 -t 1e-3
./Perf -d Census/S/GU -p 26 -t 1e-3 -n 1
DATASET    = Census/S/GU
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 26
Run 1 ./sdcel2_debug Census/S/GU/P26 /home/acald013/RIDIR/local_path/Census/S/GU/P26/ 1e-3 "26_Census/S/GU_1e-3_1"
2022-03-21 09:39:23,654|21930|application_1639015019875_1454|22178|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/GU/P26/edgesA --input2 Census/S/GU/P26/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/GU/P26//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/GU/P26//boundary.wkt --tolerance 1e-3 --qtag 26_Census/S/GU_1e-3_1 --debug --local
2022-03-21 09:39:23,720|21996|application_1639015019875_1454|INFO|scale=1000.0
2022-03-21 09:39:23,755|22031|Saved /tmp/edgesCells_26.wkt in 0.00s [76 records].
2022-03-21 09:39:23,756|22032|application_1639015019875_1454|INFO|npartitions=76
2022-03-21 09:39:23,756|22032|application_1639015019875_1454|102|TIME|start|26_Census/S/GU_1e-3_1
2022-03-21 09:39:33,041|31317|application_1639015019875_1454|INFO|nEdgesA=16826
2022-03-21 09:39:34,448|32724|application_1639015019875_1454|INFO|nEdgesB=53878
2022-03-21 09:39:34,449|32725|application_1639015019875_1454|10693|TIME|read|26_Census/S/GU_1e-3_1
2022-03-21 09:39:35,129|33405|application_1639015019875_1454|680|TIME|layer1S|26_Census/S/GU_1e-3_1
2022-03-21 09:39:35,428|33704|Saved /tmp/edgesFAC.wkt in 0.01s [301 records].
2022-03-21 09:39:36,192|34468|application_1639015019875_1454|1063|TIME|layer2S|26_Census/S/GU_1e-3_1
2022-03-21 09:39:36,726|35002|Saved /tmp/edgesFBC.wkt in 0.02s [356 records].
2022-03-21 09:39:53,805|52081|Saved /tmp/edgesS.wkt in 0.02s [1133 records].
2022-03-21 09:39:54,376|52652|application_1639015019875_1454|18184|TIME|overlayS|26_Census/S/GU_1e-3_1
2022-03-21 09:39:54,489|52765|Saved /tmp/edgesFE.wkt in 0.02s [260 records].
2022-03-21 09:39:54,489|52765|application_1639015019875_1454|113|TIME|end|26_Census/S/GU_1e-3_1
hdfs dfs -mkdir Census/S/MP/
hdfs dfs -put ~/Datasets/Census/MP/MP2000.wkt Census/S/MP/A.wkt
hdfs dfs -put ~/Datasets/Census/MP/MP2010.wkt Census/S/MP/B.wkt
./QuadPart -d Census/S/MP -p 18 -t 1e-3
./Perf -d Census/S/MP -p 18 -t 1e-3 -n 1
DATASET    = Census/S/MP
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 18
Run 1 ./sdcel2_debug Census/S/MP/P18 /home/acald013/RIDIR/local_path/Census/S/MP/P18/ 1e-3 "18_Census/S/MP_1e-3_1"
2022-03-21 09:40:13,082|13904|application_1639015019875_1455|14151|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/MP/P18/edgesA --input2 Census/S/MP/P18/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/MP/P18//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/MP/P18//boundary.wkt --tolerance 1e-3 --qtag 18_Census/S/MP_1e-3_1 --debug --local
2022-03-21 09:40:13,142|13964|application_1639015019875_1455|INFO|scale=1000.0
2022-03-21 09:40:13,165|13987|Saved /tmp/edgesCells_18.wkt in 0.00s [67 records].
2022-03-21 09:40:13,166|13988|application_1639015019875_1455|INFO|npartitions=67
2022-03-21 09:40:13,166|13988|application_1639015019875_1455|84|TIME|start|18_Census/S/MP_1e-3_1
2022-03-21 09:40:22,122|22944|application_1639015019875_1455|INFO|nEdgesA=8362
2022-03-21 09:40:23,334|24156|application_1639015019875_1455|INFO|nEdgesB=37108
2022-03-21 09:40:23,334|24156|application_1639015019875_1455|10168|TIME|read|18_Census/S/MP_1e-3_1
2022-03-21 09:40:24,234|25056|application_1639015019875_1455|900|TIME|layer1S|18_Census/S/MP_1e-3_1
2022-03-21 09:40:24,482|25304|Saved /tmp/edgesFAC.wkt in 0.00s [136 records].
2022-03-21 09:40:25,356|26178|application_1639015019875_1455|1122|TIME|layer2S|18_Census/S/MP_1e-3_1
2022-03-21 09:40:25,849|26671|Saved /tmp/edgesFBC.wkt in 0.01s [205 records].
2022-03-21 09:40:43,758|44580|Saved /tmp/edgesS.wkt in 0.01s [544 records].
2022-03-21 09:40:45,017|45839|application_1639015019875_1455|19661|TIME|overlayS|18_Census/S/MP_1e-3_1
2022-03-21 09:40:45,184|46006|Saved /tmp/edgesFE.wkt in 0.01s [84 records].
2022-03-21 09:40:45,184|46006|application_1639015019875_1455|167|TIME|end|18_Census/S/MP_1e-3_1
hdfs dfs -mkdir Census/S/PR/
hdfs dfs -put ~/Datasets/Census/PR/PR2000.wkt Census/S/PR/A.wkt
hdfs dfs -put ~/Datasets/Census/PR/PR2010.wkt Census/S/PR/B.wkt
./QuadPart -d Census/S/PR -p 169 -t 1e-3
./Perf -d Census/S/PR -p 169 -t 1e-3 -n 1
DATASET    = Census/S/PR
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 169
Run 1 ./sdcel2_debug Census/S/PR/P169 /home/acald013/RIDIR/local_path/Census/S/PR/P169/ 1e-3 "169_Census/S/PR_1e-3_1"
2022-03-21 09:41:03,334|14102|application_1639015019875_1456|14350|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/PR/P169/edgesA --input2 Census/S/PR/P169/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/PR/P169//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/PR/P169//boundary.wkt --tolerance 1e-3 --qtag 169_Census/S/PR_1e-3_1 --debug --local
2022-03-21 09:41:03,411|14179|application_1639015019875_1456|INFO|scale=1000.0
2022-03-21 09:41:03,478|14246|Saved /tmp/edgesCells_169.wkt in 0.00s [427 records].
2022-03-21 09:41:03,478|14246|application_1639015019875_1456|INFO|npartitions=427
2022-03-21 09:41:03,478|14246|application_1639015019875_1456|144|TIME|start|169_Census/S/PR_1e-3_1
2022-03-21 09:41:14,798|25566|application_1639015019875_1456|INFO|nEdgesA=334657
2022-03-21 09:41:16,640|27408|application_1639015019875_1456|INFO|nEdgesB=348645
2022-03-21 09:41:16,640|27408|application_1639015019875_1456|13162|TIME|read|169_Census/S/PR_1e-3_1
2022-03-21 09:41:18,275|29043|application_1639015019875_1456|1635|TIME|layer1S|169_Census/S/PR_1e-3_1
2022-03-21 09:41:19,315|30083|Saved /tmp/edgesFAC.wkt in 0.07s [3033 records].
2022-03-21 09:41:20,613|31381|application_1639015019875_1456|2338|TIME|layer2S|169_Census/S/PR_1e-3_1
2022-03-21 09:41:21,922|32690|Saved /tmp/edgesFBC.wkt in 0.08s [3188 records].
2022-03-21 09:41:26,467|37235|Saved /tmp/edgesS.wkt in 0.09s [7455 records].
2022-03-21 09:41:27,409|38177|application_1639015019875_1456|6796|TIME|overlayS|169_Census/S/PR_1e-3_1
2022-03-21 09:41:27,793|38561|Saved /tmp/edgesFE.wkt in 0.08s [1636 records].
2022-03-21 09:41:27,794|38562|application_1639015019875_1456|384|TIME|end|169_Census/S/PR_1e-3_1
hdfs dfs -mkdir Census/S/VI/
hdfs dfs -put ~/Datasets/Census/VI/VI2000.wkt Census/S/VI/A.wkt
hdfs dfs -put ~/Datasets/Census/VI/VI2010.wkt Census/S/VI/B.wkt
./QuadPart -d Census/S/VI -p 19 -t 1e-3
./Perf -d Census/S/VI -p 19 -t 1e-3 -n 1
DATASET    = Census/S/VI
TOLERANCE  = 1e-3
ITERATIONS = 1
PARTITIONS = 19
Run 1 ./sdcel2_debug Census/S/VI/P19 /home/acald013/RIDIR/local_path/Census/S/VI/P19/ 1e-3 "19_Census/S/VI_1e-3_1"
2022-03-21 09:41:45,177|13580|application_1639015019875_1457|13848|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/VI/P19/edgesA --input2 Census/S/VI/P19/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/VI/P19//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/VI/P19//boundary.wkt --tolerance 1e-3 --qtag 19_Census/S/VI_1e-3_1 --debug --local
2022-03-21 09:41:45,232|13635|application_1639015019875_1457|INFO|scale=1000.0
2022-03-21 09:41:45,255|13658|Saved /tmp/edgesCells_19.wkt in 0.00s [58 records].
2022-03-21 09:41:45,256|13659|application_1639015019875_1457|INFO|npartitions=58
2022-03-21 09:41:45,256|13659|application_1639015019875_1457|79|TIME|start|19_Census/S/VI_1e-3_1
2022-03-21 09:41:54,596|22999|application_1639015019875_1457|INFO|nEdgesA=58638
2022-03-21 09:41:55,593|23996|application_1639015019875_1457|INFO|nEdgesB=40172
2022-03-21 09:41:55,593|23996|application_1639015019875_1457|10337|TIME|read|19_Census/S/VI_1e-3_1
2022-03-21 09:41:57,049|25452|application_1639015019875_1457|1456|TIME|layer1S|19_Census/S/VI_1e-3_1
2022-03-21 09:41:57,821|26224|Saved /tmp/edgesFAC.wkt in 0.01s [265 records].
2022-03-21 09:41:58,849|27252|application_1639015019875_1457|1800|TIME|layer2S|19_Census/S/VI_1e-3_1
2022-03-21 09:41:59,765|28168|Saved /tmp/edgesFBC.wkt in 0.01s [334 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 18 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
