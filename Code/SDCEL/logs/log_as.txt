DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 10:32:22,871|2707|local-1647883941462|3044|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 10:32:22,917|2753|local-1647883941462|INFO|scale=1000.0
2022-03-21 10:32:22,922|2758|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:32:22,923|2759|local-1647883941462|INFO|npartitions=1
2022-03-21 10:32:22,923|2759|local-1647883941462|52|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 10:32:29,665|9501|local-1647883941462|INFO|nEdgesA=4884
2022-03-21 10:32:30,195|10031|local-1647883941462|INFO|nEdgesB=3306
2022-03-21 10:32:30,195|10031|local-1647883941462|7272|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 10:32:30,961|10797|local-1647883941462|766|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 10:32:32,100|11936|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:32:32,334|12170|local-1647883941462|1373|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 10:32:33,101|12937|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:32:36,616|16452|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:32:36,885|16721|local-1647883941462|4551|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 10:32:36,945|16781|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:32:36,945|16781|local-1647883941462|60|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 10:32:43,200|2600|local-1647883961793|2843|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 10:32:43,246|2646|local-1647883961793|INFO|scale=1000.0
2022-03-21 10:32:43,251|2651|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:32:43,251|2651|local-1647883961793|INFO|npartitions=1
2022-03-21 10:32:43,252|2652|local-1647883961793|52|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 10:32:49,737|9137|local-1647883961793|INFO|nEdgesA=4884
2022-03-21 10:32:50,165|9565|local-1647883961793|INFO|nEdgesB=3306
2022-03-21 10:32:50,165|9565|local-1647883961793|6913|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 10:32:50,769|10169|local-1647883961793|604|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 10:32:52,088|11488|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:32:52,314|11714|local-1647883961793|1545|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 10:32:53,078|12478|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:32:56,312|15712|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:32:56,565|15965|local-1647883961793|4251|TIME|overlayS|1_Census/S/AS_1e-3_2
2022-03-21 10:32:56,614|16014|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:32:56,615|16015|local-1647883961793|50|TIME|end|1_Census/S/AS_1e-3_2
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 10:33:02,916|2702|local-1647883981476|2945|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 10:33:02,967|2753|local-1647883981476|INFO|scale=1000.0
2022-03-21 10:33:02,973|2759|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:33:02,973|2759|local-1647883981476|INFO|npartitions=1
2022-03-21 10:33:02,973|2759|local-1647883981476|57|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 10:33:09,693|9479|local-1647883981476|INFO|nEdgesA=4884
2022-03-21 10:33:10,141|9927|local-1647883981476|INFO|nEdgesB=3306
2022-03-21 10:33:10,141|9927|local-1647883981476|7168|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 10:33:10,901|10687|local-1647883981476|760|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 10:33:12,042|11828|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:33:12,268|12054|local-1647883981476|1367|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 10:33:12,994|12780|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:33:16,736|16522|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:33:17,048|16834|local-1647883981476|4780|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 10:33:17,138|16924|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 10:33:17,138|16924|local-1647883981476|90|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 10:33:23,798|2856|local-1647884002162|3108|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 10:33:23,846|2904|local-1647884002162|INFO|scale=1000.0
2022-03-21 10:33:23,853|2911|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:33:23,853|2911|local-1647884002162|INFO|npartitions=1
2022-03-21 10:33:23,854|2912|local-1647884002162|56|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 10:33:30,880|9938|local-1647884002162|INFO|nEdgesA=4884
2022-03-21 10:33:31,307|10365|local-1647884002162|INFO|nEdgesB=3306
2022-03-21 10:33:31,307|10365|local-1647884002162|7453|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 10:33:32,092|11150|local-1647884002162|785|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 10:33:33,269|12327|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:33:33,487|12545|local-1647884002162|1395|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 10:33:34,177|13235|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:33:37,508|16566|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:33:37,789|16847|local-1647884002162|4302|TIME|overlayS|1_Census/S/AS_1e-3_4
2022-03-21 10:33:37,845|16903|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:33:37,845|16903|local-1647884002162|56|TIME|end|1_Census/S/AS_1e-3_4
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 10:33:44,018|2601|local-1647884022606|2848|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 10:33:44,064|2647|local-1647884022606|INFO|scale=1000.0
2022-03-21 10:33:44,070|2653|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:33:44,070|2653|local-1647884022606|INFO|npartitions=1
2022-03-21 10:33:44,070|2653|local-1647884022606|52|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 10:33:50,563|9146|local-1647884022606|INFO|nEdgesA=4884
2022-03-21 10:33:50,994|9577|local-1647884022606|INFO|nEdgesB=3306
2022-03-21 10:33:50,995|9578|local-1647884022606|6925|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 10:33:51,727|10310|local-1647884022606|732|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 10:33:52,859|11442|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:33:53,044|11627|local-1647884022606|1317|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 10:33:53,658|12241|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:33:57,153|15736|Saved /tmp/edgesS.wkt in 0.01s [73 records].
2022-03-21 10:33:57,438|16021|local-1647884022606|4394|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 10:33:57,494|16077|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:33:57,494|16077|local-1647884022606|56|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 10:34:03,904|2735|local-1647884042422|2981|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 10:34:03,950|2781|local-1647884042422|INFO|scale=1000.0
2022-03-21 10:34:03,955|2786|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:34:03,956|2787|local-1647884042422|INFO|npartitions=1
2022-03-21 10:34:03,956|2787|local-1647884042422|52|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 10:34:10,530|9361|local-1647884042422|INFO|nEdgesA=4884
2022-03-21 10:34:10,956|9787|local-1647884042422|INFO|nEdgesB=3306
2022-03-21 10:34:10,956|9787|local-1647884042422|7000|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 10:34:11,733|10564|local-1647884042422|777|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 10:34:12,953|11784|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:34:13,231|12062|local-1647884042422|1498|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 10:34:13,964|12795|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:34:17,934|16765|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:34:18,223|17054|local-1647884042422|4992|TIME|overlayS|1_Census/S/AS_1e-3_6
2022-03-21 10:34:18,286|17117|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:34:18,286|17117|local-1647884042422|63|TIME|end|1_Census/S/AS_1e-3_6
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 10:34:24,806|2589|local-1647884063397|2833|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 10:34:24,857|2640|local-1647884063397|INFO|scale=1000.0
2022-03-21 10:34:24,866|2649|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:34:24,866|2649|local-1647884063397|INFO|npartitions=1
2022-03-21 10:34:24,866|2649|local-1647884063397|60|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 10:34:31,523|9306|local-1647884063397|INFO|nEdgesA=4884
2022-03-21 10:34:31,983|9766|local-1647884063397|INFO|nEdgesB=3306
2022-03-21 10:34:31,983|9766|local-1647884063397|7117|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 10:34:32,708|10491|local-1647884063397|725|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 10:34:33,825|11608|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:34:34,051|11834|local-1647884063397|1343|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 10:34:34,739|12522|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:34:38,554|16337|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:34:38,865|16648|local-1647884063397|4814|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 10:34:38,944|16727|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:34:38,944|16727|local-1647884063397|79|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 10:34:45,225|2720|local-1647884083761|2970|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 10:34:45,274|2769|local-1647884083761|INFO|scale=1000.0
2022-03-21 10:34:45,280|2775|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:34:45,281|2776|local-1647884083761|INFO|npartitions=1
2022-03-21 10:34:45,281|2776|local-1647884083761|56|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 10:34:52,031|9526|local-1647884083761|INFO|nEdgesA=4884
2022-03-21 10:34:52,480|9975|local-1647884083761|INFO|nEdgesB=3306
2022-03-21 10:34:52,480|9975|local-1647884083761|7199|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 10:34:53,295|10790|local-1647884083761|815|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 10:34:54,590|12085|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:34:54,832|12327|local-1647884083761|1537|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 10:34:55,570|13065|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:34:59,442|16937|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:34:59,752|17247|local-1647884083761|4920|TIME|overlayS|1_Census/S/AS_1e-3_8
2022-03-21 10:34:59,822|17317|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:34:59,823|17318|local-1647884083761|71|TIME|end|1_Census/S/AS_1e-3_8
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 10:35:05,995|2596|local-1647884104586|2845|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 10:35:06,048|2649|local-1647884104586|INFO|scale=1000.0
2022-03-21 10:35:06,057|2658|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:35:06,057|2658|local-1647884104586|INFO|npartitions=1
2022-03-21 10:35:06,057|2658|local-1647884104586|62|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 10:35:12,585|9186|local-1647884104586|INFO|nEdgesA=4884
2022-03-21 10:35:13,016|9617|local-1647884104586|INFO|nEdgesB=3306
2022-03-21 10:35:13,016|9617|local-1647884104586|6959|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 10:35:13,698|10299|local-1647884104586|682|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 10:35:14,749|11350|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:35:14,929|11530|local-1647884104586|1231|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 10:35:15,532|12133|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:35:18,827|15428|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:35:19,179|15780|local-1647884104586|4250|TIME|overlayS|1_Census/S/AS_1e-3_9
2022-03-21 10:35:19,264|15865|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:35:19,264|15865|local-1647884104586|85|TIME|end|1_Census/S/AS_1e-3_9
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 10:35:25,533|2587|local-1647884124121|2832|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[1] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 10:35:25,582|2636|local-1647884124121|INFO|scale=1000.0
2022-03-21 10:35:25,588|2642|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:35:25,588|2642|local-1647884124121|INFO|npartitions=1
2022-03-21 10:35:25,588|2642|local-1647884124121|55|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 10:35:32,319|9373|local-1647884124121|INFO|nEdgesA=4884
2022-03-21 10:35:32,770|9824|local-1647884124121|INFO|nEdgesB=3306
2022-03-21 10:35:32,770|9824|local-1647884124121|7182|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 10:35:33,494|10548|local-1647884124121|724|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 10:35:34,708|11762|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:35:34,920|11974|local-1647884124121|1426|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 10:35:35,582|12636|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:35:39,120|16174|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:35:39,386|16440|local-1647884124121|4466|TIME|overlayS|1_Census/S/AS_1e-3_10
2022-03-21 10:35:39,459|16513|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:35:39,459|16513|local-1647884124121|73|TIME|end|1_Census/S/AS_1e-3_10
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 10:38:51,656|2694|local-1647884330224|2938|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 10:38:51,702|2740|local-1647884330224|INFO|scale=1000.0
2022-03-21 10:38:51,708|2746|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:38:51,708|2746|local-1647884330224|INFO|npartitions=1
2022-03-21 10:38:51,708|2746|local-1647884330224|52|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 10:38:58,243|9281|local-1647884330224|INFO|nEdgesA=4884
2022-03-21 10:38:58,674|9712|local-1647884330224|INFO|nEdgesB=3306
2022-03-21 10:38:58,674|9712|local-1647884330224|6966|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 10:38:59,376|10414|local-1647884330224|702|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 10:39:00,540|11578|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:39:00,831|11869|local-1647884330224|1455|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 10:39:01,439|12477|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:39:04,854|15892|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:39:05,187|16225|local-1647884330224|4356|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 10:39:05,259|16297|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:39:05,259|16297|local-1647884330224|72|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 10:39:11,527|2674|local-1647884350035|2916|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 10:39:11,582|2729|local-1647884350035|INFO|scale=1000.0
2022-03-21 10:39:11,590|2737|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:39:11,590|2737|local-1647884350035|INFO|npartitions=1
2022-03-21 10:39:11,590|2737|local-1647884350035|63|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 10:39:18,458|9605|local-1647884350035|INFO|nEdgesA=4884
2022-03-21 10:39:18,969|10116|local-1647884350035|INFO|nEdgesB=3306
2022-03-21 10:39:18,969|10116|local-1647884350035|7379|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 10:39:19,782|10929|local-1647884350035|813|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 10:39:21,024|12171|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:39:21,268|12415|local-1647884350035|1486|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 10:39:22,059|13206|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:39:25,659|16806|Saved /tmp/edgesS.wkt in 0.01s [73 records].
2022-03-21 10:39:26,050|17197|local-1647884350035|4782|TIME|overlayS|1_Census/S/AS_1e-3_2
2022-03-21 10:39:26,110|17257|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:39:26,110|17257|local-1647884350035|60|TIME|end|1_Census/S/AS_1e-3_2
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 10:39:32,397|2560|local-1647884371003|2804|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 10:39:32,448|2611|local-1647884371003|INFO|scale=1000.0
2022-03-21 10:39:32,455|2618|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:39:32,455|2618|local-1647884371003|INFO|npartitions=1
2022-03-21 10:39:32,456|2619|local-1647884371003|59|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 10:39:38,969|9132|local-1647884371003|INFO|nEdgesA=4884
2022-03-21 10:39:39,451|9614|local-1647884371003|INFO|nEdgesB=3306
2022-03-21 10:39:39,451|9614|local-1647884371003|6995|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 10:39:40,163|10326|local-1647884371003|712|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 10:39:41,184|11347|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:39:41,378|11541|local-1647884371003|1215|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 10:39:41,995|12158|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:39:45,235|15398|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:39:45,518|15681|local-1647884371003|4140|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 10:39:45,580|15743|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 10:39:45,580|15743|local-1647884371003|62|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 10:39:51,870|2674|local-1647884390455|2920|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 10:39:51,916|2720|local-1647884390455|INFO|scale=1000.0
2022-03-21 10:39:51,922|2726|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:39:51,922|2726|local-1647884390455|INFO|npartitions=1
2022-03-21 10:39:51,922|2726|local-1647884390455|52|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 10:39:58,425|9229|local-1647884390455|INFO|nEdgesA=4884
2022-03-21 10:39:58,988|9792|local-1647884390455|INFO|nEdgesB=3306
2022-03-21 10:39:58,988|9792|local-1647884390455|7066|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 10:39:59,723|10527|local-1647884390455|735|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 10:40:00,828|11632|Saved /tmp/edgesFAC.wkt in 0.01s [21 records].
2022-03-21 10:40:01,031|11835|local-1647884390455|1308|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 10:40:01,672|12476|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:40:04,783|15587|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:40:05,070|15874|local-1647884390455|4039|TIME|overlayS|1_Census/S/AS_1e-3_4
2022-03-21 10:40:05,127|15931|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:40:05,128|15932|local-1647884390455|58|TIME|end|1_Census/S/AS_1e-3_4
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 10:40:11,449|2685|local-1647884409923|2941|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 10:40:11,496|2732|local-1647884409923|INFO|scale=1000.0
2022-03-21 10:40:11,502|2738|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:40:11,502|2738|local-1647884409923|INFO|npartitions=1
2022-03-21 10:40:11,502|2738|local-1647884409923|53|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 10:40:18,365|9601|local-1647884409923|INFO|nEdgesA=4884
2022-03-21 10:40:18,819|10055|local-1647884409923|INFO|nEdgesB=3306
2022-03-21 10:40:18,819|10055|local-1647884409923|7317|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 10:40:19,608|10844|local-1647884409923|789|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 10:40:20,998|12234|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:40:21,219|12455|local-1647884409923|1611|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 10:40:21,956|13192|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:40:25,368|16604|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:40:25,627|16863|local-1647884409923|4408|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 10:40:25,689|16925|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:40:25,689|16925|local-1647884409923|62|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 10:40:32,023|2660|local-1647884430563|2904|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 10:40:32,071|2708|local-1647884430563|INFO|scale=1000.0
2022-03-21 10:40:32,077|2714|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:40:32,077|2714|local-1647884430563|INFO|npartitions=1
2022-03-21 10:40:32,077|2714|local-1647884430563|55|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 10:40:38,681|9318|local-1647884430563|INFO|nEdgesA=4884
2022-03-21 10:40:39,132|9769|local-1647884430563|INFO|nEdgesB=3306
2022-03-21 10:40:39,132|9769|local-1647884430563|7055|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 10:40:39,866|10503|local-1647884430563|734|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 10:40:40,976|11613|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:40:41,171|11808|local-1647884430563|1305|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 10:40:41,822|12459|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:40:45,440|16077|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:40:45,733|16370|local-1647884430563|4562|TIME|overlayS|1_Census/S/AS_1e-3_6
2022-03-21 10:40:45,799|16436|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:40:45,799|16436|local-1647884430563|66|TIME|end|1_Census/S/AS_1e-3_6
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 10:40:52,114|2620|local-1647884450656|2871|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 10:40:52,162|2668|local-1647884450656|INFO|scale=1000.0
2022-03-21 10:40:52,168|2674|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:40:52,168|2674|local-1647884450656|INFO|npartitions=1
2022-03-21 10:40:52,168|2674|local-1647884450656|54|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 10:40:58,946|9452|local-1647884450656|INFO|nEdgesA=4884
2022-03-21 10:40:59,389|9895|local-1647884450656|INFO|nEdgesB=3306
2022-03-21 10:40:59,389|9895|local-1647884450656|7221|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 10:41:00,134|10640|local-1647884450656|745|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 10:41:01,362|11868|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:41:01,604|12110|local-1647884450656|1470|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 10:41:02,394|12900|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:41:06,270|16776|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:41:06,621|17127|local-1647884450656|5017|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 10:41:06,684|17190|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:41:06,684|17190|local-1647884450656|63|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 10:41:12,982|2614|local-1647884471585|2866|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 10:41:13,027|2659|local-1647884471585|INFO|scale=1000.0
2022-03-21 10:41:13,034|2666|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:41:13,034|2666|local-1647884471585|INFO|npartitions=1
2022-03-21 10:41:13,034|2666|local-1647884471585|52|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 10:41:19,669|9301|local-1647884471585|INFO|nEdgesA=4884
2022-03-21 10:41:20,207|9839|local-1647884471585|INFO|nEdgesB=3306
2022-03-21 10:41:20,207|9839|local-1647884471585|7173|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 10:41:21,054|10686|local-1647884471585|847|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 10:41:22,276|11908|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:41:22,475|12107|local-1647884471585|1421|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 10:41:23,174|12806|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:41:26,633|16265|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:41:26,989|16621|local-1647884471585|4514|TIME|overlayS|1_Census/S/AS_1e-3_8
2022-03-21 10:41:27,061|16693|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:41:27,062|16694|local-1647884471585|73|TIME|end|1_Census/S/AS_1e-3_8
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 10:41:33,495|2794|local-1647884491970|3055|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 10:41:33,543|2842|local-1647884491970|INFO|scale=1000.0
2022-03-21 10:41:33,549|2848|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:41:33,549|2848|local-1647884491970|INFO|npartitions=1
2022-03-21 10:41:33,549|2848|local-1647884491970|54|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 10:41:40,219|9518|local-1647884491970|INFO|nEdgesA=4884
2022-03-21 10:41:40,689|9988|local-1647884491970|INFO|nEdgesB=3306
2022-03-21 10:41:40,689|9988|local-1647884491970|7140|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 10:41:41,469|10768|local-1647884491970|780|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 10:41:42,602|11901|Saved /tmp/edgesFAC.wkt in 0.01s [21 records].
2022-03-21 10:41:42,828|12127|local-1647884491970|1359|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 10:41:43,546|12845|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:41:46,983|16282|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:41:47,313|16612|local-1647884491970|4485|TIME|overlayS|1_Census/S/AS_1e-3_9
2022-03-21 10:41:47,384|16683|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 10:41:47,384|16683|local-1647884491970|71|TIME|end|1_Census/S/AS_1e-3_9
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 10:41:53,591|2635|local-1647884512104|2882|COMMAND|org.apache.spark.deploy.SparkSubmit --master local[10] --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 10:41:53,639|2683|local-1647884512104|INFO|scale=1000.0
2022-03-21 10:41:53,645|2689|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:41:53,645|2689|local-1647884512104|INFO|npartitions=1
2022-03-21 10:41:53,645|2689|local-1647884512104|54|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 10:42:02,152|11196|local-1647884512104|INFO|nEdgesA=4884
2022-03-21 10:42:02,655|11699|local-1647884512104|INFO|nEdgesB=3306
2022-03-21 10:42:02,655|11699|local-1647884512104|9010|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 10:42:03,361|12405|local-1647884512104|706|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 10:42:04,575|13619|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:42:04,766|13810|local-1647884512104|1405|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 10:42:05,443|14487|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:42:09,048|18092|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:42:09,345|18389|local-1647884512104|4579|TIME|overlayS|1_Census/S/AS_1e-3_10
2022-03-21 10:42:09,420|18464|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:42:09,420|18464|local-1647884512104|75|TIME|end|1_Census/S/AS_1e-3_10
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 10:43:45,921|13464|application_1639015019875_1461|13707|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 10:43:45,961|13504|application_1639015019875_1461|INFO|scale=1000.0
2022-03-21 10:43:45,966|13509|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:43:45,966|13509|application_1639015019875_1461|INFO|npartitions=1
2022-03-21 10:43:45,966|13509|application_1639015019875_1461|45|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 10:43:54,802|22345|application_1639015019875_1461|INFO|nEdgesA=4884
2022-03-21 10:43:59,766|27309|application_1639015019875_1461|INFO|nEdgesB=3306
2022-03-21 10:43:59,766|27309|application_1639015019875_1461|13800|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 10:44:00,609|28152|application_1639015019875_1461|843|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 10:44:01,541|29084|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:44:02,284|29827|application_1639015019875_1461|1675|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 10:44:02,927|30470|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:44:23,339|50882|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:44:23,682|51225|application_1639015019875_1461|21398|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 10:44:23,760|51303|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:44:23,760|51303|application_1639015019875_1461|78|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 10:44:42,077|13749|application_1639015019875_1462|13995|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 10:44:42,125|13797|application_1639015019875_1462|INFO|scale=1000.0
2022-03-21 10:44:42,131|13803|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:44:42,131|13803|application_1639015019875_1462|INFO|npartitions=1
2022-03-21 10:44:42,131|13803|application_1639015019875_1462|54|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 10:44:50,851|22523|application_1639015019875_1462|INFO|nEdgesA=4884
2022-03-21 10:44:55,813|27485|application_1639015019875_1462|INFO|nEdgesB=3306
2022-03-21 10:44:55,813|27485|application_1639015019875_1462|13682|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 10:44:56,657|28329|application_1639015019875_1462|844|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 10:44:57,611|29283|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:44:58,340|30012|application_1639015019875_1462|1683|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 10:44:59,004|30676|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 10:45:53,801|14975|application_1639015019875_1463|15215|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 10:45:53,842|15016|application_1639015019875_1463|INFO|scale=1000.0
2022-03-21 10:45:53,848|15022|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:45:53,848|15022|application_1639015019875_1463|INFO|npartitions=1
2022-03-21 10:45:53,848|15022|application_1639015019875_1463|48|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 10:46:02,488|23662|application_1639015019875_1463|INFO|nEdgesA=4884
2022-03-21 10:46:03,123|24297|application_1639015019875_1463|INFO|nEdgesB=3306
2022-03-21 10:46:03,123|24297|application_1639015019875_1463|9275|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 10:46:03,974|25148|application_1639015019875_1463|851|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 10:46:04,915|26089|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:46:05,139|26313|application_1639015019875_1463|1165|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 10:46:05,736|26910|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:46:08,492|29666|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:46:08,822|29996|application_1639015019875_1463|3683|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 10:46:08,896|30070|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:46:08,896|30070|application_1639015019875_1463|74|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 10:46:26,308|13769|application_1639015019875_1464|14015|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 10:46:26,355|13816|application_1639015019875_1464|INFO|scale=1000.0
2022-03-21 10:46:26,360|13821|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:46:26,361|13822|application_1639015019875_1464|INFO|npartitions=1
2022-03-21 10:46:26,361|13822|application_1639015019875_1464|53|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 10:46:35,155|22616|application_1639015019875_1464|INFO|nEdgesA=4884
2022-03-21 10:46:40,157|27618|application_1639015019875_1464|INFO|nEdgesB=3306
2022-03-21 10:46:40,157|27618|application_1639015019875_1464|13796|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 10:46:40,981|28442|application_1639015019875_1464|824|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 10:46:41,865|29326|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:46:42,598|30059|application_1639015019875_1464|1617|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 10:46:43,239|30700|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 10:47:48,854|13858|application_1639015019875_1465|14107|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 10:47:48,898|13902|application_1639015019875_1465|INFO|scale=1000.0
2022-03-21 10:47:48,903|13907|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:47:48,903|13907|application_1639015019875_1465|INFO|npartitions=1
2022-03-21 10:47:48,904|13908|application_1639015019875_1465|50|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 10:47:57,698|22702|application_1639015019875_1465|INFO|nEdgesA=4884
2022-03-21 10:48:02,765|27769|application_1639015019875_1465|INFO|nEdgesB=3306
2022-03-21 10:48:02,765|27769|application_1639015019875_1465|13861|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 10:48:03,608|28612|application_1639015019875_1465|843|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 10:48:04,526|29530|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:48:05,251|30255|application_1639015019875_1465|1643|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 10:48:05,917|30921|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:48:26,304|51308|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:48:26,651|51655|application_1639015019875_1465|21400|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 10:48:26,733|51737|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:48:26,733|51737|application_1639015019875_1465|82|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 10:48:44,934|13623|application_1639015019875_1466|13870|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 10:48:44,975|13664|application_1639015019875_1466|INFO|scale=1000.0
2022-03-21 10:48:44,980|13669|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:48:44,980|13669|application_1639015019875_1466|INFO|npartitions=1
2022-03-21 10:48:44,981|13670|application_1639015019875_1466|48|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 10:48:53,590|22279|application_1639015019875_1466|INFO|nEdgesA=4884
2022-03-21 10:48:58,569|27258|application_1639015019875_1466|INFO|nEdgesB=3306
2022-03-21 10:48:58,569|27258|application_1639015019875_1466|13588|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 10:48:59,421|28110|application_1639015019875_1466|852|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 10:49:00,305|28994|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:49:01,082|29771|application_1639015019875_1466|1661|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 10:49:01,743|30432|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 10:49:53,234|13196|application_1639015019875_1467|13467|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 10:49:53,274|13236|application_1639015019875_1467|INFO|scale=1000.0
2022-03-21 10:49:53,279|13241|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:49:53,279|13241|application_1639015019875_1467|INFO|npartitions=1
2022-03-21 10:49:53,279|13241|application_1639015019875_1467|45|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 10:50:01,824|21786|application_1639015019875_1467|INFO|nEdgesA=4884
2022-03-21 10:50:06,701|26663|application_1639015019875_1467|INFO|nEdgesB=3306
2022-03-21 10:50:06,701|26663|application_1639015019875_1467|13422|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 10:50:07,573|27535|application_1639015019875_1467|872|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 10:50:08,560|28522|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:50:09,311|29273|application_1639015019875_1467|1738|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 10:50:09,973|29935|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 10:51:15,588|13597|application_1639015019875_1468|13852|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 10:51:15,630|13639|application_1639015019875_1468|INFO|scale=1000.0
2022-03-21 10:51:15,636|13645|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:51:15,636|13645|application_1639015019875_1468|INFO|npartitions=1
2022-03-21 10:51:15,636|13645|application_1639015019875_1468|48|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 10:51:24,290|22299|application_1639015019875_1468|INFO|nEdgesA=4884
2022-03-21 10:51:29,380|27389|application_1639015019875_1468|INFO|nEdgesB=3306
2022-03-21 10:51:29,381|27390|application_1639015019875_1468|13745|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 10:51:30,248|28257|application_1639015019875_1468|867|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 10:51:31,187|29196|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:51:31,905|29914|application_1639015019875_1468|1657|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 10:51:32,567|30576|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 10:52:53,052|13674|application_1639015019875_1469|13925|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 10:52:53,096|13718|application_1639015019875_1469|INFO|scale=1000.0
2022-03-21 10:52:53,102|13724|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:52:53,102|13724|application_1639015019875_1469|INFO|npartitions=1
2022-03-21 10:52:53,102|13724|application_1639015019875_1469|50|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 10:53:01,858|22480|application_1639015019875_1469|INFO|nEdgesA=4884
2022-03-21 10:53:06,837|27459|application_1639015019875_1469|INFO|nEdgesB=3306
2022-03-21 10:53:06,837|27459|application_1639015019875_1469|13735|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 10:53:07,683|28305|application_1639015019875_1469|846|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 10:53:08,628|29250|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:53:09,323|29945|application_1639015019875_1469|1640|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 10:53:09,969|30591|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 10:54:16,621|13441|application_1639015019875_1470|13689|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 10:54:16,669|13489|application_1639015019875_1470|INFO|scale=1000.0
2022-03-21 10:54:16,675|13495|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:54:16,675|13495|application_1639015019875_1470|INFO|npartitions=1
2022-03-21 10:54:16,675|13495|application_1639015019875_1470|54|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 10:54:25,582|22402|application_1639015019875_1470|INFO|nEdgesA=4884
2022-03-21 10:54:30,686|27506|application_1639015019875_1470|INFO|nEdgesB=3306
2022-03-21 10:54:30,687|27507|application_1639015019875_1470|14012|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 10:54:31,553|28373|application_1639015019875_1470|866|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 10:54:32,479|29299|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:54:33,186|30006|application_1639015019875_1470|1633|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 10:54:33,894|30714|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 10:58:00,204|13091|application_1639015019875_1471|13332|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 10:58:00,243|13130|application_1639015019875_1471|INFO|scale=1000.0
2022-03-21 10:58:00,249|13136|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:58:00,249|13136|application_1639015019875_1471|INFO|npartitions=1
2022-03-21 10:58:00,249|13136|application_1639015019875_1471|45|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 10:58:09,094|21981|application_1639015019875_1471|INFO|nEdgesA=4884
2022-03-21 10:58:09,653|22540|application_1639015019875_1471|INFO|nEdgesB=3306
2022-03-21 10:58:09,653|22540|application_1639015019875_1471|9404|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 10:58:10,486|23373|application_1639015019875_1471|833|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 10:58:11,404|24291|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:58:11,611|24498|application_1639015019875_1471|1125|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 10:58:12,183|25070|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:58:14,840|27727|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:58:15,189|28076|application_1639015019875_1471|3578|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 10:58:15,269|28156|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:58:15,269|28156|application_1639015019875_1471|80|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 10:58:32,236|13411|application_1639015019875_1472|13655|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 10:58:32,279|13454|application_1639015019875_1472|INFO|scale=1000.0
2022-03-21 10:58:32,284|13459|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:58:32,284|13459|application_1639015019875_1472|INFO|npartitions=1
2022-03-21 10:58:32,284|13459|application_1639015019875_1472|48|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 10:58:41,255|22430|application_1639015019875_1472|INFO|nEdgesA=4884
2022-03-21 10:58:41,864|23039|application_1639015019875_1472|INFO|nEdgesB=3306
2022-03-21 10:58:41,864|23039|application_1639015019875_1472|9580|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 10:58:42,694|23869|application_1639015019875_1472|830|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 10:58:43,550|24725|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:58:43,754|24929|application_1639015019875_1472|1060|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 10:58:44,312|25487|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:58:47,028|28203|Saved /tmp/edgesS.wkt in 0.02s [73 records].
2022-03-21 10:58:47,743|28918|application_1639015019875_1472|3989|TIME|overlayS|1_Census/S/AS_1e-3_2
2022-03-21 10:58:47,820|28995|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:58:47,820|28995|application_1639015019875_1472|77|TIME|end|1_Census/S/AS_1e-3_2
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 10:59:04,114|12805|application_1639015019875_1473|13044|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 10:59:04,153|12844|application_1639015019875_1473|INFO|scale=1000.0
2022-03-21 10:59:04,159|12850|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:59:04,159|12850|application_1639015019875_1473|INFO|npartitions=1
2022-03-21 10:59:04,159|12850|application_1639015019875_1473|45|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 10:59:12,898|21589|application_1639015019875_1473|INFO|nEdgesA=4884
2022-03-21 10:59:13,463|22154|application_1639015019875_1473|INFO|nEdgesB=3306
2022-03-21 10:59:13,464|22155|application_1639015019875_1473|9305|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 10:59:14,300|22991|application_1639015019875_1473|836|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 10:59:15,265|23956|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:59:15,484|24175|application_1639015019875_1473|1184|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 10:59:16,110|24801|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:59:18,658|27349|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:59:19,131|27822|application_1639015019875_1473|3647|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 10:59:19,219|27910|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:59:19,219|27910|application_1639015019875_1473|88|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 10:59:35,681|12728|application_1639015019875_1474|12977|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 10:59:35,729|12776|application_1639015019875_1474|INFO|scale=1000.0
2022-03-21 10:59:35,735|12782|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 10:59:35,735|12782|application_1639015019875_1474|INFO|npartitions=1
2022-03-21 10:59:35,735|12782|application_1639015019875_1474|54|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 10:59:44,393|21440|application_1639015019875_1474|INFO|nEdgesA=4884
2022-03-21 10:59:44,942|21989|application_1639015019875_1474|INFO|nEdgesB=3306
2022-03-21 10:59:44,942|21989|application_1639015019875_1474|9207|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 10:59:45,771|22818|application_1639015019875_1474|829|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 10:59:46,727|23774|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 10:59:46,926|23973|application_1639015019875_1474|1155|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 10:59:47,510|24557|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 10:59:50,438|27485|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 10:59:50,786|27833|application_1639015019875_1474|3860|TIME|overlayS|1_Census/S/AS_1e-3_4
2022-03-21 10:59:50,862|27909|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 10:59:50,863|27910|application_1639015019875_1474|77|TIME|end|1_Census/S/AS_1e-3_4
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 11:00:07,563|13161|application_1639015019875_1475|13421|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 11:00:07,609|13207|application_1639015019875_1475|INFO|scale=1000.0
2022-03-21 11:00:07,615|13213|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:00:07,615|13213|application_1639015019875_1475|INFO|npartitions=1
2022-03-21 11:00:07,615|13213|application_1639015019875_1475|52|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 11:00:16,429|22027|application_1639015019875_1475|INFO|nEdgesA=4884
2022-03-21 11:00:17,017|22615|application_1639015019875_1475|INFO|nEdgesB=3306
2022-03-21 11:00:17,018|22616|application_1639015019875_1475|9403|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 11:00:17,875|23473|application_1639015019875_1475|857|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 11:00:18,821|24419|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:00:19,066|24664|application_1639015019875_1475|1191|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 11:00:19,684|25282|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:00:22,715|28313|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:00:23,082|28680|application_1639015019875_1475|4016|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 11:00:23,171|28769|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:00:23,171|28769|application_1639015019875_1475|89|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 11:00:39,802|13012|application_1639015019875_1476|13281|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 11:00:39,850|13060|application_1639015019875_1476|INFO|scale=1000.0
2022-03-21 11:00:39,855|13065|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:00:39,856|13066|application_1639015019875_1476|INFO|npartitions=1
2022-03-21 11:00:39,856|13066|application_1639015019875_1476|54|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 11:00:48,796|22006|application_1639015019875_1476|INFO|nEdgesA=4884
2022-03-21 11:00:49,358|22568|application_1639015019875_1476|INFO|nEdgesB=3306
2022-03-21 11:00:49,358|22568|application_1639015019875_1476|9502|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 11:00:50,207|23417|application_1639015019875_1476|849|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 11:00:51,162|24372|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:00:51,359|24569|application_1639015019875_1476|1152|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 11:00:52,072|25282|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:00:54,596|27806|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:00:54,992|28202|application_1639015019875_1476|3633|TIME|overlayS|1_Census/S/AS_1e-3_6
2022-03-21 11:00:55,089|28299|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:00:55,089|28299|application_1639015019875_1476|97|TIME|end|1_Census/S/AS_1e-3_6
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 11:01:11,710|13063|application_1639015019875_1477|13313|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 11:01:11,756|13109|application_1639015019875_1477|INFO|scale=1000.0
2022-03-21 11:01:11,762|13115|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:01:11,762|13115|application_1639015019875_1477|INFO|npartitions=1
2022-03-21 11:01:11,762|13115|application_1639015019875_1477|52|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 11:01:20,612|21965|application_1639015019875_1477|INFO|nEdgesA=4884
2022-03-21 11:01:21,222|22575|application_1639015019875_1477|INFO|nEdgesB=3306
2022-03-21 11:01:21,223|22576|application_1639015019875_1477|9461|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 11:01:22,138|23491|application_1639015019875_1477|915|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 11:01:23,149|24502|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:01:23,385|24738|application_1639015019875_1477|1247|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 11:01:23,996|25349|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:01:26,872|28225|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:01:27,279|28632|application_1639015019875_1477|3894|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 11:01:27,375|28728|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:01:27,375|28728|application_1639015019875_1477|96|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 11:01:44,134|13211|application_1639015019875_1478|13460|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 11:01:44,177|13254|application_1639015019875_1478|INFO|scale=1000.0
2022-03-21 11:01:44,183|13260|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:01:44,183|13260|application_1639015019875_1478|INFO|npartitions=1
2022-03-21 11:01:44,183|13260|application_1639015019875_1478|50|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 11:01:53,050|22127|application_1639015019875_1478|INFO|nEdgesA=4884
2022-03-21 11:01:53,656|22733|application_1639015019875_1478|INFO|nEdgesB=3306
2022-03-21 11:01:53,656|22733|application_1639015019875_1478|9473|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 11:01:54,510|23587|application_1639015019875_1478|854|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 11:01:55,474|24551|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:01:55,684|24761|application_1639015019875_1478|1174|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 11:01:56,273|25350|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:01:58,893|27970|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:01:59,343|28420|application_1639015019875_1478|3659|TIME|overlayS|1_Census/S/AS_1e-3_8
2022-03-21 11:01:59,448|28525|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:01:59,448|28525|application_1639015019875_1478|105|TIME|end|1_Census/S/AS_1e-3_8
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 11:02:15,780|12728|application_1639015019875_1479|12975|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 11:02:15,822|12770|application_1639015019875_1479|INFO|scale=1000.0
2022-03-21 11:02:15,827|12775|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:02:15,827|12775|application_1639015019875_1479|INFO|npartitions=1
2022-03-21 11:02:15,828|12776|application_1639015019875_1479|48|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 11:02:24,628|21576|application_1639015019875_1479|INFO|nEdgesA=4884
2022-03-21 11:02:25,197|22145|application_1639015019875_1479|INFO|nEdgesB=3306
2022-03-21 11:02:25,197|22145|application_1639015019875_1479|9369|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 11:02:26,037|22985|application_1639015019875_1479|840|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 11:02:27,001|23949|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:02:27,231|24179|application_1639015019875_1479|1194|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 11:02:27,891|24839|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:02:30,591|27539|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:02:30,944|27892|application_1639015019875_1479|3713|TIME|overlayS|1_Census/S/AS_1e-3_9
2022-03-21 11:02:31,027|27975|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:02:31,027|27975|application_1639015019875_1479|83|TIME|end|1_Census/S/AS_1e-3_9
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 11:02:47,523|12823|application_1639015019875_1480|13071|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 11:02:47,567|12867|application_1639015019875_1480|INFO|scale=1000.0
2022-03-21 11:02:47,572|12872|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:02:47,572|12872|application_1639015019875_1480|INFO|npartitions=1
2022-03-21 11:02:47,572|12872|application_1639015019875_1480|49|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 11:02:56,405|21705|application_1639015019875_1480|INFO|nEdgesA=4884
2022-03-21 11:02:56,988|22288|application_1639015019875_1480|INFO|nEdgesB=3306
2022-03-21 11:02:56,988|22288|application_1639015019875_1480|9416|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 11:02:57,875|23175|application_1639015019875_1480|887|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 11:02:58,814|24114|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:02:59,026|24326|application_1639015019875_1480|1151|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 11:02:59,613|24913|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:03:02,521|27821|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:03:02,873|28173|application_1639015019875_1480|3847|TIME|overlayS|1_Census/S/AS_1e-3_10
2022-03-21 11:03:02,952|28252|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:03:02,952|28252|application_1639015019875_1480|79|TIME|end|1_Census/S/AS_1e-3_10
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 11:05:49,025|13341|application_1639015019875_1481|13592|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 11:05:49,069|13385|application_1639015019875_1481|INFO|scale=1000.0
2022-03-21 11:05:49,075|13391|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:05:49,075|13391|application_1639015019875_1481|INFO|npartitions=1
2022-03-21 11:05:49,075|13391|application_1639015019875_1481|50|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 11:05:57,831|22147|application_1639015019875_1481|INFO|nEdgesA=4884
2022-03-21 11:05:58,489|22805|application_1639015019875_1481|INFO|nEdgesB=3306
2022-03-21 11:05:58,489|22805|application_1639015019875_1481|9414|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 11:05:59,349|23665|application_1639015019875_1481|860|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 11:06:00,241|24557|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:06:00,437|24753|application_1639015019875_1481|1088|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 11:06:00,995|25311|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:06:03,809|28125|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:06:04,175|28491|application_1639015019875_1481|3738|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 11:06:04,263|28579|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:06:04,263|28579|application_1639015019875_1481|88|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 11:06:21,622|13711|application_1639015019875_1482|13956|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 11:06:21,669|13758|application_1639015019875_1482|INFO|scale=1000.0
2022-03-21 11:06:21,674|13763|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:06:21,675|13764|application_1639015019875_1482|INFO|npartitions=1
2022-03-21 11:06:21,675|13764|application_1639015019875_1482|53|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 11:06:30,489|22578|application_1639015019875_1482|INFO|nEdgesA=4884
2022-03-21 11:06:31,093|23182|application_1639015019875_1482|INFO|nEdgesB=3306
2022-03-21 11:06:31,093|23182|application_1639015019875_1482|9418|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 11:06:31,928|24017|application_1639015019875_1482|835|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 11:06:32,887|24976|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:06:33,084|25173|application_1639015019875_1482|1156|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 11:06:33,657|25746|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:06:36,373|28462|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:06:36,714|28803|application_1639015019875_1482|3630|TIME|overlayS|1_Census/S/AS_1e-3_2
2022-03-21 11:06:36,815|28904|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:06:36,815|28904|application_1639015019875_1482|101|TIME|end|1_Census/S/AS_1e-3_2
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 11:06:53,122|12852|application_1639015019875_1483|13107|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 11:06:53,172|12902|application_1639015019875_1483|INFO|scale=1000.0
2022-03-21 11:06:53,178|12908|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:06:53,178|12908|application_1639015019875_1483|INFO|npartitions=1
2022-03-21 11:06:53,178|12908|application_1639015019875_1483|56|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 11:07:02,061|21791|application_1639015019875_1483|INFO|nEdgesA=4884
2022-03-21 11:07:02,654|22384|application_1639015019875_1483|INFO|nEdgesB=3306
2022-03-21 11:07:02,654|22384|application_1639015019875_1483|9476|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 11:07:03,536|23266|application_1639015019875_1483|882|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 11:07:04,461|24191|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:07:04,654|24384|application_1639015019875_1483|1118|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 11:07:05,238|24968|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:07:08,084|27814|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:07:08,460|28190|application_1639015019875_1483|3806|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 11:07:08,549|28279|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:07:08,549|28279|application_1639015019875_1483|89|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 11:07:24,823|12805|application_1639015019875_1484|13045|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 11:07:24,865|12847|application_1639015019875_1484|INFO|scale=1000.0
2022-03-21 11:07:24,870|12852|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:07:24,870|12852|application_1639015019875_1484|INFO|npartitions=1
2022-03-21 11:07:24,870|12852|application_1639015019875_1484|47|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 11:07:33,553|21535|application_1639015019875_1484|INFO|nEdgesA=4884
2022-03-21 11:07:34,174|22156|application_1639015019875_1484|INFO|nEdgesB=3306
2022-03-21 11:07:34,174|22156|application_1639015019875_1484|9304|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 11:07:35,007|22989|application_1639015019875_1484|833|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 11:07:35,978|23960|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:07:36,206|24188|application_1639015019875_1484|1199|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 11:07:36,864|24846|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:07:39,614|27596|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:07:39,964|27946|application_1639015019875_1484|3758|TIME|overlayS|1_Census/S/AS_1e-3_4
2022-03-21 11:07:40,041|28023|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:07:40,041|28023|application_1639015019875_1484|77|TIME|end|1_Census/S/AS_1e-3_4
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 11:07:57,092|13454|application_1639015019875_1485|13695|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 11:07:57,143|13505|application_1639015019875_1485|INFO|scale=1000.0
2022-03-21 11:07:57,149|13511|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:07:57,149|13511|application_1639015019875_1485|INFO|npartitions=1
2022-03-21 11:07:57,149|13511|application_1639015019875_1485|57|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 11:08:05,976|22338|application_1639015019875_1485|INFO|nEdgesA=4884
2022-03-21 11:08:06,551|22913|application_1639015019875_1485|INFO|nEdgesB=3306
2022-03-21 11:08:06,552|22914|application_1639015019875_1485|9403|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 11:08:07,379|23741|application_1639015019875_1485|827|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 11:08:08,313|24675|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:08:08,524|24886|application_1639015019875_1485|1145|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 11:08:09,230|25592|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:08:11,733|28095|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:08:12,071|28433|application_1639015019875_1485|3547|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 11:08:12,155|28517|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:08:12,156|28518|application_1639015019875_1485|85|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 11:08:28,651|13001|application_1639015019875_1486|13252|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 11:08:28,693|13043|application_1639015019875_1486|INFO|scale=1000.0
2022-03-21 11:08:28,699|13049|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:08:28,699|13049|application_1639015019875_1486|INFO|npartitions=1
2022-03-21 11:08:28,699|13049|application_1639015019875_1486|48|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 11:08:37,456|21806|application_1639015019875_1486|INFO|nEdgesA=4884
2022-03-21 11:08:38,012|22362|application_1639015019875_1486|INFO|nEdgesB=3306
2022-03-21 11:08:38,012|22362|application_1639015019875_1486|9313|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 11:08:38,840|23190|application_1639015019875_1486|828|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 11:08:39,693|24043|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:08:39,906|24256|application_1639015019875_1486|1066|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 11:08:40,488|24838|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:08:43,177|27527|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:08:43,513|27863|application_1639015019875_1486|3607|TIME|overlayS|1_Census/S/AS_1e-3_6
2022-03-21 11:08:43,583|27933|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:08:43,583|27933|application_1639015019875_1486|70|TIME|end|1_Census/S/AS_1e-3_6
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 11:08:59,876|12800|application_1639015019875_1487|13041|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 11:08:59,919|12843|application_1639015019875_1487|INFO|scale=1000.0
2022-03-21 11:08:59,924|12848|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:08:59,924|12848|application_1639015019875_1487|INFO|npartitions=1
2022-03-21 11:08:59,924|12848|application_1639015019875_1487|48|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 11:09:08,773|21697|application_1639015019875_1487|INFO|nEdgesA=4884
2022-03-21 11:09:09,536|22460|application_1639015019875_1487|INFO|nEdgesB=3306
2022-03-21 11:09:09,536|22460|application_1639015019875_1487|9612|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 11:09:10,380|23304|application_1639015019875_1487|844|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 11:09:11,253|24177|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:09:11,444|24368|application_1639015019875_1487|1064|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 11:09:12,013|24937|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:09:14,541|27465|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:09:14,885|27809|application_1639015019875_1487|3441|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 11:09:14,965|27889|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:09:14,965|27889|application_1639015019875_1487|80|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 11:09:31,420|12999|application_1639015019875_1488|13243|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 11:09:31,458|13037|application_1639015019875_1488|INFO|scale=1000.0
2022-03-21 11:09:31,463|13042|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:09:31,463|13042|application_1639015019875_1488|INFO|npartitions=1
2022-03-21 11:09:31,463|13042|application_1639015019875_1488|44|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 11:09:40,155|21734|application_1639015019875_1488|INFO|nEdgesA=4884
2022-03-21 11:09:40,695|22274|application_1639015019875_1488|INFO|nEdgesB=3306
2022-03-21 11:09:40,696|22275|application_1639015019875_1488|9233|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 11:09:41,530|23109|application_1639015019875_1488|834|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 11:09:42,541|24120|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:09:42,783|24362|application_1639015019875_1488|1253|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 11:09:43,426|25005|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:09:46,211|27790|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:09:46,572|28151|application_1639015019875_1488|3789|TIME|overlayS|1_Census/S/AS_1e-3_8
2022-03-21 11:09:46,656|28235|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:09:46,656|28235|application_1639015019875_1488|84|TIME|end|1_Census/S/AS_1e-3_8
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 11:10:03,115|12906|application_1639015019875_1489|13154|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 11:10:03,158|12949|application_1639015019875_1489|INFO|scale=1000.0
2022-03-21 11:10:03,163|12954|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:10:03,163|12954|application_1639015019875_1489|INFO|npartitions=1
2022-03-21 11:10:03,164|12955|application_1639015019875_1489|49|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 11:10:11,938|21729|application_1639015019875_1489|INFO|nEdgesA=4884
2022-03-21 11:10:12,550|22341|application_1639015019875_1489|INFO|nEdgesB=3306
2022-03-21 11:10:12,550|22341|application_1639015019875_1489|9386|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 11:10:13,408|23199|application_1639015019875_1489|858|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 11:10:14,330|24121|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:10:14,523|24314|application_1639015019875_1489|1115|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 11:10:15,093|24884|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:10:18,025|27816|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:10:18,359|28150|application_1639015019875_1489|3836|TIME|overlayS|1_Census/S/AS_1e-3_9
2022-03-21 11:10:18,430|28221|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:10:18,430|28221|application_1639015019875_1489|71|TIME|end|1_Census/S/AS_1e-3_9
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 11:10:35,022|12747|application_1639015019875_1490|12993|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 1 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 11:10:35,064|12789|application_1639015019875_1490|INFO|scale=1000.0
2022-03-21 11:10:35,070|12795|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:10:35,070|12795|application_1639015019875_1490|INFO|npartitions=1
2022-03-21 11:10:35,070|12795|application_1639015019875_1490|49|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 11:10:43,827|21552|application_1639015019875_1490|INFO|nEdgesA=4884
2022-03-21 11:10:44,491|22216|application_1639015019875_1490|INFO|nEdgesB=3306
2022-03-21 11:10:44,492|22217|application_1639015019875_1490|9422|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 11:10:45,306|23031|application_1639015019875_1490|814|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 11:10:46,235|23960|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:10:46,422|24147|application_1639015019875_1490|1116|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 11:10:46,967|24692|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:10:49,705|27430|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:10:50,078|27803|application_1639015019875_1490|3656|TIME|overlayS|1_Census/S/AS_1e-3_10
2022-03-21 11:10:50,159|27884|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:10:50,159|27884|application_1639015019875_1490|81|TIME|end|1_Census/S/AS_1e-3_10
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 11:12:17,607|13701|application_1639015019875_1491|13949|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 11:12:17,648|13742|application_1639015019875_1491|INFO|scale=1000.0
2022-03-21 11:12:17,653|13747|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:12:17,653|13747|application_1639015019875_1491|INFO|npartitions=1
2022-03-21 11:12:17,653|13747|application_1639015019875_1491|46|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 11:12:26,587|22681|application_1639015019875_1491|INFO|nEdgesA=4884
2022-03-21 11:12:31,569|27663|application_1639015019875_1491|INFO|nEdgesB=3306
2022-03-21 11:12:31,569|27663|application_1639015019875_1491|13916|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 11:12:32,423|28517|application_1639015019875_1491|854|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 11:12:33,382|29476|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:12:34,111|30205|application_1639015019875_1491|1688|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 11:12:34,770|30864|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:12:46,779|42873|Saved /tmp/edgesS.wkt in 0.01s [73 records].
2022-03-21 11:12:47,160|43254|application_1639015019875_1491|13049|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 11:12:47,247|43341|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 11:12:47,248|43342|application_1639015019875_1491|88|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 11:13:05,653|13957|application_1639015019875_1492|14212|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 11:13:05,702|14006|application_1639015019875_1492|INFO|scale=1000.0
2022-03-21 11:13:05,707|14011|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:13:05,707|14011|application_1639015019875_1492|INFO|npartitions=1
2022-03-21 11:13:05,708|14012|application_1639015019875_1492|54|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 11:13:14,476|22780|application_1639015019875_1492|INFO|nEdgesA=4884
2022-03-21 11:13:19,754|28058|application_1639015019875_1492|INFO|nEdgesB=3306
2022-03-21 11:13:19,755|28059|application_1639015019875_1492|14048|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 11:13:20,655|28959|application_1639015019875_1492|900|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 11:13:21,553|29857|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:13:22,265|30569|application_1639015019875_1492|1610|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 11:13:22,937|31241|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 11:14:27,160|13329|application_1639015019875_1493|13568|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 11:14:27,205|13374|application_1639015019875_1493|INFO|scale=1000.0
2022-03-21 11:14:27,211|13380|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:14:27,211|13380|application_1639015019875_1493|INFO|npartitions=1
2022-03-21 11:14:27,211|13380|application_1639015019875_1493|51|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 11:14:35,963|22132|application_1639015019875_1493|INFO|nEdgesA=4884
2022-03-21 11:14:40,848|27017|application_1639015019875_1493|INFO|nEdgesB=3306
2022-03-21 11:14:40,848|27017|application_1639015019875_1493|13637|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 11:14:41,684|27853|application_1639015019875_1493|836|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 11:14:42,608|28777|Saved /tmp/edgesFAC.wkt in 0.01s [21 records].
2022-03-21 11:14:43,341|29510|application_1639015019875_1493|1657|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 11:14:44,007|30176|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:15:10,027|56196|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:15:10,454|56623|application_1639015019875_1493|27113|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 11:15:10,541|56710|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:15:10,541|56710|application_1639015019875_1493|87|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 11:15:28,138|14063|application_1639015019875_1494|14309|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 11:15:28,181|14106|application_1639015019875_1494|INFO|scale=1000.0
2022-03-21 11:15:28,186|14111|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:15:28,186|14111|application_1639015019875_1494|INFO|npartitions=1
2022-03-21 11:15:28,186|14111|application_1639015019875_1494|48|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 11:15:36,975|22900|application_1639015019875_1494|INFO|nEdgesA=4884
2022-03-21 11:15:42,045|27970|application_1639015019875_1494|INFO|nEdgesB=3306
2022-03-21 11:15:42,045|27970|application_1639015019875_1494|13859|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 11:15:42,899|28824|application_1639015019875_1494|854|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 11:15:43,831|29756|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:15:44,577|30502|application_1639015019875_1494|1678|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 11:15:45,242|31167|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:15:57,988|43913|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:15:58,346|44271|application_1639015019875_1494|13769|TIME|overlayS|1_Census/S/AS_1e-3_4
2022-03-21 11:15:58,430|44355|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:15:58,430|44355|application_1639015019875_1494|84|TIME|end|1_Census/S/AS_1e-3_4
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 11:16:15,619|13554|application_1639015019875_1495|13811|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 11:16:15,666|13601|application_1639015019875_1495|INFO|scale=1000.0
2022-03-21 11:16:15,672|13607|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:16:15,672|13607|application_1639015019875_1495|INFO|npartitions=1
2022-03-21 11:16:15,672|13607|application_1639015019875_1495|53|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 11:16:24,245|22180|application_1639015019875_1495|INFO|nEdgesA=4884
2022-03-21 11:16:29,132|27067|application_1639015019875_1495|INFO|nEdgesB=3306
2022-03-21 11:16:29,133|27068|application_1639015019875_1495|13461|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 11:16:29,999|27934|application_1639015019875_1495|866|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 11:16:30,966|28901|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:16:31,684|29619|application_1639015019875_1495|1685|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 11:16:32,347|30282|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:17:02,376|60311|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:17:02,726|60661|application_1639015019875_1495|31042|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 11:17:02,816|60751|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 11:17:02,817|60752|application_1639015019875_1495|91|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 11:17:20,151|13746|application_1639015019875_1496|13994|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 11:17:20,209|13804|application_1639015019875_1496|INFO|scale=1000.0
2022-03-21 11:17:20,216|13811|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:17:20,216|13811|application_1639015019875_1496|INFO|npartitions=1
2022-03-21 11:17:20,216|13811|application_1639015019875_1496|65|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 11:17:28,885|22480|application_1639015019875_1496|INFO|nEdgesA=4884
2022-03-21 11:17:33,767|27362|application_1639015019875_1496|INFO|nEdgesB=3306
2022-03-21 11:17:33,767|27362|application_1639015019875_1496|13551|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 11:17:34,615|28210|application_1639015019875_1496|848|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 11:17:35,577|29172|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:17:36,343|29938|application_1639015019875_1496|1728|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 11:17:36,990|30585|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 11:18:43,556|13600|application_1639015019875_1497|13845|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 11:18:43,599|13643|application_1639015019875_1497|INFO|scale=1000.0
2022-03-21 11:18:43,604|13648|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:18:43,605|13649|application_1639015019875_1497|INFO|npartitions=1
2022-03-21 11:18:43,605|13649|application_1639015019875_1497|49|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 11:18:52,560|22604|application_1639015019875_1497|INFO|nEdgesA=4884
2022-03-21 11:18:57,440|27484|application_1639015019875_1497|INFO|nEdgesB=3306
2022-03-21 11:18:57,440|27484|application_1639015019875_1497|13835|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 11:18:58,298|28342|application_1639015019875_1497|858|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 11:18:59,289|29333|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:19:00,073|30117|application_1639015019875_1497|1775|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 11:19:00,767|30811|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:19:50,846|80890|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:19:51,193|81237|application_1639015019875_1497|51120|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 11:19:51,279|81323|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:19:51,279|81323|application_1639015019875_1497|86|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 11:20:08,377|13545|application_1639015019875_1498|13792|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 11:20:08,423|13591|application_1639015019875_1498|INFO|scale=1000.0
2022-03-21 11:20:08,430|13598|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:20:08,430|13598|application_1639015019875_1498|INFO|npartitions=1
2022-03-21 11:20:08,430|13598|application_1639015019875_1498|53|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 11:20:17,220|22388|application_1639015019875_1498|INFO|nEdgesA=4884
2022-03-21 11:20:22,206|27374|application_1639015019875_1498|INFO|nEdgesB=3306
2022-03-21 11:20:22,207|27375|application_1639015019875_1498|13777|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 11:20:23,063|28231|application_1639015019875_1498|856|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 11:20:24,033|29201|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:20:24,770|29938|application_1639015019875_1498|1707|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 11:20:25,408|30576|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 11:21:31,785|14400|application_1639015019875_1499|14647|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 11:21:31,827|14442|application_1639015019875_1499|INFO|scale=1000.0
2022-03-21 11:21:31,832|14447|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:21:31,832|14447|application_1639015019875_1499|INFO|npartitions=1
2022-03-21 11:21:31,833|14448|application_1639015019875_1499|48|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 11:21:40,528|23143|application_1639015019875_1499|INFO|nEdgesA=4884
2022-03-21 11:21:45,585|28200|application_1639015019875_1499|INFO|nEdgesB=3306
2022-03-21 11:21:45,586|28201|application_1639015019875_1499|13753|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 11:21:46,442|29057|application_1639015019875_1499|856|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 11:21:47,388|30003|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:21:48,094|30709|application_1639015019875_1499|1652|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 11:21:48,737|31352|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:158) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:158)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 11:22:53,775|13320|application_1639015019875_1500|13569|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 11:22:53,822|13367|application_1639015019875_1500|INFO|scale=1000.0
2022-03-21 11:22:53,828|13373|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:22:53,828|13373|application_1639015019875_1500|INFO|npartitions=1
2022-03-21 11:22:53,829|13374|application_1639015019875_1500|53|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 11:23:02,475|22020|application_1639015019875_1500|INFO|nEdgesA=4884
2022-03-21 11:23:07,505|27050|application_1639015019875_1500|INFO|nEdgesB=3306
2022-03-21 11:23:07,506|27051|application_1639015019875_1500|13678|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 11:23:08,349|27894|application_1639015019875_1500|843|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 11:23:09,288|28833|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:23:10,013|29558|application_1639015019875_1500|1664|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 11:23:10,683|30228|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:23:30,059|49604|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:23:30,419|49964|application_1639015019875_1500|20406|TIME|overlayS|1_Census/S/AS_1e-3_10
2022-03-21 11:23:30,499|50044|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:23:30,499|50044|application_1639015019875_1500|80|TIME|end|1_Census/S/AS_1e-3_10
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 11:25:15,879|13535|application_1639015019875_1501|13796|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 11:25:15,924|13580|application_1639015019875_1501|INFO|scale=1000.0
2022-03-21 11:25:15,929|13585|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:25:15,930|13586|application_1639015019875_1501|INFO|npartitions=1
2022-03-21 11:25:15,930|13586|application_1639015019875_1501|52|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 11:25:24,749|22405|application_1639015019875_1501|INFO|nEdgesA=4884
2022-03-21 11:25:29,947|27603|application_1639015019875_1501|INFO|nEdgesB=3306
2022-03-21 11:25:29,947|27603|application_1639015019875_1501|14017|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 11:25:30,819|28475|application_1639015019875_1501|872|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 11:25:31,828|29484|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:25:32,575|30231|application_1639015019875_1501|1756|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 11:25:33,275|30931|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 11:26:39,169|13359|application_1639015019875_1502|13604|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 11:26:39,212|13402|application_1639015019875_1502|INFO|scale=1000.0
2022-03-21 11:26:39,217|13407|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:26:39,217|13407|application_1639015019875_1502|INFO|npartitions=1
2022-03-21 11:26:39,218|13408|application_1639015019875_1502|48|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 11:26:47,830|22020|application_1639015019875_1502|INFO|nEdgesA=4884
2022-03-21 11:26:48,345|22535|application_1639015019875_1502|INFO|nEdgesB=3306
2022-03-21 11:26:48,345|22535|application_1639015019875_1502|9128|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 11:26:49,148|23338|application_1639015019875_1502|803|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 11:26:50,048|24238|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:26:50,247|24437|application_1639015019875_1502|1098|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 11:26:50,834|25024|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:26:53,281|27471|application_1639015019875_1502|3035|TIME|overlayP|1_Census/S/AS_1e-3_2
2022-03-21 11:26:53,394|27584|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 11:26:55,577|29767|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:26:55,911|30101|application_1639015019875_1502|2630|TIME|overlayS|1_Census/S/AS_1e-3_2
2022-03-21 11:26:55,985|30175|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:26:55,985|30175|application_1639015019875_1502|74|TIME|end|1_Census/S/AS_1e-3_2
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 11:27:13,682|13641|application_1639015019875_1503|13886|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 11:27:13,727|13686|application_1639015019875_1503|INFO|scale=1000.0
2022-03-21 11:27:13,733|13692|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:27:13,733|13692|application_1639015019875_1503|INFO|npartitions=1
2022-03-21 11:27:13,733|13692|application_1639015019875_1503|51|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 11:27:22,674|22633|application_1639015019875_1503|INFO|nEdgesA=4884
2022-03-21 11:27:27,810|27769|application_1639015019875_1503|INFO|nEdgesB=3306
2022-03-21 11:27:27,810|27769|application_1639015019875_1503|14077|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 11:27:28,650|28609|application_1639015019875_1503|840|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 11:27:29,557|29516|Saved /tmp/edgesFAC.wkt in 0.01s [21 records].
2022-03-21 11:27:30,285|30244|application_1639015019875_1503|1635|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 11:27:30,930|30889|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 11:28:36,844|13933|application_1639015019875_1504|14197|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 11:28:36,888|13977|application_1639015019875_1504|INFO|scale=1000.0
2022-03-21 11:28:36,893|13982|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:28:36,893|13982|application_1639015019875_1504|INFO|npartitions=1
2022-03-21 11:28:36,893|13982|application_1639015019875_1504|49|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 11:28:45,589|22678|application_1639015019875_1504|INFO|nEdgesA=4884
2022-03-21 11:28:50,730|27819|application_1639015019875_1504|INFO|nEdgesB=3306
2022-03-21 11:28:50,730|27819|application_1639015019875_1504|13837|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 11:28:51,608|28697|application_1639015019875_1504|878|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 11:28:52,508|29597|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:28:53,233|30322|application_1639015019875_1504|1625|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 11:28:53,908|30997|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 11:30:13,470|13831|application_1639015019875_1505|14081|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 11:30:13,521|13882|application_1639015019875_1505|INFO|scale=1000.0
2022-03-21 11:30:13,527|13888|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:30:13,527|13888|application_1639015019875_1505|INFO|npartitions=1
2022-03-21 11:30:13,527|13888|application_1639015019875_1505|57|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 11:30:22,218|22579|application_1639015019875_1505|INFO|nEdgesA=4884
2022-03-21 11:30:27,287|27648|application_1639015019875_1505|INFO|nEdgesB=3306
2022-03-21 11:30:27,288|27649|application_1639015019875_1505|13761|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 11:30:28,144|28505|application_1639015019875_1505|856|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 11:30:29,128|29489|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:30:29,843|30204|application_1639015019875_1505|1699|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 11:30:30,493|30854|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 11:31:35,227|14070|application_1639015019875_1506|14351|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 11:31:35,274|14117|application_1639015019875_1506|INFO|scale=1000.0
2022-03-21 11:31:35,279|14122|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:31:35,280|14123|application_1639015019875_1506|INFO|npartitions=1
2022-03-21 11:31:35,280|14123|application_1639015019875_1506|53|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 11:31:44,143|22986|application_1639015019875_1506|INFO|nEdgesA=4884
2022-03-21 11:31:49,268|28111|application_1639015019875_1506|INFO|nEdgesB=3306
2022-03-21 11:31:49,268|28111|application_1639015019875_1506|13988|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 11:31:50,100|28943|application_1639015019875_1506|832|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 11:31:50,969|29812|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:31:51,720|30563|application_1639015019875_1506|1620|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 11:31:52,406|31249|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 11:32:43,649|13550|application_1639015019875_1507|13810|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 11:32:43,695|13596|application_1639015019875_1507|INFO|scale=1000.0
2022-03-21 11:32:43,701|13602|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:32:43,701|13602|application_1639015019875_1507|INFO|npartitions=1
2022-03-21 11:32:43,701|13602|application_1639015019875_1507|52|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 11:32:52,493|22394|application_1639015019875_1507|INFO|nEdgesA=4884
2022-03-21 11:32:57,520|27421|application_1639015019875_1507|INFO|nEdgesB=3306
2022-03-21 11:32:57,520|27421|application_1639015019875_1507|13819|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 11:32:58,385|28286|application_1639015019875_1507|865|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 11:32:59,334|29235|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:33:00,073|29974|application_1639015019875_1507|1688|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 11:33:00,768|30669|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
2022-03-21 11:33:35,492|65393|application_1639015019875_1507|35419|TIME|overlayP|1_Census/S/AS_1e-3_7
2022-03-21 11:33:35,622|65523|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 11:33:37,883|67784|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 11:33:38,242|68143|application_1639015019875_1507|2750|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 11:33:38,318|68219|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 11:33:38,319|68220|application_1639015019875_1507|77|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 11:33:56,354|14395|application_1639015019875_1508|14641|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 11:33:56,397|14438|application_1639015019875_1508|INFO|scale=1000.0
2022-03-21 11:33:56,402|14443|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:33:56,402|14443|application_1639015019875_1508|INFO|npartitions=1
2022-03-21 11:33:56,403|14444|application_1639015019875_1508|49|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 11:34:05,119|23160|application_1639015019875_1508|INFO|nEdgesA=4884
2022-03-21 11:34:10,116|28157|application_1639015019875_1508|INFO|nEdgesB=3306
2022-03-21 11:34:10,116|28157|application_1639015019875_1508|13713|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 11:34:10,961|29002|application_1639015019875_1508|845|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 11:34:11,922|29963|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:34:12,624|30665|application_1639015019875_1508|1663|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 11:34:13,264|31305|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 11:35:19,188|13699|application_1639015019875_1509|13944|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 11:35:19,236|13747|application_1639015019875_1509|INFO|scale=1000.0
2022-03-21 11:35:19,241|13752|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:35:19,241|13752|application_1639015019875_1509|INFO|npartitions=1
2022-03-21 11:35:19,241|13752|application_1639015019875_1509|53|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 11:35:28,123|22634|application_1639015019875_1509|INFO|nEdgesA=4884
2022-03-21 11:35:33,027|27538|application_1639015019875_1509|INFO|nEdgesB=3306
2022-03-21 11:35:33,027|27538|application_1639015019875_1509|13786|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 11:35:33,906|28417|application_1639015019875_1509|879|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 11:35:34,844|29355|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:35:35,588|30099|application_1639015019875_1509|1682|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 11:35:36,295|30806|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 11:36:43,562|13509|application_1639015019875_1510|13757|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 1 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 11:36:43,606|13553|application_1639015019875_1510|INFO|scale=1000.0
2022-03-21 11:36:43,611|13558|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 11:36:43,612|13559|application_1639015019875_1510|INFO|npartitions=1
2022-03-21 11:36:43,612|13559|application_1639015019875_1510|50|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 11:36:52,295|22242|application_1639015019875_1510|INFO|nEdgesA=4884
2022-03-21 11:36:57,499|27446|application_1639015019875_1510|INFO|nEdgesB=3306
2022-03-21 11:36:57,499|27446|application_1639015019875_1510|13887|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 11:36:58,431|28378|application_1639015019875_1510|932|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 11:36:59,354|29301|Saved /tmp/edgesFAC.wkt in 0.00s [21 records].
2022-03-21 11:37:00,159|30106|application_1639015019875_1510|1728|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 11:37:00,852|30799|Saved /tmp/edgesFBC.wkt in 0.00s [19 records].
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: ResultStage 14 (count at SDCEL2.scala:149) has failed the maximum allowable number of times: 4. Most recent failure reason: org.apache.spark.shuffle.MetadataFetchFailedException: Missing an output location for shuffle 0 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:867) 	at org.apache.spark.MapOutputTracker$$anonfun$convertMapStatuses$2.apply(MapOutputTracker.scala:863) 	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:733) 	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33) 	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186) 	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:732) 	at org.apache.spark.MapOutputTracker$.convertMapStatuses(MapOutputTracker.scala:863) 	at org.apache.spark.MapOutputTrackerWorker.getMapSizesByExecutorId(MapOutputTracker.scala:677) 	at org.apache.spark.shuffle.BlockStoreShuffleReader.read(BlockStoreShuffleReader.scala:49) 	at org.apache.spark.rdd.ShuffledRDD.compute(ShuffledRDD.scala:105) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:288) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:49) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:89) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:337) 	at org.apache.spark.rdd.RDD$$anonfun$7.apply(RDD.scala:335) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1094) 	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:1020) 	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1085) 	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:811) 	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:335) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:286) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 	at org.apache.spark.scheduler.Task.run(Task.scala:109) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1651)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1639)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1638)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1638)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1348)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1869)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1821)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1810)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)
	at org.apache.spark.rdd.RDD.count(RDD.scala:1168)
	at edu.ucr.dblab.sdcel.SDCEL2$.main(SDCEL2.scala:149)
	at edu.ucr.dblab.sdcel.SDCEL2.main(SDCEL2.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:904)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:198)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:228)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
DATASET    = Census/S/AS
TOLERANCE  = 1e-3
ITERATIONS = 10
PARTITIONS = 1
Run 1 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_1"
2022-03-21 12:51:10,927|13901|application_1639015019875_1518|14156|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_1 --debug --local
2022-03-21 12:51:10,970|13944|application_1639015019875_1518|INFO|scale=1000.0
2022-03-21 12:51:10,975|13949|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:51:10,975|13949|application_1639015019875_1518|INFO|npartitions=1
2022-03-21 12:51:10,975|13949|application_1639015019875_1518|48|TIME|start|1_Census/S/AS_1e-3_1
2022-03-21 12:51:19,735|22709|application_1639015019875_1518|INFO|nEdgesA=4884
2022-03-21 12:51:25,023|27997|application_1639015019875_1518|INFO|nEdgesB=3306
2022-03-21 12:51:25,023|27997|application_1639015019875_1518|14048|TIME|read|1_Census/S/AS_1e-3_1
2022-03-21 12:51:25,910|28884|application_1639015019875_1518|887|TIME|layer1S|1_Census/S/AS_1e-3_1
2022-03-21 12:51:26,711|29685|application_1639015019875_1518|801|TIME|layer2S|1_Census/S/AS_1e-3_1
2022-03-21 12:51:29,820|32794|application_1639015019875_1518|3109|TIME|overlayP|1_Census/S/AS_1e-3_1
2022-03-21 12:51:29,945|32919|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:51:32,095|35069|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:51:32,460|35434|application_1639015019875_1518|2640|TIME|overlayS|1_Census/S/AS_1e-3_1
2022-03-21 12:51:32,562|35536|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 12:51:32,563|35537|application_1639015019875_1518|103|TIME|end|1_Census/S/AS_1e-3_1
Run 2 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_2"
2022-03-21 12:51:49,723|13607|application_1639015019875_1519|13857|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_2 --debug --local
2022-03-21 12:51:49,766|13650|application_1639015019875_1519|INFO|scale=1000.0
2022-03-21 12:51:49,771|13655|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:51:49,771|13655|application_1639015019875_1519|INFO|npartitions=1
2022-03-21 12:51:49,771|13655|application_1639015019875_1519|48|TIME|start|1_Census/S/AS_1e-3_2
2022-03-21 12:51:58,713|22597|application_1639015019875_1519|INFO|nEdgesA=4884
2022-03-21 12:52:03,730|27614|application_1639015019875_1519|INFO|nEdgesB=3306
2022-03-21 12:52:03,730|27614|application_1639015019875_1519|13959|TIME|read|1_Census/S/AS_1e-3_2
2022-03-21 12:52:04,593|28477|application_1639015019875_1519|863|TIME|layer1S|1_Census/S/AS_1e-3_2
2022-03-21 12:52:05,262|29146|application_1639015019875_1519|669|TIME|layer2S|1_Census/S/AS_1e-3_2
2022-03-21 12:52:08,196|32080|application_1639015019875_1519|2934|TIME|overlayP|1_Census/S/AS_1e-3_2
2022-03-21 12:52:08,323|32207|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:52:10,487|34371|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:52:10,829|34713|application_1639015019875_1519|2633|TIME|overlayS|1_Census/S/AS_1e-3_2
2022-03-21 12:52:10,912|34796|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:52:10,912|34796|application_1639015019875_1519|83|TIME|end|1_Census/S/AS_1e-3_2
Run 3 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_3"
2022-03-21 12:52:28,043|13590|application_1639015019875_1520|13835|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_3 --debug --local
2022-03-21 12:52:28,085|13632|application_1639015019875_1520|INFO|scale=1000.0
2022-03-21 12:52:28,090|13637|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:52:28,090|13637|application_1639015019875_1520|INFO|npartitions=1
2022-03-21 12:52:28,090|13637|application_1639015019875_1520|47|TIME|start|1_Census/S/AS_1e-3_3
2022-03-21 12:52:36,921|22468|application_1639015019875_1520|INFO|nEdgesA=4884
2022-03-21 12:52:42,009|27556|application_1639015019875_1520|INFO|nEdgesB=3306
2022-03-21 12:52:42,009|27556|application_1639015019875_1520|13919|TIME|read|1_Census/S/AS_1e-3_3
2022-03-21 12:52:42,916|28463|application_1639015019875_1520|907|TIME|layer1S|1_Census/S/AS_1e-3_3
2022-03-21 12:52:43,676|29223|application_1639015019875_1520|760|TIME|layer2S|1_Census/S/AS_1e-3_3
2022-03-21 12:52:46,916|32463|application_1639015019875_1520|3239|TIME|overlayP|1_Census/S/AS_1e-3_3
2022-03-21 12:52:47,076|32623|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:52:49,368|34915|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:52:49,725|35272|application_1639015019875_1520|2810|TIME|overlayS|1_Census/S/AS_1e-3_3
2022-03-21 12:52:49,833|35380|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:52:49,833|35380|application_1639015019875_1520|108|TIME|end|1_Census/S/AS_1e-3_3
Run 4 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_4"
2022-03-21 12:53:06,925|13576|application_1639015019875_1521|13837|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_4 --debug --local
2022-03-21 12:53:06,971|13622|application_1639015019875_1521|INFO|scale=1000.0
2022-03-21 12:53:06,977|13628|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:53:06,977|13628|application_1639015019875_1521|INFO|npartitions=1
2022-03-21 12:53:06,977|13628|application_1639015019875_1521|52|TIME|start|1_Census/S/AS_1e-3_4
2022-03-21 12:53:15,639|22290|application_1639015019875_1521|INFO|nEdgesA=4884
2022-03-21 12:53:16,119|22770|application_1639015019875_1521|INFO|nEdgesB=3306
2022-03-21 12:53:16,119|22770|application_1639015019875_1521|9142|TIME|read|1_Census/S/AS_1e-3_4
2022-03-21 12:53:17,083|23734|application_1639015019875_1521|964|TIME|layer1S|1_Census/S/AS_1e-3_4
2022-03-21 12:53:17,293|23944|application_1639015019875_1521|210|TIME|layer2S|1_Census/S/AS_1e-3_4
2022-03-21 12:53:19,922|26573|application_1639015019875_1521|2629|TIME|overlayP|1_Census/S/AS_1e-3_4
2022-03-21 12:53:20,054|26705|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:53:22,178|28829|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:53:22,578|29229|application_1639015019875_1521|2656|TIME|overlayS|1_Census/S/AS_1e-3_4
2022-03-21 12:53:22,665|29316|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:53:22,665|29316|application_1639015019875_1521|87|TIME|end|1_Census/S/AS_1e-3_4
Run 5 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_5"
2022-03-21 12:53:39,862|13694|application_1639015019875_1522|13942|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_5 --debug --local
2022-03-21 12:53:39,903|13735|application_1639015019875_1522|INFO|scale=1000.0
2022-03-21 12:53:39,908|13740|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:53:39,908|13740|application_1639015019875_1522|INFO|npartitions=1
2022-03-21 12:53:39,908|13740|application_1639015019875_1522|46|TIME|start|1_Census/S/AS_1e-3_5
2022-03-21 12:53:48,482|22314|application_1639015019875_1522|INFO|nEdgesA=4884
2022-03-21 12:53:53,527|27359|application_1639015019875_1522|INFO|nEdgesB=3306
2022-03-21 12:53:53,527|27359|application_1639015019875_1522|13619|TIME|read|1_Census/S/AS_1e-3_5
2022-03-21 12:53:54,447|28279|application_1639015019875_1522|920|TIME|layer1S|1_Census/S/AS_1e-3_5
2022-03-21 12:53:55,135|28967|application_1639015019875_1522|688|TIME|layer2S|1_Census/S/AS_1e-3_5
2022-03-21 12:53:58,219|32051|application_1639015019875_1522|3084|TIME|overlayP|1_Census/S/AS_1e-3_5
2022-03-21 12:53:58,372|32204|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:54:00,668|34500|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:54:01,032|34864|application_1639015019875_1522|2813|TIME|overlayS|1_Census/S/AS_1e-3_5
2022-03-21 12:54:01,122|34954|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:54:01,122|34954|application_1639015019875_1522|90|TIME|end|1_Census/S/AS_1e-3_5
Run 6 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_6"
2022-03-21 12:54:18,215|13560|application_1639015019875_1523|13805|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_6 --debug --local
2022-03-21 12:54:18,255|13600|application_1639015019875_1523|INFO|scale=1000.0
2022-03-21 12:54:18,260|13605|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:54:18,260|13605|application_1639015019875_1523|INFO|npartitions=1
2022-03-21 12:54:18,261|13606|application_1639015019875_1523|46|TIME|start|1_Census/S/AS_1e-3_6
2022-03-21 12:54:27,083|22428|application_1639015019875_1523|INFO|nEdgesA=4884
2022-03-21 12:54:27,663|23008|application_1639015019875_1523|INFO|nEdgesB=3306
2022-03-21 12:54:27,663|23008|application_1639015019875_1523|9402|TIME|read|1_Census/S/AS_1e-3_6
2022-03-21 12:54:28,495|23840|application_1639015019875_1523|832|TIME|layer1S|1_Census/S/AS_1e-3_6
2022-03-21 12:54:28,708|24053|application_1639015019875_1523|213|TIME|layer2S|1_Census/S/AS_1e-3_6
2022-03-21 12:54:31,355|26700|application_1639015019875_1523|2646|TIME|overlayP|1_Census/S/AS_1e-3_6
2022-03-21 12:54:31,482|26827|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:54:33,666|29011|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:54:33,993|29338|application_1639015019875_1523|2639|TIME|overlayS|1_Census/S/AS_1e-3_6
2022-03-21 12:54:34,075|29420|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:54:34,076|29421|application_1639015019875_1523|83|TIME|end|1_Census/S/AS_1e-3_6
Run 7 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_7"
2022-03-21 12:54:51,351|13704|application_1639015019875_1524|13954|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_7 --debug --local
2022-03-21 12:54:51,399|13752|application_1639015019875_1524|INFO|scale=1000.0
2022-03-21 12:54:51,405|13758|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:54:51,405|13758|application_1639015019875_1524|INFO|npartitions=1
2022-03-21 12:54:51,405|13758|application_1639015019875_1524|54|TIME|start|1_Census/S/AS_1e-3_7
2022-03-21 12:55:00,200|22553|application_1639015019875_1524|INFO|nEdgesA=4884
2022-03-21 12:55:05,135|27488|application_1639015019875_1524|INFO|nEdgesB=3306
2022-03-21 12:55:05,135|27488|application_1639015019875_1524|13730|TIME|read|1_Census/S/AS_1e-3_7
2022-03-21 12:55:05,965|28318|application_1639015019875_1524|830|TIME|layer1S|1_Census/S/AS_1e-3_7
2022-03-21 12:55:06,723|29076|application_1639015019875_1524|758|TIME|layer2S|1_Census/S/AS_1e-3_7
2022-03-21 12:55:09,820|32173|application_1639015019875_1524|3097|TIME|overlayP|1_Census/S/AS_1e-3_7
2022-03-21 12:55:09,955|32308|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:55:12,154|34507|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:55:12,503|34856|application_1639015019875_1524|2683|TIME|overlayS|1_Census/S/AS_1e-3_7
2022-03-21 12:55:12,591|34944|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:55:12,592|34945|application_1639015019875_1524|88|TIME|end|1_Census/S/AS_1e-3_7
Run 8 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_8"
2022-03-21 12:55:30,084|14006|application_1639015019875_1525|14246|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_8 --debug --local
2022-03-21 12:55:30,126|14048|application_1639015019875_1525|INFO|scale=1000.0
2022-03-21 12:55:30,132|14054|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:55:30,132|14054|application_1639015019875_1525|INFO|npartitions=1
2022-03-21 12:55:30,132|14054|application_1639015019875_1525|48|TIME|start|1_Census/S/AS_1e-3_8
2022-03-21 12:55:38,932|22854|application_1639015019875_1525|INFO|nEdgesA=4884
2022-03-21 12:55:39,413|23335|application_1639015019875_1525|INFO|nEdgesB=3306
2022-03-21 12:55:39,413|23335|application_1639015019875_1525|9281|TIME|read|1_Census/S/AS_1e-3_8
2022-03-21 12:55:40,373|24295|application_1639015019875_1525|960|TIME|layer1S|1_Census/S/AS_1e-3_8
2022-03-21 12:55:40,583|24505|application_1639015019875_1525|210|TIME|layer2S|1_Census/S/AS_1e-3_8
2022-03-21 12:55:43,207|27129|application_1639015019875_1525|2624|TIME|overlayP|1_Census/S/AS_1e-3_8
2022-03-21 12:55:43,339|27261|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:55:45,465|29387|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:55:45,821|29743|application_1639015019875_1525|2614|TIME|overlayS|1_Census/S/AS_1e-3_8
2022-03-21 12:55:45,900|29822|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:55:45,900|29822|application_1639015019875_1525|79|TIME|end|1_Census/S/AS_1e-3_8
Run 9 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_9"
2022-03-21 12:56:03,089|13636|application_1639015019875_1526|13891|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_9 --debug --local
2022-03-21 12:56:03,134|13681|application_1639015019875_1526|INFO|scale=1000.0
2022-03-21 12:56:03,140|13687|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:56:03,140|13687|application_1639015019875_1526|INFO|npartitions=1
2022-03-21 12:56:03,140|13687|application_1639015019875_1526|51|TIME|start|1_Census/S/AS_1e-3_9
2022-03-21 12:56:11,972|22519|application_1639015019875_1526|INFO|nEdgesA=4884
2022-03-21 12:56:12,555|23102|application_1639015019875_1526|INFO|nEdgesB=3306
2022-03-21 12:56:12,555|23102|application_1639015019875_1526|9415|TIME|read|1_Census/S/AS_1e-3_9
2022-03-21 12:56:13,397|23944|application_1639015019875_1526|842|TIME|layer1S|1_Census/S/AS_1e-3_9
2022-03-21 12:56:13,630|24177|application_1639015019875_1526|233|TIME|layer2S|1_Census/S/AS_1e-3_9
2022-03-21 12:56:16,267|26814|application_1639015019875_1526|2637|TIME|overlayP|1_Census/S/AS_1e-3_9
2022-03-21 12:56:16,406|26953|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:56:18,516|29063|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:56:18,878|29425|application_1639015019875_1526|2611|TIME|overlayS|1_Census/S/AS_1e-3_9
2022-03-21 12:56:18,966|29513|Saved /tmp/edgesFE.wkt in 0.00s [73 records].
2022-03-21 12:56:18,967|29514|application_1639015019875_1526|89|TIME|end|1_Census/S/AS_1e-3_9
Run 10 ./sdcel2_debug Census/S/AS/P1 /home/acald013/RIDIR/local_path/Census/S/AS/P1/ 1e-3 "1_Census/S/AS_1e-3_10"
2022-03-21 12:56:36,599|13646|application_1639015019875_1527|13893|COMMAND|org.apache.spark.deploy.SparkSubmit --master yarn --deploy-mode client --conf spark.driver.memory=35g --conf spark.locality.wait=3s --conf spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/home/acald013/Spark/2.4/conf/log4j.properties --class edu.ucr.dblab.sdcel.SDCEL2 --files /home/acald013/Spark/2.4/conf/log4j.properties --jars /home/acald013/Spark/2.4/jars/geospark-1.2.0.jar,/home/acald013/Spark/2.4/jars/scallop_2.11-3.1.5.jar,/home/acald013/Spark/2.4/jars/spark-measure_2.11-0.16.jar --num-executors 12 --executor-cores 9 --executor-memory 20g /home/acald013/RIDIR/Code/SDCEL/target/scala-2.11/sdcel_2.11-0.1.jar --input1 Census/S/AS/P1/edgesA --input2 Census/S/AS/P1/edgesB --quadtree /home/acald013/RIDIR/local_path/Census/S/AS/P1//quadtree.wkt --boundary /home/acald013/RIDIR/local_path/Census/S/AS/P1//boundary.wkt --tolerance 1e-3 --qtag 1_Census/S/AS_1e-3_10 --debug --local
2022-03-21 12:56:36,640|13687|application_1639015019875_1527|INFO|scale=1000.0
2022-03-21 12:56:36,645|13692|Saved /tmp/edgesCells_1.wkt in 0.00s [1 records].
2022-03-21 12:56:36,646|13693|application_1639015019875_1527|INFO|npartitions=1
2022-03-21 12:56:36,646|13693|application_1639015019875_1527|47|TIME|start|1_Census/S/AS_1e-3_10
2022-03-21 12:56:45,258|22305|application_1639015019875_1527|INFO|nEdgesA=4884
2022-03-21 12:56:50,306|27353|application_1639015019875_1527|INFO|nEdgesB=3306
2022-03-21 12:56:50,306|27353|application_1639015019875_1527|13660|TIME|read|1_Census/S/AS_1e-3_10
2022-03-21 12:56:51,140|28187|application_1639015019875_1527|834|TIME|layer1S|1_Census/S/AS_1e-3_10
2022-03-21 12:56:51,910|28957|application_1639015019875_1527|770|TIME|layer2S|1_Census/S/AS_1e-3_10
2022-03-21 12:56:54,886|31933|application_1639015019875_1527|2976|TIME|overlayP|1_Census/S/AS_1e-3_10
2022-03-21 12:56:55,025|32072|Saved /tmp/edgesFC.wkt in 0.00s [73 records].
2022-03-21 12:56:57,288|34335|Saved /tmp/edgesS.wkt in 0.00s [73 records].
2022-03-21 12:56:57,911|34958|application_1639015019875_1527|3025|TIME|overlayS|1_Census/S/AS_1e-3_10
2022-03-21 12:56:58,045|35092|Saved /tmp/edgesFE.wkt in 0.01s [73 records].
2022-03-21 12:56:58,047|35094|application_1639015019875_1527|136|TIME|end|1_Census/S/AS_1e-3_10
