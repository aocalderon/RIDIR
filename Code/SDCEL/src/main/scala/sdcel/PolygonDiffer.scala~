package edu.ucr.dblab.sdcel

import com.vividsolutions.jts.geom.{Polygon, LineString}
import com.vividsolutions.jts.geom.{PrecisionModel, GeometryFactory}
import com.vividsolutions.jts.io.WKTReader
import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.sql.SparkSession
import org.apache.spark.TaskContext
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.serde.GeoSparkKryoRegistrator
import org.datasyslab.geospark.spatialRDD.SpatialRDD
import org.slf4j.{Logger, LoggerFactory}
import org.geotools.geometry.jts.GeometryClipper
import edu.ucr.dblab.sdcel.PartitionReader.readQuadtree
import edu.ucr.dblab.sdcel.quadtree._

object PolygonDiffer {
  implicit val logger: Logger = LoggerFactory.getLogger("myLogger")

  def main(args: Array[String]) = {
    // Starting session...
    logger.info("Starting session...")
    val params = new Params(args)
    val model = new PrecisionModel(params.scale())
    implicit val geofactory = new GeometryFactory(model)

    implicit val spark = SparkSession.builder()
        .config("spark.serializer", classOf[KryoSerializer].getName)
        .config("spark.kryo.registrator", classOf[GeoSparkKryoRegistrator].getName)
        .getOrCreate()
    import spark.implicits._
    logger.info("Starting session... Done!")

    // Read polygons and quadtree...
    logger.info("Reading data...")
    val (quadtree, cells) = readQuadtree[Polygon](params.quadtree(), params.boundary())
    val polyRDD = read(params.input1())
    val partitioner = new QuadTreePartitioner(quadtree)
    polyRDD.spatialPartitioning(partitioner)
    val polygons = polyRDD.spatialPartitionedRDD.rdd.persist()    
    logger.info(s"Polygons: ${polygons.count}")
    logger.info("Reading data... Done!")
    
    val differs = polygons.mapPartitionsWithIndex{ (pid, itPolygons) =>
      val cell = cells(pid)
      val clipper = new GeometryClipper(cell.mbr.getEnvelopeInternal)
      val polygons = itPolygons.map{ a =>
        val clip = clipper.clip(a, true)
        clip.setUserData(a.getUserData)
        clip
      }.toList
      val differs = for{
        p1 <- polygons
        p2 <- polygons
        if p1.getUserData.asInstanceOf[Long] < p2.getUserData.asInstanceOf[Long]
      } yield {
        val differ = p1.difference(p2)
        differ.setUserData(p2.getUserData)
        List(p1, p2, differ)
      }

      differs.zipWithIndex.flatMap{ case(geoms, id) =>
        geoms.map{ geom => 
          val wkt = geom.toText
          val pid = geom.getUserData
          s"$wkt\t$pid"
        }
      }.toIterator
    }
    val n = differs.count
    logger.info(s"Results: $n")

    spark.close
  }

  def read(input: String)
    (implicit spark: SparkSession, geofactory: GeometryFactory): SpatialRDD[Polygon] = {

    val polygonRaw = spark.read.textFile(input).rdd.zipWithIndex()
      .mapPartitionsWithIndex{ case(index, lines) =>
        val reader = new WKTReader(geofactory)
        lines.flatMap{ case(line, id) =>
          val geom = reader.read(line.replaceAll("\"", ""))
            (0 until geom.getNumGeometries).map{ i =>
              val poly = geom.getGeometryN(i).asInstanceOf[Polygon]
              val shell = geofactory.createPolygon(poly.getExteriorRing.getCoordinates())
              shell.setUserData(id)
              shell
            }
        }.toIterator
      }
    val polygonRDD = new SpatialRDD[Polygon]()
    polygonRDD.setRawSpatialRDD(polygonRaw)
    polygonRDD.analyze()

    polygonRDD
  }
}
