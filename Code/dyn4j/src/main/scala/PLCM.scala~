import com.vividsolutions.jts.geom.{Point, Geometry, GeometryFactory, Coordinate, Envelope, Polygon}
import org.slf4j.{LoggerFactory, Logger}
import org.rogach.scallop._
import org.apache.spark.storage.StorageLevel
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.serializer.KryoSerializer
import org.datasyslab.geospark.enums.{FileDataSplitter, GridType, IndexType}
import org.datasyslab.geospark.spatialOperator.JoinQuery
import org.datasyslab.geospark.spatialRDD.{CircleRDD, PointRDD}
import com.vividsolutions.jts.index.strtree.STRtree
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import scala.collection.JavaConverters._
import SPMF.{AlgoLCM2, Transactions}
import java.io._

object PLCM{
  private val logger: Logger = LoggerFactory.getLogger("myLogger")
  private val geofactory: GeometryFactory = new GeometryFactory()

  def main(args: Array[String]) = {
    val params = new PCLMConf(args)
    val input = params.input()
    val master = params.master()
    val partitions = params.partitions()
    val offset = params.offset()
    val debug = params.debug()

    // Starting session...
    var timer = clocktime
    val spark = SparkSession.builder()
      .config("spark.serializer",classOf[KryoSerializer].getName)
      .master(master).appName("PLCM").getOrCreate()
    import spark.implicits._
    logger.info(s"Session started [${(clocktime - timer) / 1000.0}]")

    // Reading disks...
    timer = clocktime
    val disks = new PointRDD(spark.sparkContext, input, offset, FileDataSplitter.TSV, true, partitions)
    val nDisks = disks.rawSpatialRDD.count()
    log("Disks read", timer, nDisks)

    /* Partitioner Benchmark...

    val partitioners = List(GridType.QUADTREE, GridType.KDBTREE, GridType.EQUALGRID, GridType.RTREE)
    val partition_set = 5 to 100 by 5
    for(partitioner <- partitioners; partition <- partition_set){
      timer = clocktime
      disks.analyze()
      disks.spatialPartitioning(GridType.QUADTREE, partition)
      logger.info(s";${partitioner.toString()};${partition};${(clocktime-timer)/1000.0}")
    }
     */

    // Partitioning disks...
    timer = clocktime
    val partitioner = GridType.KDBTREE
    disks.analyze()
    disks.spatialPartitioning(partitioner, partitions)
    log("Disks partitioned", timer, nDisks)

    // Computing partition statistics...
    val stats = disks.spatialPartitionedRDD.rdd.mapPartitions{ p =>
      List(p.length).toIterator
    }
    logger.info(s"Number of partitions: ${stats.count()}")
    logger.info(s"Max: ${stats.max()}, Min: ${stats.min()}, Avg: ${stats.sum()/stats.count()}")

    if(debug){
      val points = disks.spatialPartitionedRDD.rdd.mapPartitionsWithIndex{ (index, partition) =>
        partition.map{ disk =>
          val pids = disk.getUserData.toString()
          (disk.getX, disk.getY, pids, index)
        }
      }.persist(StorageLevel.MEMORY_ONLY)
      val nPoints = points.count()
      val pw1 = new PrintWriter(s"/tmp/${partitioner.toString()}_points.wkt")
      var wkt = points.map{ p => s"POINT(${p._1} ${p._2});${p._3};${p._4}\n"}.collect().mkString("")
      pw1.write(wkt)
      pw1.close()
      val pw2 = new PrintWriter(s"/tmp/${partitioner.toString()}_partitions.wkt")
      wkt = disks.getPartitioner.getGrids().asScala.map{ e =>
        val minx = e.getMinX
        val miny = e.getMinY
        val maxx = e.getMaxX
        val maxy = e.getMaxY
        s"POLYGON(($minx $miny, $minx $maxy, $maxx $maxy, $maxx $miny, $minx $miny))\n"
      }.mkString("")
      pw2.write(wkt)
      pw2.close()
    }

    // Prunning disks...
    timer = clocktime
    val points = disks.spatialPartitionedRDD.rdd.mapPartitionsWithIndex{ (index, partition) =>
      val transactions = partition.map{ d =>
        d.getUserData.toString().split(" ").map(new Integer(_)).toList.sorted.asJava
      }.toList.asJava
      val LCM = new AlgoLCM2()
      val data = new Transactions(transactions)
      LCM.run(data).asScala.map{ m =>
        (m.asScala.mkString(" "), index)
      }.toIterator
    }.persist(StorageLevel.MEMORY_ONLY)
    val nPoints = points.count()
    log("Disks prunned", timer, nPoints)

    points.toDF().show(10, true)

    // Closing session...
    timer = clocktime
    spark.close()
    logger.info(s"Session closed [${(clocktime - timer) / 1000.0}]")
  }

  def clocktime = System.currentTimeMillis()

  def log(msg: String, timer: Long, n: Long = 0): Unit = {
    if(n == 0)
      logger.info("%-50s|%6.2f".format(msg,(clocktime - timer)/1000.0))
    else
      logger.info("%-50s|%6.2f|%6d".format(msg,(clocktime - timer)/1000.0,n))
  }
}

class PCLMConf(args: Seq[String]) extends ScallopConf(args) {
  val input:      ScallopOption[String]  = opt[String]  (required = true)
  val master:     ScallopOption[String]  = opt[String]  (default = Some("spark://169.235.27.134:7077"))
  val epsilon:    ScallopOption[Int]     = opt[Int]     (default = Some(10))
  val partitions: ScallopOption[Int]     = opt[Int]     (default = Some(24))
  val offset:     ScallopOption[Int]     = opt[Int]     (default = Some(0))
  val debug:      ScallopOption[Boolean] = opt[Boolean] (default = Some(false))

  verify()
}
