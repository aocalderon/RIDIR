import org.apache.spark.storage.StorageLevel
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.serializer.KryoSerializer
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._
import org.datasyslab.geospark.enums.{GridType, IndexType}
import org.datasyslab.geospark.spatialOperator.JoinQuery
import org.rogach.scallop._
import org.slf4j.{LoggerFactory, Logger}
import scala.collection.JavaConverters._
import java.io._

object Areal{
  private val logger: Logger = LoggerFactory.getLogger("myLogger")

  val spark = SparkSession.builder()
    .config("spark.serializer",classOf[KryoSerializer].getName)
    .master("local[*]").appName("Areal")
    .getOrCreate()
  import spark.implicits._
  val appID = spark.sparkContext.applicationId

  var source = "/home/acald013/Datasets/gdf_2000"
  val sourceRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, source)
  val sourceIDs = sourceRDD.rawSpatialRDD.rdd.zipWithUniqueId.map{ s =>
    val id = s._2
    val geom = s._1
    geom.setUserData(s"${id}\t${geom.getUserData.toString()}")
    geom
  }.persist(StorageLevel.MEMORY_ONLY)
  sourceRDD.setRawSpatialRDD(sourceIDs)
  val nSourceRDD = sourceRDD.rawSpatialRDD.rdd.count()

  var target = "/home/acald013/Datasets/gdf_1990"
  val targetRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, target)
  val targetIDs = targetRDD.rawSpatialRDD.rdd.zipWithUniqueId.map{ t =>
    val id = t._2
    val geom = t._1
    geom.setUserData(s"${id}\t${geom.getUserData.toString()}")
    geom
  }.persist(StorageLevel.MEMORY_ONLY)
  targetRDD.setRawSpatialRDD(targetIDs)
  val nTargetRDD = targetRDD.rawSpatialRDD.rdd.count()

  val considerBoundaryIntersection = true // Only return gemeotries fully covered by each query window in queryWindowRDD
  val buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query
  val usingIndex = true
  val partitions = 256

  sourceRDD.analyze()
  sourceRDD.spatialPartitioning(GridType.QUADTREE, partitions)
  targetRDD.spatialPartitioning(sourceRDD.getPartitioner)
  sourceRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)

  val joined = JoinQuery.SpatialJoinQuery(sourceRDD, targetRDD, usingIndex, considerBoundaryIntersection)
  val nJoined = joined.count()

  val flattened = joined.rdd.flatMap{ pair =>
    val a = pair._1
    pair._2.asScala.map(b => (a, b))
  }
  val nFlattened = flattened.count()

  val areal = flattened.map{ pair =>
    val source_id  = pair._1.getUserData.toString().split("\t")(0)
    val target_id  = pair._2.getUserData.toString().split("\t")(0)
    val area = pair._1.intersection(pair._2).getArea
    (source_id, target_id, area)
  }
  val nAreal = areal.count()

  areal.toDF("SourceID", "TargetID", "Area").show(truncate=false)

  spark.close()
}

class ArealConf(args: Seq[String]) extends ScallopConf(args) {
  val source:     ScallopOption[String]  = opt[String]  (required = true)
  val target:     ScallopOption[String]  = opt[String]  (required = true)
  val host:       ScallopOption[String]  = opt[String]  (default = Some("169.235.27.134"))
  val cores:      ScallopOption[Int]     = opt[Int]     (default = Some(4))
  val executors:  ScallopOption[Int]     = opt[Int]     (default = Some(3))
  val spatial:    ScallopOption[String]  = opt[String]  (default = Some("QUADTREE"))
  val partitions: ScallopOption[Int]     = opt[Int]     (default = Some(24))
  val local:      ScallopOption[Boolean] = opt[Boolean] (default = Some(false))
  val debug:      ScallopOption[Boolean] = opt[Boolean] (default = Some(false))

  verify()
}
