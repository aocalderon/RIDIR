{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.evaluation.RegressionMetrics\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5a61213e\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1556122525216\"\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0`\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .master(\"local[*]\").appName(\"Validator\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting datasets...\n",
    "Polygons from 18 states were collected for both source and target in WKT format. They are available at: https://github.com/aocalderon/RIDIR/tree/master/Datasets/AreaTablesValidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 acald013 acald013 7.3M Apr 19 09:29 AL_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.9M Apr 19 09:29 AL_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 5.4M Apr 19 09:29 AZ_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.4M Apr 19 09:29 AZ_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 5.0M Apr 19 09:29 CO_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 2.8M Apr 19 09:29 CO_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 1.5M Apr 19 09:29 CT_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 1.4M Apr 19 09:29 CT_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 9.0M Apr 19 09:29 GA_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 4.8M Apr 19 09:29 GA_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.2M Apr 19 09:29 IL_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 1.8M Apr 19 09:29 IL_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.3M Apr 19 09:29 IN_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 2.3M Apr 19 09:29 IN_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 6.9M Apr 19 09:29 LA_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.0M Apr 19 09:29 LA_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 1.5M Apr 19 09:29 MD_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 1.3M Apr 19 09:29 MD_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 9.3M Apr 19 09:29 NC_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 5.6M Apr 19 09:29 NC_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 2.7M Apr 19 09:29 NV_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 1.4M Apr 19 09:29 NV_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 4.4M Apr 19 09:29 NY_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 2.3M Apr 19 09:29 NY_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 4.5M Apr 19 09:29 OK_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 2.1M Apr 19 09:29 OK_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 7.3M Apr 19 09:29 PA_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 4.6M Apr 19 09:29 PA_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 5.5M Apr 19 09:29 SC_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.0M Apr 19 09:29 SC_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 7.2M Apr 19 09:29 TN_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 4.1M Apr 19 09:29 TN_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 6.1M Apr 19 09:29 WA_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.7M Apr 19 09:29 WA_target.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 3.3M Apr 19 09:29 WI_source.wkt\n",
      "-rw-rw-r-- 1 acald013 acald013 2.2M Apr 19 09:29 WI_target.wkt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36msys.process._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/AreaTablesValidation\"\u001b[39m\n",
       "\u001b[36mres29_2\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m0\u001b[39m"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys.process._\n",
    "\n",
    "val path = \"/home/acald013/RIDIR/Datasets/AreaTablesValidation\"\n",
    "s\"ls -lah ${path}\" #| \"grep wkt\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each set of source & target, we run the corresponding script:\n",
    "\n",
    "* For GeoPandas implementation: https://github.com/aocalderon/RIDIR/blob/master/Code/Validation/GeoPandas_area_table_tester.py\n",
    "* For GeoSpark implementation: https://github.com/aocalderon/RIDIR/blob/master/Code/Areal/src/main/scala/GeoSpark_area_table_tester.scala\n",
    "\n",
    "Each script save the results to disk for further analysis (files are also available in the same repo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 acald013 acald013 162K Apr 19 09:29 AL_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 157K Apr 19 09:29 AL_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 165K Apr 19 09:29 AZ_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 161K Apr 19 09:29 AZ_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 169K Apr 19 09:29 CO_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 164K Apr 19 09:29 CO_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 145K Apr 19 09:29 CT_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 141K Apr 19 09:29 CT_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 257K Apr 19 09:29 GA_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 251K Apr 19 09:29 GA_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 126K Apr 19 09:29 IL_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 123K Apr 19 09:29 IL_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 186K Apr 19 09:29 IN_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 181K Apr 19 09:29 IN_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 186K Apr 19 09:29 LA_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 181K Apr 19 09:29 LA_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 120K Apr 19 09:29 MD_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 116K Apr 19 09:29 MD_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 251K Apr 19 09:29 NC_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 244K Apr 19 09:29 NC_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013  66K Apr 19 09:29 NV_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013  64K Apr 19 09:29 NV_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 273K Apr 19 09:29 NY_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 266K Apr 19 09:29 NY_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 142K Apr 19 09:29 OK_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 138K Apr 19 09:29 OK_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 369K Apr 19 09:29 PA_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 359K Apr 19 09:29 PA_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 141K Apr 19 09:29 SC_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 138K Apr 19 09:29 SC_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 162K Apr 19 09:29 TN_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 157K Apr 19 09:29 TN_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 199K Apr 19 09:29 WA_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 194K Apr 19 09:29 WA_geospark_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 197K Apr 19 09:29 WI_geopandas_test.tsv\n",
      "-rw-rw-r-- 1 acald013 acald013 191K Apr 19 09:29 WI_geospark_test.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/AreaTablesValidation\"\u001b[39m\n",
       "\u001b[36mres30_1\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m0\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"/home/acald013/RIDIR/Datasets/AreaTablesValidation\"\n",
    "s\"ls -lah ${path}\" #| \"grep tsv\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a particular state to run the validation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mstate\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"NY\"\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val state = \"NY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading results from geopandas implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgeopandas\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [_c0: string, _c1: string ... 1 more field]\n",
       "\u001b[36mres32_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m8151L\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geopandas = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(s\"${path}/${state}_geopandas_test.tsv\").distinct()\n",
    "geopandas.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading results from geospark implementation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgeospark\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [_c0: string, _c1: string ... 1 more field]\n",
       "\u001b[36mres33_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m8151L\u001b[39m"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geospark = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(s\"${path}/${state}_geospark_test.tsv\").distinct()\n",
    "geospark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging both result sets..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|area1                |area2                |\n",
      "+---------------------+---------------------+\n",
      "|1.5625384114862862E-9|1.5625384114862862E-9|\n",
      "|6.0613064292227E-7   |6.0613064292227E-7   |\n",
      "|7.053096052975711E-7 |7.053096052975713E-7 |\n",
      "|2.739295819020814E-6 |2.739295819020812E-6 |\n",
      "|3.460224149717579E-6 |3.460224149717578E-6 |\n",
      "|5.562846016544312E-6 |5.562846016544313E-6 |\n",
      "|2.5261844522811287E-4|2.526184452281128E-4 |\n",
      "|3.136674887704342E-6 |3.136674887704342E-6 |\n",
      "|1.5807291728437404E-4|1.580729172843741E-4 |\n",
      "|1.4531710548415795E-6|1.453171054841604E-6 |\n",
      "|1.5161355047734145E-7|1.5161355047734315E-7|\n",
      "|0.011766889918222573 |0.01176688991822259  |\n",
      "|5.807427196973919E-5 |5.807427196973879E-5 |\n",
      "|8.105140766924522E-6 |8.105140766924817E-6 |\n",
      "|0.017480647972263185 |0.017480647972263178 |\n",
      "|4.167211632987942E-4 |4.167211632987943E-4 |\n",
      "|2.1286769616081068E-6|2.1286769616081097E-6|\n",
      "|8.20496423948473E-6  |8.204964239484736E-6 |\n",
      "|1.054274046567112E-5 |1.0542740465671122E-5|\n",
      "|4.828190247372042E-4 |4.8281902473720415E-4|\n",
      "+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mp\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = MapPartitionsRDD[808] at map at cmd65.sc:2\n",
       "\u001b[36ms\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mDouble\u001b[39m] = MapPartitionsRDD[822] at map at cmd65.sc:4\n",
       "\u001b[36mareas\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m)] = ZippedPartitionsRDD2[823] at zip at cmd65.sc:5"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val p = geopandas.map(p => (p.getString(0).toInt, p.getString(1).toInt, p.getString(2).toDouble)).rdd\n",
    "            .sortBy(p => (p._1, p._2, p._3)).map(_._3)\n",
    "val s = geospark.map(s => (s.getString(0).toInt, s.getString(1).toInt, s.getString(2).toDouble)).rdd\n",
    "            .sortBy(p => (p._1, p._2, p._3)).map(_._3)\n",
    "val areas = p.zip(s)\n",
    "areas.toDF(\"area1\", \"area2\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running some metrics to test similarity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mreg\u001b[39m: \u001b[32mRegressionMetrics\u001b[39m = org.apache.spark.mllib.evaluation.RegressionMetrics@7dfeb7b6\n",
       "\u001b[36mres64_1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m1.0\u001b[39m\n",
       "\u001b[36mres64_2\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m5.888702431505328E-19\u001b[39m\n",
       "\u001b[36mres64_3\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m1.9481015695749533E-35\u001b[39m\n",
       "\u001b[36mres64_4\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m4.413730360562314E-18\u001b[39m"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reg = new RegressionMetrics(areas)\n",
    "reg.r2\n",
    "reg.meanAbsoluteError\n",
    "reg.meanSquaredError\n",
    "reg.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
