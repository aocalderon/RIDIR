{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialRDD.SpatialRDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.vividsolutions.jts.geom.{GeometryFactory, Geometry}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.vividsolutions.jts.io.WKTReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2bcdee23\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._ \n",
       "\u001b[39m\n",
       "\u001b[36mgeofactory\u001b[39m: \u001b[32mGeometryFactory\u001b[39m = com.vividsolutions.jts.geom.GeometryFactory@38f81aab\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1556736026565\"\u001b[39m\n",
       "\u001b[36mgridType\u001b[39m: \u001b[32mGridType\u001b[39m = QUADTREE\n",
       "\u001b[36mindexType\u001b[39m: \u001b[32mIndexType\u001b[39m = QUADTREE\n",
       "\u001b[36mpartitions\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` \n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.datasyslab.geospark.spatialRDD.SpatialRDD\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import com.vividsolutions.jts.geom.{GeometryFactory, Geometry}\n",
    "import com.vividsolutions.jts.io.WKTReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Area_interpolate\")\n",
    "    .getOrCreate()\n",
    "\n",
    "import spark.implicits._ \n",
    "val geofactory: GeometryFactory = new GeometryFactory()\n",
    "val appID = spark.sparkContext.applicationId\n",
    "val gridType = GridType.QUADTREE\n",
    "val indexType = IndexType.QUADTREE\n",
    "val partitions = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined \u001b[32mfunction\u001b[39m \u001b[36marea_table\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def area_table(sourceRDD: SpatialRDD[Geometry], targetRDD: SpatialRDD[Geometry]): RDD[(Int, Int, Double)] = {\n",
    "     // Doing spatial join...                                                                                                                                                                                                                                                   \n",
    "     val considerBoundaryIntersection = true // Only return gemeotries fully covered by each query window in queryWindowRDD                                                                                                                                                     \n",
    "     val buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query                                                                                                                                                                                              \n",
    "     val usingIndex = true\n",
    "\n",
    "     sourceRDD.analyze()\n",
    "     sourceRDD.spatialPartitioning(gridType, partitions)\n",
    "     targetRDD.spatialPartitioning(sourceRDD.getPartitioner)\n",
    "     sourceRDD.buildIndex(indexType, buildOnSpatialPartitionedRDD)\n",
    "\n",
    "     val joined = JoinQuery.SpatialJoinQuery(targetRDD, sourceRDD, usingIndex, considerBoundaryIntersection)\n",
    "     val nJoined = joined.count()\n",
    "     \n",
    "     // Flattening join results...                                                                                                                                                                                                                                              \n",
    "     val flattened = joined.rdd.flatMap{ pair =>\n",
    "       val a = pair._1\n",
    "       pair._2.asScala.map(b => (a, b))                                                                                                                                                                                                                                         \n",
    "     }                                                                                                                                                                                                                                                                          \n",
    "     val nFlattened = flattened.count()\n",
    "     \n",
    "     // Computing intersection area...                                                                                                                                                                                                                                          \n",
    "     val areal = flattened.map{ pair =>\n",
    "       val source_id  = pair._1.getUserData.toString().split(\"\\t\")(0).toInt\n",
    "       val target_id  = pair._2.getUserData.toString().split(\"\\t\")(0).toInt\n",
    "       val area = pair._1.intersection(pair._2).getArea\n",
    "       (source_id, target_id, area)\n",
    "     }                                                                                                                                                                                                                                                                          \n",
    "     val nAreaTable = areal.count()\n",
    "     \n",
    "     areal\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._ \n",
       "\u001b[39m\n",
       "defined \u001b[32mfunction\u001b[39m \u001b[36marea_interpolate\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spark.implicits._ \n",
    "def area_interpolate(spark: SparkSession, sourceRDD: SpatialRDD[Geometry], targetRDD: SpatialRDD[Geometry], extensive_variables: List[String]): Unit = {\n",
    "     val areas = area_table(sourceRDD, targetRDD).toDF(\"SID\", \"TID\", \"area\")\n",
    "     areas.show(truncate = false)\n",
    "\n",
    "     val extensiveAttributes = sourceRDD.rawSpatialRDD.rdd.map{ s =>                                                                                                                                                                                                                                                         \n",
    "       val attr = s.getUserData().toString().split(\"\\t\")                                                                                                                                                                                                                                                                     \n",
    "       val id = attr(0).toInt                                                                                                                                                                                                                                                                                                \n",
    "       val tarea = s.getArea()                                                                                                                                                                                                                                                                                               \n",
    "       val population = attr(1).toInt                                                                                                                                                                                                                                                                                        \n",
    "       val income = attr(3).toDouble                                                                                                                                                                                                                                                                                         \n",
    "       (id, tarea, population, income)                                                                                                                                                                                                                                                                                       \n",
    "     }.toDF(\"ID\", \"tarea\", \"population\", \"income\")\n",
    "\n",
    "     val table_extensive = areas.join(extensiveAttributes, $\"SID\" === $\"ID\")\n",
    "       .withColumn(\"tpopulation\", $\"area\" / $\"tarea\" * $\"population\")\n",
    "       .withColumn(\"tincome\", $\"area\" / $\"tarea\" * $\"income\")\n",
    "\n",
    "     table_extensive.orderBy($\"SID\").show(truncate = false)\n",
    "\n",
    "     val target_extensive = table_extensive.select(\"TID\", \"tpopulation\", \"tincome\")\n",
    "       .groupBy($\"TID\")\n",
    "       .agg(\n",
    "         sum($\"tpopulation\").as(\"population\"),\n",
    "         sum($\"tincome\").as(\"income\")\n",
    "       )\n",
    "\n",
    "     target_extensive.orderBy($\"TID\").show(truncate = false)\n",
    "\n",
    "     val intensiveAttributes = sourceRDD.rawSpatialRDD.rdd.map{ s =>                                                                                                                                                                                                                                                         \n",
    "       val attr = s.getUserData().toString().split(\"\\t\")                                                                                                                                                                                                                                                                     \n",
    "       val id = attr(0).toInt                                                                                                                                                                                                                                                                                                \n",
    "       val pci = attr(2).toDouble                                                                                                                                                                                                                                                                                            \n",
    "       (id, pci)                                                                                                                                                                                                                                                                                                             \n",
    "     }.toDF(\"IDS\", \"pci\")\n",
    "     val targetAreas = targetRDD.rawSpatialRDD.rdd.map{ t =>                                                                                                                                                                                                                                                                 \n",
    "       val attr = t.getUserData().toString().split(\"\\t\")                                                                                                                                                                                                                                                                     \n",
    "       val id = attr(0).toInt                                                                                                                                                                                                                                                                                                \n",
    "       val tarea = t.getArea()                                                                                                                                                                                                                                                                                               \n",
    "       (id, tarea)                                                                                                                                                                                                                                                                                                           \n",
    "     }.toDF(\"IDT\", \"tarea\")\n",
    "\n",
    "     val table_intensive = areas.join(targetAreas, $\"TID\" === $\"IDT\", \"left_outer\")\n",
    "       .join(intensiveAttributes, $\"SID\" === $\"IDS\", \"left_outer\")\n",
    "       .withColumn(\"tpci\", $\"area\" / $\"tarea\" * $\"pci\")\n",
    "\n",
    "     table_intensive.orderBy($\"TID\").show(truncate = false)\n",
    "\n",
    "     val target_intensive = table_intensive.select(\"TID\", \"tpci\")\n",
    "       .groupBy($\"TID\")\n",
    "       .agg(\n",
    "         sum($\"tpci\").as(\"pci\")\n",
    "       )\n",
    "\n",
    "     target_intensive.orderBy($\"TID\").show(truncate = false)\n",
    "\n",
    "   }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msource\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/A.wkt\"\u001b[39m\n",
       "\u001b[36msourceRDD\u001b[39m: \u001b[32mSpatialRDD\u001b[39m[\u001b[32mGeometry\u001b[39m] = org.datasyslab.geospark.spatialRDD.SpatialRDD@22f9fc50\n",
       "\u001b[36msourceWKT\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mGeometry\u001b[39m] = MapPartitionsRDD[27] at map at cmd14.sc:5\n",
       "\u001b[36mnSourceRDD\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m2L\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     // Reading source...                                                                                                                                                                                                                                                                                                    \n",
    "     val source = \"/home/acald013/RIDIR/Datasets/A.wkt\"\n",
    "     val sourceRDD = new SpatialRDD[Geometry]()\n",
    "     val sourceWKT = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").\n",
    "       csv(source).rdd.\n",
    "       map{ s =>\n",
    "         val geom = new WKTReader(geofactory).read(s.getString(0))\n",
    "         val id = s.getString(1)\n",
    "         val population = s.getString(2).toInt\n",
    "         val pci = s.getString(3).toDouble\n",
    "         val income = population * pci\n",
    "         val userData = s\"$id\\t$population\\t$pci\\t$income\"\n",
    "         geom.setUserData(userData)\n",
    "         geom\n",
    "       }                                                                                                                                                                                                                                                                                                                     \n",
    "     sourceRDD.setRawSpatialRDD(sourceWKT)\n",
    "     val nSourceRDD = sourceRDD.rawSpatialRDD.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtarget\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/B.wkt\"\u001b[39m\n",
       "\u001b[36mtargetRDD\u001b[39m: \u001b[32mSpatialRDD\u001b[39m[\u001b[32mGeometry\u001b[39m] = org.datasyslab.geospark.spatialRDD.SpatialRDD@263604e\n",
       "\u001b[36mtargetWKT\u001b[39m: \u001b[32mRDD\u001b[39m[\u001b[32mGeometry\u001b[39m] = MapPartitionsRDD[41] at map at cmd15.sc:5\n",
       "\u001b[36mnTargetRDD\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m3L\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "     // Reading target...     \n",
    "    val target = \"/home/acald013/RIDIR/Datasets/B.wkt\"\n",
    "    val targetRDD = new SpatialRDD[Geometry]()\n",
    "    val targetWKT = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").\n",
    "       csv(target).rdd.\n",
    "       map{ s =>\n",
    "         val geom = new WKTReader(geofactory).read(s.getString(0))\n",
    "         val id = s.getString(1)\n",
    "         geom.setUserData(id)\n",
    "         geom\n",
    "       }                                                                                                                                                                                                                                                                                                                     \n",
    "     targetRDD.setRawSpatialRDD(targetWKT)\n",
    "     val nTargetRDD = targetRDD.rawSpatialRDD.rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+\n",
      "|SID|TID|area|\n",
      "+---+---+----+\n",
      "|1  |2  |25.0|\n",
      "|1  |1  |25.0|\n",
      "|2  |1  |10.0|\n",
      "|2  |2  |25.0|\n",
      "|2  |3  |15.0|\n",
      "+---+---+----+\n",
      "\n",
      "+---+---+----+---+-----+----------+-------+-----------+-------+\n",
      "|SID|TID|area|ID |tarea|population|income |tpopulation|tincome|\n",
      "+---+---+----+---+-----+----------+-------+-----------+-------+\n",
      "|1  |2  |25.0|1  |50.0 |500       |37500.0|250.0      |18750.0|\n",
      "|1  |1  |25.0|1  |50.0 |500       |37500.0|250.0      |18750.0|\n",
      "|2  |1  |10.0|2  |50.0 |200       |20000.0|40.0       |4000.0 |\n",
      "|2  |2  |25.0|2  |50.0 |200       |20000.0|100.0      |10000.0|\n",
      "|2  |3  |15.0|2  |50.0 |200       |20000.0|60.0       |6000.0 |\n",
      "+---+---+----+---+-----+----------+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "     // Calling area_table method...                                                                                                                                                                                                                                                                                         \n",
    "     val extensive = List(\"population\", \"income\")\n",
    "     area_interpolate(spark, sourceRDD, targetRDD, extensive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
