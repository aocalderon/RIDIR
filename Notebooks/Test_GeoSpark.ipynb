{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries if need them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "//import $ivy.`sh.almond::almond-spark:0.3\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "//import $ivy.`sh.almond::almond-spark:0.3\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@43a9e0f7\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1557272476150\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Areal\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading source..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msource\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/Datasets/gdf_2000\"\u001b[39m\n",
       "\u001b[36msourceRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mdatasyslab\u001b[39m.\u001b[32mgeospark\u001b[39m.\u001b[32mspatialRDD\u001b[39m.\u001b[32mSpatialRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = org.datasyslab.geospark.spatialRDD.SpatialRDD@543ad71\n",
       "\u001b[36msourceIDs\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = MapPartitionsRDD[5] at map at cmd3.sc:3\n",
       "\u001b[36mnSourceRDD\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m72693L\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var source = \"/home/acald013/Datasets/gdf_2000\"\n",
    "val sourceRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, source)\n",
    "val sourceIDs = sourceRDD.rawSpatialRDD.rdd.zipWithUniqueId.map{ s =>\n",
    "    val id = s._2\n",
    "    val geom = s._1\n",
    "    geom.setUserData(s\"${id}\\t${geom.getUserData.toString()}\")\n",
    "    geom\n",
    "}.persist(StorageLevel.MEMORY_ONLY)\n",
    "sourceRDD.setRawSpatialRDD(sourceIDs)\n",
    "val nSourceRDD = sourceRDD.rawSpatialRDD.rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading target..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtarget\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/Datasets/gdf_1990\"\u001b[39m\n",
       "\u001b[36mtargetRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mdatasyslab\u001b[39m.\u001b[32mgeospark\u001b[39m.\u001b[32mspatialRDD\u001b[39m.\u001b[32mSpatialRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = org.datasyslab.geospark.spatialRDD.SpatialRDD@19272eb3\n",
       "\u001b[36mtargetIDs\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = MapPartitionsRDD[11] at map at cmd4.sc:3\n",
       "\u001b[36mnTargetRDD\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m61332L\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var target = \"/home/acald013/Datasets/gdf_1990\"\n",
    "val targetRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, target)\n",
    "val targetIDs = targetRDD.rawSpatialRDD.rdd.zipWithUniqueId.map{ t =>\n",
    "    val id = t._2\n",
    "    val geom = t._1\n",
    "    geom.setUserData(s\"${id}\\t${geom.getUserData.toString()}\")\n",
    "    geom\n",
    "}.persist(StorageLevel.MEMORY_ONLY)\n",
    "targetRDD.setRawSpatialRDD(targetIDs)\n",
    "val nTargetRDD = targetRDD.rawSpatialRDD.rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing spatial join..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sourceRDD.rawSpatialRDD.rdd.map{s => \n",
    "    val arr = s.getUserData().toString().split(\"\\t\")\n",
    "    (arr(12), arr(14), s.toText())\n",
    "}.toDF()\n",
    "//.filter(_.getString(0) == \"01001020300\")\n",
    ".show(truncate = true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t01023009567\t\t0103000000010000007B070000DC9C4A06801A56C08C852172FA164040C9C9C4AD821A56C02F87DD770C174040E6948098841A56C0D36BB3B112174040FED30D14781A56C0F209D9791B174040745FCE6C571A56C0DDEBA4BE2C1740407024D060531A56C053B4722F301740409696917A4F1A56C0868DB27E33174040C32D\n",
      "1\t01023009568\t\t010300000001000000F6090000A3E5400FB51456C024624A24D1034040F0517FBDC21456C024624A24D1034040BA30D28BDA1456C033E02C25CB0340402BC1E270E61456C02A029CDEC5034040195932C7F21456C08FDE701FB9034040CFF6E80DF71456C043A9BD88B6034040925CFE43FA1456C03E20D099B4034040925C\n",
      "2\t01023009569\t\t0103000000010000008B040000840EBA84431D56C001C11C3D7ECF3F40A699EE75521D56C0707B82C476CF3F40FD6B79E57A1D56C01841632651CF3F400AB952CF821D56C0F9BF232A54CF3F4009371955861D56C0EE04FBAF73CF3F408D9AAF928F1D56C0AF9811DE1ED03F408E058541991D56C01363997E89D03F401DC9\n",
      "3\t01023009570\t\t01030000000100000082020000FA7C9411170C56C00E881057CEB23F4023DBF97E6A0C56C0C6DB4AAFCDB23F40087767EDB60C56C07F2F8507CDB23F4062C092AB580D56C050DF32A7CBB23F40632AFD84B30D56C044FD2E6CCDB23F40E76F4221020F56C0EB19C231CBB23F40CBF78C44680F56C02D095053CBB23F400000\n",
      "4\t01037009610\t\t01030000000100000032030000A62BD8463C8655C0950C0055DC7440401EFB592C458655C05E2F4D11E0744040FBC9181F668655C0D331E719FB7440403E5B07077B8655C0FD1532570675404038A45181938655C07D586FD40A754040DD60A8C30A8755C00CE544BB0A7540400A698D41278755C054C554FA097540404A41\n",
      "5\t01037009611\t\t010300000001000000A4040000B1A6B228EC9455C04C8A8F4FC86E4040FE60E0B9F79455C004AA7F10C96E404047E350BF0B9555C03D0CAD4ECE6E404043AB9333149555C0BE4EEACBD26E4040527DE717259555C0EB707495EE6E4040CB4C69FD2D9555C0BD715298F76E404025E99AC9379555C03EB48F15FC6E40405FD0\n",
      "6\t01037009612\t\t0103000000010000004A040000D594641D8E9555C0E3DF675C38624040FE0DDAAB8F9555C0E4D9E55B1F624040051901158E9555C063B323D577604040E60819C8B39555C088BD50C07660404007EA9447379655C06092CA14736040405BD1E638B79655C026FC523F6F604040003B376DC69755C0C68B8521726040406AF7\n",
      "7\t01009000502\t\t0103000000010000005103000054E3A59BC4A355C06361889CBEF840408907944DB9A355C09F76F86BB2F84040BC5B59A2B3A355C0FA0CA837A3F8404099D36531B1A355C03352EFA99CF8404031D3F6AFACA355C067F1626188F84040732A1900AAA355C00A4B3CA06CF840404F0647C9ABA355C047C8409E5DF84040A83A\n",
      "8\t01009000503\t\t010300000001000000F901000011E0F42EDE9B55C020F0C000C2FF40403A3FC571E09B55C02F6EA301BCFF40401A8A3BDEE49B55C000529B38B9FF4040DD408177F29B55C0340EF5BBB0FF404016DD7A4D0F9C55C0FCABC77DABFF4040F46E2C280C9C55C020F0C000C2FF404052431B800D9C55C0E7357689EAFF4040213D\n",
      "9\t01009000504\t\t010300000001000000BD020000FF25A94C319955C0F1129CFA401441405DFA97A4329955C09AB4A9BA47144140EC866D8B329955C01F4C8A8F4F144140641F6459309955C0198D7C5EF114414094A30051309955C052BAF42F49154140771211FE459955C07A19C5724B15414051F701486D9955C07F349C3237154140022C\n",
      "10\t01009000505\t\t010300000001000000DC0300006C98A1F144B055C0191F662FDBFC4040F3CB608C48B055C08FFE976BD1FC4040BD56427749B055C004594FADBEFC4040448A01124DB055C0B7B585E7A5FC40404EB6813B50B055C07575C7629BFC4040C4E9245B5DB055C05E81E84999FC4040A585CB2A6CB055C0C5AEEDED96FC40404CC2\n",
      "11\t01009000506\t\t010300000001000000B6020000CBB9145795BD55C00648348122EC4040B4942C27A1BD55C0A873452921EC404000FE2955A2BD55C0B935E9B644EC4040C4978922A4BD55C0068195438BEC404098689082A7BD55C01FF64201DBED4040BCADF4DA6CBD55C0836A8313D1ED4040130CE71A66BD55C0D4282499D5ED40408CD8\n",
      "12\t01009000507\t\t010300000001000000830200001711C5E40DAD55C0791F477364E74040C00303081FAD55C0276728EE78E740406A50340F60AD55C0B29DEFA7C6E740407862D68BA1AD55C03DD4B66114E84040787FBC57ADAD55C036CCD07822E84040051555BFD2AD55C0F5B86FB54EE8404023BF7E880DAE55C0378AAC3594E84040242A\n",
      "13\t01009050101\t\t010300000001000000520300006938656EBE9655C06B64575A46F0404033DDEBA4BE9655C0E564E25641F04040F790F0BDBF9655C075594C6C3EF040402C9E7AA4C19655C08EE9094B3CF0404036CAFACDC49655C06A1327F73BF04040A6EF3504C79655C0B2F336363BF04040EC67B114C99655C0541F48DE39F04040E4F4\n",
      "14\t01009050102\t\t0103000000010000005B04000002124DA0889F55C0280EA0DFF7F740403A5D16139B9F55C0008C67D0D0F740402252D32EA69F55C0E60819C8B3F74040D8F2CAF5B69F55C08BE1EA0088F74040BABF7ADCB79F55C0B9E00CFE7EF740405F268A90BA9F55C001C11C3D7EF740403A22DFA5D49F55C0AC1C5A643BF74040202A\n"
     ]
    }
   ],
   "source": [
    "targetRDD.rawSpatialRDD.rdd.map(s => s.getUserData().toString()).take(15).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconsiderBoundaryIntersection\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36mbuildOnSpatialPartitionedRDD\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36musingIndex\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36mpartitions\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m1024\u001b[39m\n",
       "\u001b[36mres5_4\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36mjoined\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mapi\u001b[39m.\u001b[32mjava\u001b[39m.\u001b[32mJavaPairRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m, \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mHashSet\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m]] = org.apache.spark.api.java.JavaPairRDD@7ad91aa1\n",
       "\u001b[36mnJoined\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m61313L\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val considerBoundaryIntersection = true // Only return gemeotries fully covered by each query window in queryWindowRDD\n",
    "val buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\n",
    "val usingIndex = true\n",
    "val partitions = 1024\n",
    "\n",
    "sourceRDD.analyze()\n",
    "sourceRDD.spatialPartitioning(GridType.QUADTREE, partitions)\n",
    "targetRDD.spatialPartitioning(sourceRDD.getPartitioner)\n",
    "sourceRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n",
    "\n",
    "val joined = JoinQuery.SpatialJoinQuery(sourceRDD, targetRDD, usingIndex, considerBoundaryIntersection)\n",
    "val nJoined = joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing the join results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mflattened\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m, \u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m)] = MapPartitionsRDD[24] at flatMap at cmd6.sc:1\n",
       "\u001b[36mnFlattened\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m357427L\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattened = joined.rdd.flatMap{ pair =>\n",
    "    val a = pair._1\n",
    "    pair._2.asScala.map(b => (a, b))\n",
    "}\n",
    "val nFlattened = flattened.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the area..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mareal\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mString\u001b[39m, \u001b[32mString\u001b[39m, \u001b[32mDouble\u001b[39m)] = MapPartitionsRDD[25] at map at cmd7.sc:1\n",
       "\u001b[36mnAreal\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m357427L\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val areal = flattened.map{ pair =>\n",
    "    val source_id  = pair._1.getUserData.toString().split(\"\\t\")(0)\n",
    "    val target_id  = pair._2.getUserData.toString().split(\"\\t\")(0)\n",
    "    val area = pair._1.intersection(pair._2).getArea\n",
    "    (source_id, target_id, area)\n",
    "}\n",
    "val nAreal = areal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------------+\n",
      "|SourceID|TargetID|Area                 |\n",
      "+--------+--------+---------------------+\n",
      "|22193   |28801   |1.6695735321167273E-6|\n",
      "|22193   |28841   |5.571185507966348E-7 |\n",
      "|22193   |28888   |3.7806561723031056E-5|\n",
      "|22193   |28906   |1.1618461940393888E-6|\n",
      "|25527   |31078   |2.5008364368929475E-6|\n",
      "|25527   |31080   |0.002344273245773022 |\n",
      "|25527   |31081   |1.286372551098695E-5 |\n",
      "|25527   |31079   |8.344323279225162E-6 |\n",
      "|37570   |43943   |6.932227441694678E-5 |\n",
      "|37570   |43933   |0.1662606571830884   |\n",
      "|37570   |43953   |8.687769861680146E-7 |\n",
      "|37570   |44004   |7.392856252058025E-5 |\n",
      "|37570   |44006   |1.0873067647508585E-4|\n",
      "|37570   |43937   |1.158960074807664E-4 |\n",
      "|37570   |47552   |4.338532850032905E-6 |\n",
      "|37570   |43939   |3.5375703881186553E-6|\n",
      "|37570   |43940   |1.0177161635319456E-5|\n",
      "|37570   |43934   |6.810584669886284E-5 |\n",
      "|37570   |47559   |1.159701892252949E-8 |\n",
      "|13873   |18515   |9.74852410303433E-5  |\n",
      "+--------+--------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "areal.toDF(\"SourceID\", \"TargetID\", \"Area\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
