{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "//import $ivy.`sh.almond::almond-spark:0.3\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@6cf8004a\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1556076169608\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "//import $ivy.`sh.almond::almond-spark:0.3\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Sampler\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mCA_source\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, _c1: string]\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m5846L\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CA_source = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(\"/home/acald013/Datasets/Validation/CA_source.wkt\")\n",
    "CA_source.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mCA_target\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, _c1: string]\n",
       "\u001b[36mres2_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m7953L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CA_target = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(\"/home/acald013/Datasets/Validation/CA_target.wkt\")\n",
    "CA_target.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 size: 584 \n",
      "Sample 1 size: 1151 \n",
      "Sample 2 size: 1695 \n",
      "Sample 3 size: 2289 \n",
      "Sample 4 size: 2864 \n",
      "Sample 5 size: 3461 \n",
      "Sample 6 size: 4025 \n",
      "Sample 7 size: 4666 \n",
      "Sample 8 size: 5270 \n",
      "Sample 9 size: 5846 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwithReplacement\u001b[39m: \u001b[32mBoolean\u001b[39m = false\n",
       "\u001b[36mfractions\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m0.1\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.4\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.6\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.8\u001b[39m, \u001b[32m0.9\u001b[39m, \u001b[32m1.0\u001b[39m)\n",
       "\u001b[36mseed\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m42\u001b[39m\n",
       "\u001b[36mi\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withReplacement = false\n",
    "val fractions = List(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)\n",
    "val seed = 42\n",
    "var i = 0\n",
    "for(fraction <- fractions){\n",
    "    val source_sample = CA_source.sample(withReplacement, fraction, seed)\n",
    "    val n = source_sample.count()\n",
    "    val pw = new PrintWriter(s\"/home/acald013/Datasets/Validation/SampleCA_source${i}.wkt\")\n",
    "    pw.write(source_sample.map(x => s\"${x.getString(0)}\\t${x.getString(1)}\\n\").collect().mkString(\"\"))\n",
    "    pw.close()\n",
    "    print(s\"Sample $i size: $n \\n\")\n",
    "    i = i + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 0 size: 787 \n",
      "Sample 1 size: 1581 \n",
      "Sample 2 size: 2342 \n",
      "Sample 3 size: 3160 \n",
      "Sample 4 size: 3960 \n",
      "Sample 5 size: 4748 \n",
      "Sample 6 size: 5534 \n",
      "Sample 7 size: 6380 \n",
      "Sample 8 size: 7185 \n",
      "Sample 9 size: 7953 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwithReplacement\u001b[39m: \u001b[32mBoolean\u001b[39m = false\n",
       "\u001b[36mfractions\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m0.1\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.4\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.6\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.8\u001b[39m, \u001b[32m0.9\u001b[39m, \u001b[32m1.0\u001b[39m)\n",
       "\u001b[36mseed\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m42\u001b[39m\n",
       "\u001b[36mj\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m10\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withReplacement = false\n",
    "val fractions = List(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)\n",
    "val seed = 42\n",
    "var j = 0\n",
    "for(fraction <- fractions){\n",
    "    val target_sample = CA_target.sample(withReplacement, fraction, seed)\n",
    "    val n = target_sample.count()\n",
    "    val pw = new PrintWriter(s\"/home/acald013/Datasets/Validation/SampleCA_target${j}.wkt\")\n",
    "    pw.write(target_sample.map(x => s\"${x.getString(0)}\\t${x.getString(1)}\\n\").collect().mkString(\"\"))\n",
    "    pw.close()\n",
    "    print(s\"Sample $j size: $n \\n\")\n",
    "    j = j + 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
