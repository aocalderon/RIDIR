{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "//import $ivy.`sh.almond::almond-spark:0.3\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3b65decc\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1556074836109\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "//import $ivy.`sh.almond::almond-spark:0.3\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Sampler\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mCA_source\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, _c1: string]\n",
       "\u001b[36mres1_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m5846L\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CA_source = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(\"/home/acald013/Datasets/Validation/CA_source.wkt\")\n",
    "CA_source.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mCA_target\u001b[39m: \u001b[32mDataFrame\u001b[39m = [_c0: string, _c1: string]\n",
       "\u001b[36mres2_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m7953L\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val CA_target = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(\"/home/acald013/Datasets/Validation/CA_target.wkt\")\n",
    "CA_target.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 584 \n",
      "Sample size: 1151 \n",
      "Sample size: 1695 \n",
      "Sample size: 2289 \n",
      "Sample size: 2864 \n",
      "Sample size: 3461 \n",
      "Sample size: 4025 \n",
      "Sample size: 4666 \n",
      "Sample size: 5270 \n",
      "Sample size: 5846 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwithReplacement\u001b[39m: \u001b[32mBoolean\u001b[39m = false\n",
       "\u001b[36mfractions\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mDouble\u001b[39m] = \u001b[33mList\u001b[39m(\u001b[32m0.1\u001b[39m, \u001b[32m0.2\u001b[39m, \u001b[32m0.3\u001b[39m, \u001b[32m0.4\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m0.6\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m0.8\u001b[39m, \u001b[32m0.9\u001b[39m, \u001b[32m1.0\u001b[39m)\n",
       "\u001b[36mseed\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m42\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val withReplacement = false\n",
    "val fractions = List(0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1)\n",
    "val seed = 42\n",
    "for(fraction <- fractions){\n",
    "    val source_sample = CA_source.sample(withReplacement, fraction, seed)\n",
    "    val n = source_sample.count()\n",
    "    print(s\"Sample size: $n \\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample size: 787 \n",
      "Sample size: 1581 \n",
      "Sample size: 2342 \n",
      "Sample size: 3160 \n",
      "Sample size: 3960 \n",
      "Sample size: 4748 \n",
      "Sample size: 5534 \n",
      "Sample size: 6380 \n",
      "Sample size: 7185 \n",
      "Sample size: 7953 \n"
     ]
    }
   ],
   "source": [
    "for(fraction <- fractions){\n",
    "    val target_sample = CA_target.sample(withReplacement, fraction, seed)\n",
    "    val n = target_sample.count()\n",
    "    print(s\"Sample size: $n \\n\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
