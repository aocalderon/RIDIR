{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing libraries if need them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "//import $ivy.`sh.almond::almond-spark:0.3\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "//import $ivy.`sh.almond::almond-spark:0.3\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing classes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@b3c6acd\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1555048174635\"\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Areal\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading source..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36msource\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/Datasets/gdf_2000\"\u001b[39m\n",
       "\u001b[36msourceRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mdatasyslab\u001b[39m.\u001b[32mgeospark\u001b[39m.\u001b[32mspatialRDD\u001b[39m.\u001b[32mSpatialRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = org.datasyslab.geospark.spatialRDD.SpatialRDD@7b0e36dd\n",
       "\u001b[36msourceIDs\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = MapPartitionsRDD[5] at map at cmd14.sc:3\n",
       "\u001b[36mnSourceRDD\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m72693L\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var source = \"/home/acald013/Datasets/gdf_2000\"\n",
    "val sourceRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, source)\n",
    "val sourceIDs = sourceRDD.rawSpatialRDD.rdd.zipWithUniqueId.map{ s =>\n",
    "    val id = s._2\n",
    "    val geom = s._1\n",
    "    geom.setUserData(s\"${id}\\t${geom.getUserData.toString()}\")\n",
    "    geom\n",
    "}.persist(StorageLevel.MEMORY_ONLY)\n",
    "sourceRDD.setRawSpatialRDD(sourceIDs)\n",
    "val nSourceRDD = sourceRDD.rawSpatialRDD.rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading target..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mtarget\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/Datasets/gdf_1990\"\u001b[39m\n",
       "\u001b[36mtargetRDD\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mdatasyslab\u001b[39m.\u001b[32mgeospark\u001b[39m.\u001b[32mspatialRDD\u001b[39m.\u001b[32mSpatialRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = org.datasyslab.geospark.spatialRDD.SpatialRDD@5edcfebb\n",
       "\u001b[36mtargetIDs\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m] = MapPartitionsRDD[11] at map at cmd15.sc:3\n",
       "\u001b[36mnTargetRDD\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m61332L\u001b[39m"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var target = \"/home/acald013/Datasets/gdf_1990\"\n",
    "val targetRDD = ShapefileReader.readToGeometryRDD(spark.sparkContext, target)\n",
    "val targetIDs = targetRDD.rawSpatialRDD.rdd.zipWithUniqueId.map{ t =>\n",
    "    val id = t._2\n",
    "    val geom = t._1\n",
    "    geom.setUserData(s\"${id}\\t${geom.getUserData.toString()}\")\n",
    "    geom\n",
    "}.persist(StorageLevel.MEMORY_ONLY)\n",
    "targetRDD.setRawSpatialRDD(targetIDs)\n",
    "val nTargetRDD = targetRDD.rawSpatialRDD.rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing spatial join..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mconsiderBoundaryIntersection\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36mbuildOnSpatialPartitionedRDD\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36musingIndex\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36mpartitions\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m256\u001b[39m\n",
       "\u001b[36mres16_4\u001b[39m: \u001b[32mBoolean\u001b[39m = true\n",
       "\u001b[36mjoined\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mapi\u001b[39m.\u001b[32mjava\u001b[39m.\u001b[32mJavaPairRDD\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m, \u001b[32mjava\u001b[39m.\u001b[32mutil\u001b[39m.\u001b[32mHashSet\u001b[39m[\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m]] = org.apache.spark.api.java.JavaPairRDD@75e288de\n",
       "\u001b[36mnJoined\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m61313L\u001b[39m"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val considerBoundaryIntersection = true // Only return gemeotries fully covered by each query window in queryWindowRDD\n",
    "val buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query\n",
    "val usingIndex = true\n",
    "val partitions = 256\n",
    "\n",
    "sourceRDD.analyze()\n",
    "sourceRDD.spatialPartitioning(GridType.QUADTREE, partitions)\n",
    "targetRDD.spatialPartitioning(sourceRDD.getPartitioner)\n",
    "sourceRDD.buildIndex(IndexType.QUADTREE, buildOnSpatialPartitionedRDD)\n",
    "\n",
    "val joined = JoinQuery.SpatialJoinQuery(sourceRDD, targetRDD, usingIndex, considerBoundaryIntersection)\n",
    "val nJoined = joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postprocessing the join results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mflattened\u001b[39m: \u001b[32morg\u001b[39m.\u001b[32mapache\u001b[39m.\u001b[32mspark\u001b[39m.\u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[(\u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m, \u001b[32mcom\u001b[39m.\u001b[32mvividsolutions\u001b[39m.\u001b[32mjts\u001b[39m.\u001b[32mgeom\u001b[39m.\u001b[32mGeometry\u001b[39m)] = MapPartitionsRDD[24] at flatMap at cmd17.sc:1\n",
       "\u001b[36mnFlattened\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m357427L\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flattened = joined.rdd.flatMap{ pair =>\n",
    "    val a = pair._1\n",
    "    pair._2.asScala.map(b => (a, b))\n",
    "}\n",
    "val nFlattened = flattened.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the area..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val areal = flattened.map{ pair =>\n",
    "    val source_id  = pair._1.getUserData.toString().split(\"\\t\")(0)\n",
    "    val target_id  = pair._2.getUserData.toString().split(\"\\t\")(0)\n",
    "    val area = pair._1.intersection(pair._2).getArea\n",
    "    (source_id, target_id, area)\n",
    "}\n",
    "val nAreal = areal.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reporting the results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areal.toDF(\"SourceID\", \"TargetID\", \"Area\").show(truncate=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing the session..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
