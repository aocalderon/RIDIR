{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating SparkSession\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql.functions._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialRDD.SpatialRDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.vividsolutions.jts.geom.{GeometryFactory, Geometry}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mcom.vividsolutions.jts.io.WKTReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@2bcdee23\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1556736026565\"\u001b[39m\n",
       "\u001b[36mgridType\u001b[39m: \u001b[32mGridType\u001b[39m = QUADTREE\n",
       "\u001b[36mindexType\u001b[39m: \u001b[32mIndexType\u001b[39m = QUADTREE"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` \n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.datasyslab.geospark.spatialRDD.SpatialRDD\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import com.vividsolutions.jts.geom.{GeometryFactory, Geometry}\n",
    "import com.vividsolutions.jts.io.WKTReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .config(\"spark.serializer\",classOf[KryoSerializer].getName)\n",
    "    .master(\"local[*]\").appName(\"Area_interpolate\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId\n",
    "val gridType = GridType.QUADTREE\n",
    "val indexType = IndexType.QUADTREE\n",
    "val partitions = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cmd4.sc:8: not found: value partitions\n",
      "     sourceRDD.spatialPartitioning(gridType, partitions)\n",
      "                                             ^cmd4.sc:29: not found: value nAreaTable\n",
      "     nAreaTable = areal.count()\n",
      "     ^Compilation Failed"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Compilation Failed"
     ]
    }
   ],
   "source": [
    "def area_table(sourceRDD: SpatialRDD[Geometry], targetRDD: SpatialRDD[Geometry]): RDD[(Int, Int, Double)] = {\n",
    "     // Doing spatial join...                                                                                                                                                                                                                                                   \n",
    "     val considerBoundaryIntersection = true // Only return gemeotries fully covered by each query window in queryWindowRDD                                                                                                                                                     \n",
    "     val buildOnSpatialPartitionedRDD = true // Set to TRUE only if run join query                                                                                                                                                                                              \n",
    "     val usingIndex = true\n",
    "\n",
    "     sourceRDD.analyze()\n",
    "     sourceRDD.spatialPartitioning(gridType, partitions)\n",
    "     targetRDD.spatialPartitioning(sourceRDD.getPartitioner)\n",
    "     sourceRDD.buildIndex(indexType, buildOnSpatialPartitionedRDD)\n",
    "\n",
    "     val joined = JoinQuery.SpatialJoinQuery(targetRDD, sourceRDD, usingIndex, considerBoundaryIntersection)\n",
    "     val nJoined = joined.count()\n",
    "     \n",
    "     // Flattening join results...                                                                                                                                                                                                                                              \n",
    "     val flattened = joined.rdd.flatMap{ pair =>\n",
    "       val a = pair._1\n",
    "       pair._2.asScala.map(b => (a, b))                                                                                                                                                                                                                                         \n",
    "     }                                                                                                                                                                                                                                                                          \n",
    "     val nFlattened = flattened.count()\n",
    "     \n",
    "     // Computing intersection area...                                                                                                                                                                                                                                          \n",
    "     val areal = flattened.map{ pair =>\n",
    "       val source_id  = pair._1.getUserData.toString().split(\"\\t\")(0).toInt\n",
    "       val target_id  = pair._2.getUserData.toString().split(\"\\t\")(0).toInt\n",
    "       val area = pair._1.intersection(pair._2).getArea\n",
    "       (source_id, target_id, area)\n",
    "     }                                                                                                                                                                                                                                                                          \n",
    "     val nAreaTable = areal.count()\n",
    "     areal\n",
    "   }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
