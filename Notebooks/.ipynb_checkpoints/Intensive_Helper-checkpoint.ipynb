{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.evaluation.RegressionMetrics\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@58bc7093\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1557884378542\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0`\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .master(\"local[*]\").appName(\"Validator\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+----------------+\n",
      "|      geoid|n_total_pop|n_white_under_15|p_white_under_15|\n",
      "+-----------+-----------+----------------+----------------+\n",
      "|42101000100|       2650|              34|            1.28|\n",
      "|42101000200|       1362|               5|            0.37|\n",
      "|42101000300|       2570|             102|            3.97|\n",
      "|42101000400|       4313|              40|            0.93|\n",
      "|42101000500|       1126|               1|            0.09|\n",
      "|42101000600|       1315|               7|            0.53|\n",
      "|42101000700|       2564|              32|            1.25|\n",
      "|42101000800|       8461|             188|            2.22|\n",
      "|42101000900|       4969|              50|            1.01|\n",
      "|42101001000|       5808|             278|            4.79|\n",
      "|42101001100|       5925|             124|            2.09|\n",
      "|42101001200|       8148|             418|            5.13|\n",
      "|42101001300|       4353|              75|            1.72|\n",
      "|42101001400|       3585|              49|            1.37|\n",
      "|42101001500|       2179|              79|            3.63|\n",
      "|42101001600|       1981|             105|             5.3|\n",
      "|42101001700|       2415|             150|            6.21|\n",
      "|42101001800|       2398|             114|            4.75|\n",
      "|42101001900|       2686|               4|            0.15|\n",
      "|42101002000|       2334|               4|            0.17|\n",
      "+-----------+-----------+----------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_phili_2000_path\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/df_phili_2000.csv\"\u001b[39m\n",
       "\u001b[36mdf_phili_2000\u001b[39m: \u001b[32mDataFrame\u001b[39m = [geoid: string, n_total_pop: string ... 2 more fields]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_phili_2000_path = \"/home/acald013/RIDIR/Datasets/df_phili_2000.csv\"\n",
    "val df_phili_2000 = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(df_phili_2000_path)\n",
    "df_phili_2000.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+----------------+------------------+\n",
      "|      geoid|n_total_pop|n_white_under_15|  p_white_under_15|\n",
      "+-----------+-----------+----------------+------------------+\n",
      "|42101000100|     3183.0|            16.0|0.5026704366949418|\n",
      "|42101000200|     2009.0|             0.0|               0.0|\n",
      "|42101000300|     3509.0|           143.0| 4.075235109717868|\n",
      "|42101000401|     2287.0|            20.0| 0.874508089199825|\n",
      "|42101000402|     3021.0|            19.0| 0.628930817610063|\n",
      "|42101000500|     2194.0|            13.0|0.5925250683682771|\n",
      "|42101000600|     1565.0|             5.0|0.3194888178913738|\n",
      "|42101000700|     2650.0|             0.0|               0.0|\n",
      "|42101000801|     1713.0|            49.0| 2.860478692352598|\n",
      "|42101000803|     3680.0|           134.0| 3.641304347826087|\n",
      "|42101000804|     3224.0|            35.0|  1.08560794044665|\n",
      "|42101000901|     1910.0|             0.0|               0.0|\n",
      "|42101000902|     2367.0|            86.0| 3.633291085762569|\n",
      "|42101001001|     2392.0|           220.0| 9.197324414715718|\n",
      "|42101001002|     3311.0|           283.0|  8.54726668680157|\n",
      "|42101001101|     3123.0|            59.0|1.8892090938200448|\n",
      "|42101001102|     2483.0|           129.0| 5.195328231977447|\n",
      "|42101001201|     3636.0|           220.0| 6.050605060506051|\n",
      "|42101001202|     5050.0|           175.0|3.4653465346534658|\n",
      "|42101001300|     5413.0|           423.0|7.8145205985590245|\n",
      "+-----------+-----------+----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mdf_phili_2010_path\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/df_phili_2010.csv\"\u001b[39m\n",
       "\u001b[36mdf_phili_2010\u001b[39m: \u001b[32mDataFrame\u001b[39m = [geoid: string, n_total_pop: string ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_phili_2010_path = \"/home/acald013/RIDIR/Datasets/df_phili_2010.csv\"\n",
    "val df_phili_2010 = spark.read.option(\"header\", \"true\").option(\"delimiter\", \"\\t\").csv(df_phili_2010_path)\n",
    "df_phili_2010.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
