{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spark-stubs\n",
      "Creating SparkSession\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                   // Or use any other 2.x version here\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                    \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.serializer.KryoSerializer\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.storage.StorageLevel\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.mllib.evaluation.RegressionMetrics\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.rdd.RDD\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.enums.{GridType, IndexType}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.spatialOperator.JoinQuery\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mscala.collection.JavaConverters._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.io._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.log4j.{Level, Logger}\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@5a61213e\n",
       "\u001b[32mimport \u001b[39m\u001b[36mspark.implicits._\n",
       "\u001b[39m\n",
       "\u001b[36mappID\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"local-1556122525216\"\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.0` // Or use any other 2.x version here\n",
    "import $ivy.`org.apache.spark::spark-mllib:2.4.0`\n",
    "import $ivy.`sh.almond::ammonite-spark:0.4.0`\n",
    "import $ivy.`org.datasyslab:geospark:1.2.0`\n",
    "\n",
    "import org.apache.spark.serializer.KryoSerializer\n",
    "import org.apache.spark.storage.StorageLevel\n",
    "import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
    "import org.apache.spark.rdd.RDD\n",
    "import org.datasyslab.geospark.enums.{GridType, IndexType}\n",
    "import org.datasyslab.geospark.spatialOperator.JoinQuery\n",
    "import org.datasyslab.geospark.formatMapper.shapefileParser.ShapefileReader\n",
    "import scala.collection.JavaConverters._\n",
    "import java.io._\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = AmmoniteSparkSession.builder()\n",
    "    .master(\"local[*]\").appName(\"Validator\")\n",
    "    .getOrCreate()\n",
    "import spark.implicits._\n",
    "val appID = spark.sparkContext.applicationId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpath\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"/home/acald013/RIDIR/Datasets/AreaTablesValidation\"\u001b[39m\n",
       "\u001b[36mstate\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"NY\"\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val path = \"/home/acald013/RIDIR/Datasets/AreaTablesValidation\"\n",
    "val state = \"NY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgeopandas\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [_c0: string, _c1: string ... 1 more field]\n",
       "\u001b[36mres13_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m8151L\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geopandas = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(s\"${path}/${state}_geopandas_test.tsv\").distinct()\n",
    "geopandas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mgeospark\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [_c0: string, _c1: string ... 1 more field]\n",
       "\u001b[36mres14_1\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m8151L\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val geospark = spark.read.option(\"header\", \"false\").option(\"delimiter\", \"\\t\").csv(s\"${path}/${state}_geospark_test.tsv\").distinct()\n",
    "geospark.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mp\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [s1: string, t1: string ... 1 more field]\n",
       "\u001b[36ms\u001b[39m: \u001b[32mDataset\u001b[39m[\u001b[32mRow\u001b[39m] = [s2: string, t2: string ... 1 more field]\n",
       "\u001b[36mareas\u001b[39m: \u001b[32mRDD\u001b[39m[(\u001b[32mDouble\u001b[39m, \u001b[32mDouble\u001b[39m)] = ZippedPartitionsRDD2[191] at zip at cmd15.sc:3"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val p = geopandas.toDF(\"s1\", \"t1\", \"area1\").orderBy(\"s1\", \"t1\", \"area1\")\n",
    "val s = geopandas.toDF(\"s2\", \"t2\", \"area2\").orderBy(\"s2\", \"t2\", \"area2\")\n",
    "val areas = p.select(\"area1\").map(_.getString(0).toDouble).rdd.zip(s.select(\"area2\").map(_.getString(0).toDouble).rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+---------------------+\n",
      "|area1                |area2                |\n",
      "+---------------------+---------------------+\n",
      "|2.5261844522811287E-4|2.5261844522811287E-4|\n",
      "|1.5625384114862862E-9|1.5625384114862862E-9|\n",
      "|2.739295819020814E-6 |2.739295819020814E-6 |\n",
      "|3.460224149717579E-6 |3.460224149717579E-6 |\n",
      "|5.562846016544312E-6 |5.562846016544312E-6 |\n",
      "|6.0613064292227E-7   |6.0613064292227E-7   |\n",
      "|7.053096052975711E-7 |7.053096052975711E-7 |\n",
      "|1.5807291728437404E-4|1.5807291728437404E-4|\n",
      "|3.136674887704342E-6 |3.136674887704342E-6 |\n",
      "|1.4531710548415795E-6|1.4531710548415795E-6|\n",
      "|1.5161355047734145E-7|1.5161355047734145E-7|\n",
      "|0.011766889918222573 |0.011766889918222573 |\n",
      "|5.807427196973919E-5 |5.807427196973919E-5 |\n",
      "|0.017480647972263185 |0.017480647972263185 |\n",
      "|8.105140766924522E-6 |8.105140766924522E-6 |\n",
      "|4.167211632987942E-4 |4.167211632987942E-4 |\n",
      "|2.1286769616081068E-6|2.1286769616081068E-6|\n",
      "|8.20496423948473E-6  |8.20496423948473E-6  |\n",
      "|4.828190247372042E-4 |4.828190247372042E-4 |\n",
      "|1.054274046567112E-5 |1.054274046567112E-5 |\n",
      "+---------------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "areas.toDF(\"area1\", \"area2\").show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mreg\u001b[39m: \u001b[32mRegressionMetrics\u001b[39m = org.apache.spark.mllib.evaluation.RegressionMetrics@34c93c6\n",
       "\u001b[36mres17_1\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m1.0\u001b[39m\n",
       "\u001b[36mres17_2\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.0\u001b[39m\n",
       "\u001b[36mres17_3\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.0\u001b[39m\n",
       "\u001b[36mres17_4\u001b[39m: \u001b[32mDouble\u001b[39m = \u001b[32m0.0\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val reg = new RegressionMetrics(areas)\n",
    "reg.r2\n",
    "reg.meanAbsoluteError\n",
    "reg.meanSquaredError\n",
    "reg.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
